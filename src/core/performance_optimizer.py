#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
YOLOSæ€§èƒ½ä¼˜åŒ–å™¨
æä¾›ç³»ç»Ÿçº§æ€§èƒ½ä¼˜åŒ–å’Œç¨³å®šæ€§å¢å¼ºåŠŸèƒ½
"""

import gc
import os
import sys
import time
import psutil
import threading
import multiprocessing
from typing import Dict, List, Any, Optional, Callable, Tuple
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
import logging
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import queue
import weakref

# å°è¯•å¯¼å…¥GPUç›¸å…³åº“
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

logger = logging.getLogger(__name__)

class OptimizationLevel(Enum):
    """ä¼˜åŒ–çº§åˆ«"""
    CONSERVATIVE = "conservative"  # ä¿å®ˆä¼˜åŒ–
    BALANCED = "balanced"         # å¹³è¡¡ä¼˜åŒ–
    AGGRESSIVE = "aggressive"     # æ¿€è¿›ä¼˜åŒ–

class ResourceType(Enum):
    """èµ„æºç±»å‹"""
    CPU = "cpu"
    MEMORY = "memory"
    GPU = "gpu"
    DISK = "disk"
    NETWORK = "network"

@dataclass
class SystemMetrics:
    """ç³»ç»ŸæŒ‡æ ‡"""
    cpu_usage: float = 0.0
    memory_usage: float = 0.0
    memory_available: int = 0
    gpu_usage: float = 0.0
    gpu_memory_usage: float = 0.0
    disk_usage: float = 0.0
    network_io: Dict[str, int] = field(default_factory=dict)
    timestamp: float = field(default_factory=time.time)

@dataclass
class OptimizationConfig:
    """ä¼˜åŒ–é…ç½®"""
    level: OptimizationLevel = OptimizationLevel.BALANCED
    enable_memory_optimization: bool = True
    enable_cpu_optimization: bool = True
    enable_gpu_optimization: bool = True
    enable_cache_optimization: bool = True
    enable_threading_optimization: bool = True
    max_workers: Optional[int] = None
    memory_threshold: float = 0.8  # å†…å­˜ä½¿ç”¨é˜ˆå€¼
    cpu_threshold: float = 0.9     # CPUä½¿ç”¨é˜ˆå€¼
    gc_frequency: int = 100        # åƒåœ¾å›æ”¶é¢‘ç‡
    cache_size_limit: int = 1000   # ç¼“å­˜å¤§å°é™åˆ¶

class MemoryManager:
    """å†…å­˜ç®¡ç†å™¨"""
    
    def __init__(self, config: OptimizationConfig):
        self.config = config
        self.cache = {}
        self.cache_access_count = {}
        self.cache_lock = threading.Lock()
        
    def get_memory_info(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ä¿¡æ¯"""
        memory = psutil.virtual_memory()
        return {
            'total': memory.total,
            'available': memory.available,
            'used': memory.used,
            'percentage': memory.percent,
            'free': memory.free
        }
    
    def optimize_memory(self):
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        memory_info = self.get_memory_info()
        
        if memory_info['percentage'] > self.config.memory_threshold * 100:
            logger.warning(f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_info['percentage']:.1f}%")
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            self.force_garbage_collection()
            
            # æ¸…ç†ç¼“å­˜
            self.cleanup_cache()
            
            # å¦‚æœæœ‰PyTorchï¼Œæ¸…ç†GPUç¼“å­˜
            if TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    def force_garbage_collection(self):
        """å¼ºåˆ¶åƒåœ¾å›æ”¶"""
        collected = gc.collect()
        logger.debug(f"åƒåœ¾å›æ”¶å®Œæˆï¼Œå›æ”¶å¯¹è±¡æ•°: {collected}")
    
    def cleanup_cache(self, force: bool = False):
        """æ¸…ç†ç¼“å­˜"""
        with self.cache_lock:
            if force or len(self.cache) > self.config.cache_size_limit:
                # æŒ‰è®¿é—®æ¬¡æ•°æ’åºï¼Œåˆ é™¤æœ€å°‘ä½¿ç”¨çš„é¡¹
                if self.cache:
                    sorted_items = sorted(
                        self.cache_access_count.items(),
                        key=lambda x: x[1]
                    )
                    
                    # åˆ é™¤ä¸€åŠæœ€å°‘ä½¿ç”¨çš„é¡¹
                    items_to_remove = len(sorted_items) // 2
                    for key, _ in sorted_items[:items_to_remove]:
                        if key in self.cache:
                            del self.cache[key]
                        if key in self.cache_access_count:
                            del self.cache_access_count[key]
                    
                    logger.debug(f"æ¸…ç†ç¼“å­˜ï¼Œåˆ é™¤ {items_to_remove} ä¸ªé¡¹ç›®")
    
    def cache_get(self, key: str) -> Any:
        """ä»ç¼“å­˜è·å–æ•°æ®"""
        with self.cache_lock:
            if key in self.cache:
                self.cache_access_count[key] = self.cache_access_count.get(key, 0) + 1
                return self.cache[key]
            return None
    
    def cache_set(self, key: str, value: Any):
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        with self.cache_lock:
            self.cache[key] = value
            self.cache_access_count[key] = 1
            
            # æ£€æŸ¥ç¼“å­˜å¤§å°
            if len(self.cache) > self.config.cache_size_limit:
                self.cleanup_cache()

class CPUOptimizer:
    """CPUä¼˜åŒ–å™¨"""
    
    def __init__(self, config: OptimizationConfig):
        self.config = config
        self.cpu_count = multiprocessing.cpu_count()
        self.optimal_workers = self._calculate_optimal_workers()
        
    def _calculate_optimal_workers(self) -> int:
        """è®¡ç®—æœ€ä¼˜å·¥ä½œçº¿ç¨‹æ•°"""
        if self.config.max_workers:
            return min(self.config.max_workers, self.cpu_count)
        
        # æ ¹æ®ä¼˜åŒ–çº§åˆ«è°ƒæ•´
        if self.config.level == OptimizationLevel.CONSERVATIVE:
            return max(1, self.cpu_count // 2)
        elif self.config.level == OptimizationLevel.BALANCED:
            return max(2, self.cpu_count - 1)
        else:  # AGGRESSIVE
            return self.cpu_count
    
    def get_cpu_info(self) -> Dict[str, Any]:
        """è·å–CPUä¿¡æ¯"""
        return {
            'count': self.cpu_count,
            'usage': psutil.cpu_percent(interval=1),
            'frequency': psutil.cpu_freq()._asdict() if psutil.cpu_freq() else {},
            'load_average': os.getloadavg() if hasattr(os, 'getloadavg') else []
        }
    
    def optimize_cpu_usage(self):
        """ä¼˜åŒ–CPUä½¿ç”¨"""
        cpu_usage = psutil.cpu_percent(interval=1)
        
        if cpu_usage > self.config.cpu_threshold * 100:
            logger.warning(f"CPUä½¿ç”¨ç‡è¿‡é«˜: {cpu_usage:.1f}%")
            
            # è°ƒæ•´è¿›ç¨‹ä¼˜å…ˆçº§
            try:
                current_process = psutil.Process()
                if cpu_usage > 95:
                    # æé«˜ä½¿ç”¨ç‡æ—¶é™ä½ä¼˜å…ˆçº§
                    current_process.nice(5)
                elif cpu_usage > 85:
                    # é«˜ä½¿ç”¨ç‡æ—¶ç¨å¾®é™ä½ä¼˜å…ˆçº§
                    current_process.nice(2)
            except Exception as e:
                logger.debug(f"è°ƒæ•´è¿›ç¨‹ä¼˜å…ˆçº§å¤±è´¥: {e}")

class GPUOptimizer:
    """GPUä¼˜åŒ–å™¨"""
    
    def __init__(self, config: OptimizationConfig):
        self.config = config
        self.gpu_available = TORCH_AVAILABLE and torch.cuda.is_available()
        self.device_count = torch.cuda.device_count() if self.gpu_available else 0
        
    def get_gpu_info(self) -> Dict[str, Any]:
        """è·å–GPUä¿¡æ¯"""
        if not self.gpu_available:
            return {'available': False}
        
        info = {
            'available': True,
            'device_count': self.device_count,
            'devices': []
        }
        
        for i in range(self.device_count):
            device_info = {
                'id': i,
                'name': torch.cuda.get_device_name(i),
                'memory_total': torch.cuda.get_device_properties(i).total_memory,
                'memory_allocated': torch.cuda.memory_allocated(i),
                'memory_cached': torch.cuda.memory_reserved(i)
            }
            info['devices'].append(device_info)
        
        return info
    
    def optimize_gpu_memory(self):
        """ä¼˜åŒ–GPUå†…å­˜"""
        if not self.gpu_available:
            return
        
        for i in range(self.device_count):
            allocated = torch.cuda.memory_allocated(i)
            total = torch.cuda.get_device_properties(i).total_memory
            usage_ratio = allocated / total
            
            if usage_ratio > 0.9:
                logger.warning(f"GPU {i} å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {usage_ratio:.1%}")
                torch.cuda.empty_cache()
    
    def set_optimal_device(self) -> str:
        """è®¾ç½®æœ€ä¼˜GPUè®¾å¤‡"""
        if not self.gpu_available:
            return 'cpu'
        
        if self.device_count == 1:
            return 'cuda:0'
        
        # é€‰æ‹©å†…å­˜ä½¿ç”¨ç‡æœ€ä½çš„GPU
        best_device = 0
        min_usage = float('inf')
        
        for i in range(self.device_count):
            allocated = torch.cuda.memory_allocated(i)
            total = torch.cuda.get_device_properties(i).total_memory
            usage = allocated / total
            
            if usage < min_usage:
                min_usage = usage
                best_device = i
        
        return f'cuda:{best_device}'

class ThreadPoolManager:
    """çº¿ç¨‹æ± ç®¡ç†å™¨"""
    
    def __init__(self, config: OptimizationConfig):
        self.config = config
        self.cpu_optimizer = CPUOptimizer(config)
        self.thread_pool = None
        self.process_pool = None
        self._lock = threading.Lock()
        
    def get_thread_pool(self) -> ThreadPoolExecutor:
        """è·å–çº¿ç¨‹æ± """
        if self.thread_pool is None:
            with self._lock:
                if self.thread_pool is None:
                    max_workers = self.cpu_optimizer.optimal_workers
                    self.thread_pool = ThreadPoolExecutor(
                        max_workers=max_workers,
                        thread_name_prefix="YOLOS-Thread"
                    )
                    logger.debug(f"åˆ›å»ºçº¿ç¨‹æ± ï¼Œå·¥ä½œçº¿ç¨‹æ•°: {max_workers}")
        
        return self.thread_pool
    
    def get_process_pool(self) -> ProcessPoolExecutor:
        """è·å–è¿›ç¨‹æ± """
        if self.process_pool is None:
            with self._lock:
                if self.process_pool is None:
                    max_workers = min(4, self.cpu_optimizer.optimal_workers)
                    self.process_pool = ProcessPoolExecutor(
                        max_workers=max_workers
                    )
                    logger.debug(f"åˆ›å»ºè¿›ç¨‹æ± ï¼Œå·¥ä½œè¿›ç¨‹æ•°: {max_workers}")
        
        return self.process_pool
    
    def shutdown(self):
        """å…³é—­çº¿ç¨‹æ± å’Œè¿›ç¨‹æ± """
        if self.thread_pool:
            self.thread_pool.shutdown(wait=True)
            self.thread_pool = None
        
        if self.process_pool:
            self.process_pool.shutdown(wait=True)
            self.process_pool = None

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨ä¸»ç±»"""
    
    def __init__(self, config: OptimizationConfig = None):
        self.config = config or OptimizationConfig()
        self.memory_manager = MemoryManager(self.config)
        self.cpu_optimizer = CPUOptimizer(self.config)
        self.gpu_optimizer = GPUOptimizer(self.config)
        self.thread_pool_manager = ThreadPoolManager(self.config)
        
        self.monitoring_active = False
        self.monitoring_thread = None
        self.metrics_history: List[SystemMetrics] = []
        self.optimization_callbacks: List[Callable] = []
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'optimizations_performed': 0,
            'memory_cleanups': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
    
    def start_monitoring(self, interval: float = 30.0):
        """å¼€å§‹æ€§èƒ½ç›‘æ§"""
        if self.monitoring_active:
            return
        
        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(
            target=self._monitoring_loop,
            args=(interval,),
            daemon=True
        )
        self.monitoring_thread.start()
        logger.info(f"æ€§èƒ½ç›‘æ§å·²å¯åŠ¨ï¼Œç›‘æ§é—´éš”: {interval}ç§’")
    
    def stop_monitoring(self):
        """åœæ­¢æ€§èƒ½ç›‘æ§"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5.0)
        logger.info("æ€§èƒ½ç›‘æ§å·²åœæ­¢")
    
    def _monitoring_loop(self, interval: float):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring_active:
            try:
                metrics = self.collect_metrics()
                self.metrics_history.append(metrics)
                
                # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
                if len(self.metrics_history) > 1000:
                    self.metrics_history = self.metrics_history[-500:]
                
                # æ‰§è¡Œä¼˜åŒ–
                self.auto_optimize(metrics)
                
                # æ‰§è¡Œå›è°ƒ
                for callback in self.optimization_callbacks:
                    try:
                        callback(metrics)
                    except Exception as e:
                        logger.error(f"ä¼˜åŒ–å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
                
                time.sleep(interval)
                
            except Exception as e:
                logger.error(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                time.sleep(interval)
    
    def collect_metrics(self) -> SystemMetrics:
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        metrics = SystemMetrics()
        
        # CPUæŒ‡æ ‡
        metrics.cpu_usage = psutil.cpu_percent(interval=0.1)
        
        # å†…å­˜æŒ‡æ ‡
        memory = psutil.virtual_memory()
        metrics.memory_usage = memory.percent
        metrics.memory_available = memory.available
        
        # GPUæŒ‡æ ‡
        if self.gpu_optimizer.gpu_available:
            try:
                device_count = torch.cuda.device_count()
                total_memory = 0
                used_memory = 0
                
                for i in range(device_count):
                    props = torch.cuda.get_device_properties(i)
                    total_memory += props.total_memory
                    used_memory += torch.cuda.memory_allocated(i)
                
                if total_memory > 0:
                    metrics.gpu_memory_usage = (used_memory / total_memory) * 100
            except Exception as e:
                logger.debug(f"GPUæŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
        
        # ç£ç›˜æŒ‡æ ‡
        try:
            disk = psutil.disk_usage('/')
            metrics.disk_usage = disk.percent
        except Exception as e:
            logger.debug(f"ç£ç›˜æŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
        
        # ç½‘ç»œæŒ‡æ ‡
        try:
            net_io = psutil.net_io_counters()
            metrics.network_io = {
                'bytes_sent': net_io.bytes_sent,
                'bytes_recv': net_io.bytes_recv,
                'packets_sent': net_io.packets_sent,
                'packets_recv': net_io.packets_recv
            }
        except Exception as e:
            logger.debug(f"ç½‘ç»œæŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
        
        return metrics
    
    def auto_optimize(self, metrics: SystemMetrics):
        """è‡ªåŠ¨ä¼˜åŒ–"""
        optimized = False
        
        # å†…å­˜ä¼˜åŒ–
        if self.config.enable_memory_optimization:
            if metrics.memory_usage > self.config.memory_threshold * 100:
                self.memory_manager.optimize_memory()
                self.stats['memory_cleanups'] += 1
                optimized = True
        
        # CPUä¼˜åŒ–
        if self.config.enable_cpu_optimization:
            if metrics.cpu_usage > self.config.cpu_threshold * 100:
                self.cpu_optimizer.optimize_cpu_usage()
                optimized = True
        
        # GPUä¼˜åŒ–
        if self.config.enable_gpu_optimization:
            if metrics.gpu_memory_usage > 90:
                self.gpu_optimizer.optimize_gpu_memory()
                optimized = True
        
        if optimized:
            self.stats['optimizations_performed'] += 1
    
    def optimize_for_inference(self):
        """ä¸ºæ¨ç†ä¼˜åŒ–ç³»ç»Ÿ"""
        logger.info("å¼€å§‹æ¨ç†ä¼˜åŒ–...")
        
        # è®¾ç½®æœ€ä¼˜GPUè®¾å¤‡
        if self.config.enable_gpu_optimization:
            optimal_device = self.gpu_optimizer.set_optimal_device()
            logger.info(f"è®¾ç½®æœ€ä¼˜è®¾å¤‡: {optimal_device}")
        
        # é¢„çƒ­å†…å­˜ç®¡ç†
        self.memory_manager.force_garbage_collection()
        
        # æ¸…ç†ç¼“å­˜
        self.memory_manager.cleanup_cache()
        
        # è®¾ç½®çº¿ç¨‹æ± 
        if self.config.enable_threading_optimization:
            thread_pool = self.thread_pool_manager.get_thread_pool()
            logger.info(f"çº¿ç¨‹æ± å·²å‡†å¤‡ï¼Œå·¥ä½œçº¿ç¨‹æ•°: {thread_pool._max_workers}")
    
    def optimize_for_training(self):
        """ä¸ºè®­ç»ƒä¼˜åŒ–ç³»ç»Ÿ"""
        logger.info("å¼€å§‹è®­ç»ƒä¼˜åŒ–...")
        
        # æ›´æ¿€è¿›çš„å†…å­˜ç®¡ç†
        original_threshold = self.config.memory_threshold
        self.config.memory_threshold = 0.7  # é™ä½å†…å­˜é˜ˆå€¼
        
        # å¯ç”¨è¿›ç¨‹æ± ç”¨äºæ•°æ®åŠ è½½
        if self.config.enable_threading_optimization:
            process_pool = self.thread_pool_manager.get_process_pool()
            logger.info(f"è¿›ç¨‹æ± å·²å‡†å¤‡ï¼Œå·¥ä½œè¿›ç¨‹æ•°: {process_pool._max_workers}")
        
        # GPUå†…å­˜ä¼˜åŒ–
        if self.gpu_optimizer.gpu_available:
            self.gpu_optimizer.optimize_gpu_memory()
        
        return original_threshold
    
    def add_optimization_callback(self, callback: Callable[[SystemMetrics], None]):
        """æ·»åŠ ä¼˜åŒ–å›è°ƒ"""
        self.optimization_callbacks.append(callback)
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        current_metrics = self.collect_metrics()
        
        report = {
            'current_metrics': {
                'cpu_usage': current_metrics.cpu_usage,
                'memory_usage': current_metrics.memory_usage,
                'memory_available_gb': current_metrics.memory_available / (1024**3),
                'gpu_memory_usage': current_metrics.gpu_memory_usage,
                'disk_usage': current_metrics.disk_usage
            },
            'system_info': {
                'cpu_count': self.cpu_optimizer.cpu_count,
                'optimal_workers': self.cpu_optimizer.optimal_workers,
                'gpu_available': self.gpu_optimizer.gpu_available,
                'gpu_count': self.gpu_optimizer.device_count
            },
            'optimization_stats': self.stats.copy(),
            'config': {
                'level': self.config.level.value,
                'memory_threshold': self.config.memory_threshold,
                'cpu_threshold': self.config.cpu_threshold,
                'cache_size_limit': self.config.cache_size_limit
            }
        }
        
        # æ·»åŠ å†å²è¶‹åŠ¿
        if len(self.metrics_history) > 1:
            recent_metrics = self.metrics_history[-10:]  # æœ€è¿‘10æ¬¡
            report['trends'] = {
                'avg_cpu_usage': sum(m.cpu_usage for m in recent_metrics) / len(recent_metrics),
                'avg_memory_usage': sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
                'avg_gpu_memory_usage': sum(m.gpu_memory_usage for m in recent_metrics) / len(recent_metrics)
            }
        
        return report
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.stop_monitoring()
        self.thread_pool_manager.shutdown()
        self.memory_manager.cleanup_cache(force=True)
        logger.info("æ€§èƒ½ä¼˜åŒ–å™¨å·²æ¸…ç†")
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.start_monitoring()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.cleanup()

# å…¨å±€ä¼˜åŒ–å™¨å®ä¾‹
_global_optimizer: Optional[PerformanceOptimizer] = None

def get_global_optimizer(config: OptimizationConfig = None) -> PerformanceOptimizer:
    """è·å–å…¨å±€ä¼˜åŒ–å™¨å®ä¾‹"""
    global _global_optimizer
    
    if _global_optimizer is None:
        _global_optimizer = PerformanceOptimizer(config)
    
    return _global_optimizer

def optimize_for_platform(platform: str = "pc") -> OptimizationConfig:
    """ä¸ºç‰¹å®šå¹³å°åˆ›å»ºä¼˜åŒ–é…ç½®"""
    if platform.lower() in ["pc", "desktop", "server"]:
        return OptimizationConfig(
            level=OptimizationLevel.AGGRESSIVE,
            max_workers=None,  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
            memory_threshold=0.8,
            cpu_threshold=0.9,
            cache_size_limit=2000
        )
    elif platform.lower() in ["laptop", "mobile"]:
        return OptimizationConfig(
            level=OptimizationLevel.BALANCED,
            max_workers=4,
            memory_threshold=0.7,
            cpu_threshold=0.8,
            cache_size_limit=1000
        )
    elif platform.lower() in ["raspberry_pi", "jetson", "embedded"]:
        return OptimizationConfig(
            level=OptimizationLevel.CONSERVATIVE,
            max_workers=2,
            memory_threshold=0.6,
            cpu_threshold=0.7,
            cache_size_limit=500
        )
    else:
        return OptimizationConfig()  # é»˜è®¤é…ç½®

# è£…é¥°å™¨ï¼šæ€§èƒ½ä¼˜åŒ–
def performance_optimized(optimization_level: OptimizationLevel = OptimizationLevel.BALANCED):
    """æ€§èƒ½ä¼˜åŒ–è£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            config = OptimizationConfig(level=optimization_level)
            optimizer = PerformanceOptimizer(config)
            
            try:
                optimizer.optimize_for_inference()
                result = func(*args, **kwargs)
                return result
            finally:
                optimizer.cleanup()
        
        return wrapper
    return decorator

if __name__ == "__main__":
    # æµ‹è¯•æ€§èƒ½ä¼˜åŒ–å™¨
    print("ğŸš€ YOLOSæ€§èƒ½ä¼˜åŒ–å™¨æµ‹è¯•")
    
    config = optimize_for_platform("pc")
    
    with PerformanceOptimizer(config) as optimizer:
        print("æ€§èƒ½ç›‘æ§å·²å¯åŠ¨...")
        
        # æ¨¡æ‹Ÿä¸€äº›å·¥ä½œè´Ÿè½½
        time.sleep(5)
        
        # è·å–æ€§èƒ½æŠ¥å‘Š
        report = optimizer.get_performance_report()
        print("\næ€§èƒ½æŠ¥å‘Š:")
        print(f"CPUä½¿ç”¨ç‡: {report['current_metrics']['cpu_usage']:.1f}%")
        print(f"å†…å­˜ä½¿ç”¨ç‡: {report['current_metrics']['memory_usage']:.1f}%")
        print(f"å¯ç”¨å†…å­˜: {report['current_metrics']['memory_available_gb']:.1f}GB")
        print(f"ä¼˜åŒ–æ¬¡æ•°: {report['optimization_stats']['optimizations_performed']}")
        
        print("\nâœ… æ€§èƒ½ä¼˜åŒ–å™¨æµ‹è¯•å®Œæˆ")