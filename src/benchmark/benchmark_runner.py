#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºå‡†æµ‹è¯•è¿è¡Œå™¨
æä¾›ç®€åŒ–çš„æ¥å£æ¥è¿è¡Œå„ç§æ€§èƒ½åŸºå‡†æµ‹è¯•
"""

import argparse
import logging
import sys
from pathlib import Path
from typing import List, Optional

from .performance_benchmark import (
    PerformanceBenchmark, BenchmarkConfig, BenchmarkType, 
    TestScenario, PlatformType
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('benchmark.log')
    ]
)

logger = logging.getLogger(__name__)

class BenchmarkRunner:
    """åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.benchmark = None
    
    def run_quick_test(self, platform: str = 'desktop') -> None:
        """è¿è¡Œå¿«é€Ÿæµ‹è¯•"""
        logger.info("å¼€å§‹å¿«é€Ÿæ€§èƒ½æµ‹è¯•")
        
        platform_type = self._parse_platform(platform)
        
        config = BenchmarkConfig(
            test_types=[
                BenchmarkType.DETECTION_SPEED,
                BenchmarkType.MEMORY_USAGE
            ],
            test_scenarios=[
                TestScenario.SINGLE_OBJECT,
                TestScenario.MULTI_OBJECT
            ],
            target_platforms=[platform_type],
            test_duration=15,
            warmup_duration=3
        )
        
        self.benchmark = PerformanceBenchmark(config)
        results = self.benchmark.run_benchmark()
        
        self._print_summary(results)
    
    def run_comprehensive_test(self, platforms: List[str] = None) -> None:
        """è¿è¡Œå…¨é¢æµ‹è¯•"""
        logger.info("å¼€å§‹å…¨é¢æ€§èƒ½æµ‹è¯•")
        
        if platforms is None:
            platforms = ['desktop']
        
        platform_types = [self._parse_platform(p) for p in platforms]
        
        config = BenchmarkConfig(
            test_types=list(BenchmarkType),
            test_scenarios=list(TestScenario),
            target_platforms=platform_types,
            test_duration=60,
            warmup_duration=10
        )
        
        self.benchmark = PerformanceBenchmark(config)
        results = self.benchmark.run_benchmark()
        
        self._print_summary(results)
    
    def run_tracking_test(self, platform: str = 'desktop') -> None:
        """è¿è¡Œè·Ÿè¸ªæ€§èƒ½æµ‹è¯•"""
        logger.info("å¼€å§‹è·Ÿè¸ªæ€§èƒ½æµ‹è¯•")
        
        platform_type = self._parse_platform(platform)
        
        config = BenchmarkConfig(
            test_types=[
                BenchmarkType.TRACKING_PERFORMANCE,
                BenchmarkType.DETECTION_SPEED,
                BenchmarkType.MEMORY_USAGE
            ],
            test_scenarios=[
                TestScenario.MULTI_OBJECT,
                TestScenario.FAST_MOTION,
                TestScenario.OCCLUSION
            ],
            target_platforms=[platform_type],
            test_duration=45,
            warmup_duration=5
        )
        
        self.benchmark = PerformanceBenchmark(config)
        results = self.benchmark.run_benchmark()
        
        self._print_summary(results)
    
    def run_aggregation_test(self, platform: str = 'desktop') -> None:
        """è¿è¡Œèšåˆæ•ˆæœæµ‹è¯•"""
        logger.info("å¼€å§‹èšåˆæ•ˆæœæµ‹è¯•")
        
        platform_type = self._parse_platform(platform)
        
        config = BenchmarkConfig(
            test_types=[
                BenchmarkType.AGGREGATION_EFFECTIVENESS,
                BenchmarkType.DETECTION_ACCURACY,
                BenchmarkType.DETECTION_SPEED
            ],
            test_scenarios=[
                TestScenario.CROWDED_SCENE,
                TestScenario.LIGHTING_CHANGE,
                TestScenario.SCALE_VARIATION
            ],
            target_platforms=[platform_type],
            test_duration=30,
            warmup_duration=5
        )
        
        self.benchmark = PerformanceBenchmark(config)
        results = self.benchmark.run_benchmark()
        
        self._print_summary(results)
    
    def run_platform_comparison(self) -> None:
        """è¿è¡Œå¹³å°å¯¹æ¯”æµ‹è¯•"""
        logger.info("å¼€å§‹å¹³å°å¯¹æ¯”æµ‹è¯•")
        
        config = BenchmarkConfig(
            test_types=[
                BenchmarkType.DETECTION_SPEED,
                BenchmarkType.MEMORY_USAGE,
                BenchmarkType.CPU_USAGE
            ],
            test_scenarios=[
                TestScenario.SINGLE_OBJECT,
                TestScenario.MULTI_OBJECT
            ],
            target_platforms=[
                PlatformType.DESKTOP,
                PlatformType.RASPBERRY_PI,
                PlatformType.ESP32
            ],
            test_duration=30,
            warmup_duration=5
        )
        
        self.benchmark = PerformanceBenchmark(config)
        results = self.benchmark.run_benchmark()
        
        self._print_summary(results)
    
    def run_custom_test(self, config_file: str) -> None:
        """è¿è¡Œè‡ªå®šä¹‰æµ‹è¯•"""
        logger.info(f"ä»é…ç½®æ–‡ä»¶è¿è¡Œæµ‹è¯•: {config_file}")
        
        # è¿™é‡Œå¯ä»¥å®ç°ä»JSON/YAMLæ–‡ä»¶åŠ è½½é…ç½®çš„é€»è¾‘
        # æš‚æ—¶ä½¿ç”¨é»˜è®¤é…ç½®
        config = BenchmarkConfig()
        
        self.benchmark = PerformanceBenchmark(config)
        results = self.benchmark.run_benchmark()
        
        self._print_summary(results)
    
    def _parse_platform(self, platform: str) -> PlatformType:
        """è§£æå¹³å°ç±»å‹"""
        platform_map = {
            'desktop': PlatformType.DESKTOP,
            'pc': PlatformType.DESKTOP,
            'raspberry_pi': PlatformType.RASPBERRY_PI,
            'rpi': PlatformType.RASPBERRY_PI,
            'jetson': PlatformType.JETSON,
            'esp32': PlatformType.ESP32,
            'k230': PlatformType.K230
        }
        
        platform_lower = platform.lower()
        if platform_lower not in platform_map:
            logger.warning(f"æœªçŸ¥å¹³å°ç±»å‹: {platform}ï¼Œä½¿ç”¨é»˜è®¤å¹³å° desktop")
            return PlatformType.DESKTOP
        
        return platform_map[platform_lower]
    
    def _print_summary(self, results) -> None:
        """æ‰“å°æµ‹è¯•ç»“æœæ‘˜è¦"""
        if not results:
            logger.warning("æ²¡æœ‰æµ‹è¯•ç»“æœ")
            return
        
        print("\n" + "="*60)
        print("åŸºå‡†æµ‹è¯•ç»“æœæ‘˜è¦")
        print("="*60)
        
        # æŒ‰å¹³å°åˆ†ç»„æ˜¾ç¤º
        platform_results = {}
        for result in results:
            platform = result.platform.value
            if platform not in platform_results:
                platform_results[platform] = []
            platform_results[platform].append(result)
        
        for platform, platform_results_list in platform_results.items():
            print(f"\nğŸ“± å¹³å°: {platform.upper()}")
            print("-" * 40)
            
            for result in platform_results_list:
                status_icon = "âœ…" if result.error_count == 0 else "âš ï¸"
                print(f"{status_icon} {result.test_type.value} - {result.scenario.value}")
                
                if result.avg_fps > 0:
                    print(f"   ğŸ“Š FPS: {result.avg_fps:.1f} (min: {result.min_fps:.1f}, max: {result.max_fps:.1f})")
                
                if result.avg_latency > 0:
                    print(f"   â±ï¸  å»¶è¿Ÿ: {result.avg_latency:.1f}ms")
                
                if result.avg_memory_mb > 0:
                    print(f"   ğŸ’¾ å†…å­˜: {result.avg_memory_mb:.1f}MB (å³°å€¼: {result.peak_memory_mb:.1f}MB)")
                
                if result.avg_cpu_percent > 0:
                    print(f"   ğŸ”¥ CPU: {result.avg_cpu_percent:.1f}% (å³°å€¼: {result.peak_cpu_percent:.1f}%)")
                
                if result.detection_count > 0:
                    print(f"   ğŸ¯ æ£€æµ‹æ•°: {result.detection_count}")
                
                if result.error_count > 0:
                    print(f"   âŒ é”™è¯¯æ•°: {result.error_count}")
                
                print()
        
        # æ•´ä½“ç»Ÿè®¡
        total_tests = len(results)
        successful_tests = len([r for r in results if r.error_count == 0])
        avg_fps_all = sum(r.avg_fps for r in results if r.avg_fps > 0) / max(1, len([r for r in results if r.avg_fps > 0]))
        
        print("\nğŸ“ˆ æ•´ä½“ç»Ÿè®¡")
        print("-" * 40)
        print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"æˆåŠŸæµ‹è¯•: {successful_tests} ({successful_tests/total_tests*100:.1f}%)")
        print(f"å¹³å‡FPS: {avg_fps_all:.1f}")
        
        if self.benchmark:
            output_dir = self.benchmark.output_dir
            print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Šä¿å­˜åœ¨: {output_dir}")
            print(f"   - è¯¦ç»†ç»“æœ: {output_dir}/detailed_results.json")
            print(f"   - æµ‹è¯•æŠ¥å‘Š: {output_dir}/benchmark_report.md")
            print(f"   - ç³»ç»Ÿä¿¡æ¯: {output_dir}/system_info.json")
        
        print("\n" + "="*60)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='YOLOS æ€§èƒ½åŸºå‡†æµ‹è¯•è¿è¡Œå™¨')
    parser.add_argument('test_type', choices=[
        'quick', 'comprehensive', 'tracking', 'aggregation', 'platform', 'custom'
    ], help='æµ‹è¯•ç±»å‹')
    parser.add_argument('--platform', '-p', default='desktop', 
                       help='ç›®æ ‡å¹³å° (desktop, raspberry_pi, jetson, esp32, k230)')
    parser.add_argument('--platforms', nargs='+', 
                       help='å¤šä¸ªå¹³å° (ç”¨äºcomprehensiveå’Œplatformæµ‹è¯•)')
    parser.add_argument('--config', '-c', help='è‡ªå®šä¹‰é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    runner = BenchmarkRunner()
    
    try:
        if args.test_type == 'quick':
            runner.run_quick_test(args.platform)
        elif args.test_type == 'comprehensive':
            platforms = args.platforms or [args.platform]
            runner.run_comprehensive_test(platforms)
        elif args.test_type == 'tracking':
            runner.run_tracking_test(args.platform)
        elif args.test_type == 'aggregation':
            runner.run_aggregation_test(args.platform)
        elif args.test_type == 'platform':
            runner.run_platform_comparison()
        elif args.test_type == 'custom':
            if not args.config:
                print("é”™è¯¯: è‡ªå®šä¹‰æµ‹è¯•éœ€è¦æŒ‡å®šé…ç½®æ–‡ä»¶ (--config)")
                sys.exit(1)
            runner.run_custom_test(args.config)
        
    except KeyboardInterrupt:
        logger.info("æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()