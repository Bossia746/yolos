#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆæµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨

æ±‡æ€»æ‰€æœ‰æµ‹è¯•ç»“æœï¼Œç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Šå’Œæ”¹è¿›å»ºè®®
åŒ…æ‹¬ï¼š
1. åŸºç¡€åŠŸèƒ½æµ‹è¯•ç»“æœ
2. æ ¸å¿ƒåŠŸèƒ½å¤æ‚è·¯å¾„æµ‹è¯•ç»“æœ
3. ç¡¬ä»¶å¹³å°å…¼å®¹æ€§æµ‹è¯•ç»“æœ
4. æ€§èƒ½å‹åŠ›å’Œè¾¹ç•Œæ¡ä»¶æµ‹è¯•ç»“æœ
5. éƒ¨ç½²å’Œé›†æˆåœºæ™¯æµ‹è¯•ç»“æœ
6. ç»¼åˆåˆ†æå’Œæ”¹è¿›å»ºè®®
"""

import json
import os
import sys
import glob
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from datetime import datetime
import logging
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TestSummary:
    """æµ‹è¯•æ‘˜è¦"""
    test_type: str
    total_tests: int
    successful_tests: int
    failed_tests: int
    success_rate: float
    avg_performance_score: float
    key_metrics: Dict[str, Any] = field(default_factory=dict)
    issues: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)

class ComprehensiveTestReportGenerator:
    """ç»¼åˆæµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, test_results_dir: str = "."):
        self.test_results_dir = test_results_dir
        self.test_summaries: List[TestSummary] = []
        self.logger = logging.getLogger(__name__)
    
    def find_test_reports(self) -> Dict[str, str]:
        """æŸ¥æ‰¾æµ‹è¯•æŠ¥å‘Šæ–‡ä»¶"""
        report_files = {}
        
        # æŸ¥æ‰¾å„ç±»æµ‹è¯•æŠ¥å‘Š
        patterns = {
            'basic_function': '*basic_function_test_report*.json',
            'core_complex': '*core_function_complex_test_report*.json',
            'hardware_compatibility': '*hardware_compatibility_report*.json',
            'performance_stress': '*performance_stress_boundary_report*.json',
            'deployment_integration': '*deployment_integration_report*.json'
        }
        
        for test_type, pattern in patterns.items():
            files = glob.glob(os.path.join(self.test_results_dir, pattern))
            if files:
                # é€‰æ‹©æœ€æ–°çš„æŠ¥å‘Šæ–‡ä»¶
                latest_file = max(files, key=os.path.getctime)
                report_files[test_type] = latest_file
                self.logger.info(f"æ‰¾åˆ°{test_type}æµ‹è¯•æŠ¥å‘Š: {latest_file}")
            else:
                self.logger.warning(f"æœªæ‰¾åˆ°{test_type}æµ‹è¯•æŠ¥å‘Š")
        
        return report_files
    
    def load_test_report(self, file_path: str) -> Optional[Dict[str, Any]]:
        """åŠ è½½æµ‹è¯•æŠ¥å‘Š"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.error(f"åŠ è½½æµ‹è¯•æŠ¥å‘Šå¤±è´¥ {file_path}: {e}")
            return None
    
    def analyze_basic_function_report(self, report: Dict[str, Any]) -> TestSummary:
        """åˆ†æåŸºç¡€åŠŸèƒ½æµ‹è¯•æŠ¥å‘Š"""
        test_summary = report.get('test_summary', {})
        
        total_tests = test_summary.get('total_tests', 0)
        successful_tests = test_summary.get('successful_tests', 0)
        failed_tests = test_summary.get('failed_tests', 0)
        success_rate = test_summary.get('success_rate', 0)
        
        # æå–å…³é”®æŒ‡æ ‡
        key_metrics = {
            'avg_execution_time': test_summary.get('avg_execution_time', 0),
            'total_execution_time': test_summary.get('total_execution_time', 0)
        }
        
        # æå–é—®é¢˜å’Œå»ºè®®
        issues = []
        recommendations = []
        
        if failed_tests > 0:
            issues.append(f"æœ‰{failed_tests}ä¸ªåŸºç¡€åŠŸèƒ½æµ‹è¯•å¤±è´¥")
            recommendations.append("ä¿®å¤å¤±è´¥çš„åŸºç¡€åŠŸèƒ½æµ‹è¯•")
        
        if success_rate < 90:
            issues.append(f"åŸºç¡€åŠŸèƒ½æµ‹è¯•æˆåŠŸç‡è¾ƒä½: {success_rate:.1f}%")
            recommendations.append("æé«˜åŸºç¡€åŠŸèƒ½çš„ç¨³å®šæ€§å’Œå¯é æ€§")
        
        return TestSummary(
            test_type="åŸºç¡€åŠŸèƒ½æµ‹è¯•",
            total_tests=total_tests,
            successful_tests=successful_tests,
            failed_tests=failed_tests,
            success_rate=success_rate,
            avg_performance_score=success_rate,
            key_metrics=key_metrics,
            issues=issues,
            recommendations=recommendations
        )
    
    def analyze_core_complex_report(self, report: Dict[str, Any]) -> TestSummary:
        """åˆ†ææ ¸å¿ƒåŠŸèƒ½å¤æ‚è·¯å¾„æµ‹è¯•æŠ¥å‘Š"""
        test_summary = report.get('test_summary', {})
        
        total_tests = test_summary.get('total_tests', 0)
        successful_tests = test_summary.get('successful_tests', 0)
        failed_tests = test_summary.get('failed_tests', 0)
        success_rate = test_summary.get('success_rate', 0)
        avg_performance_score = test_summary.get('avg_performance_score', 0)
        
        # æå–å…³é”®æŒ‡æ ‡
        key_metrics = {
            'avg_execution_time': test_summary.get('avg_execution_time', 0),
            'avg_memory_usage': test_summary.get('avg_memory_usage', 0),
            'avg_cpu_usage': test_summary.get('avg_cpu_usage', 0)
        }
        
        # æå–é—®é¢˜å’Œå»ºè®®
        issues = []
        recommendations = []
        
        if failed_tests > 0:
            issues.append(f"æœ‰{failed_tests}ä¸ªå¤æ‚è·¯å¾„æµ‹è¯•å¤±è´¥")
            recommendations.append("ä¼˜åŒ–å¤æ‚åœºæ™¯ä¸‹çš„ç®—æ³•å¤„ç†èƒ½åŠ›")
        
        if avg_performance_score < 80:
            issues.append(f"å¤æ‚è·¯å¾„æµ‹è¯•æ€§èƒ½è¯„åˆ†è¾ƒä½: {avg_performance_score:.1f}")
            recommendations.append("ä¼˜åŒ–å¤æ‚åœºæ™¯çš„æ€§èƒ½è¡¨ç°")
        
        return TestSummary(
            test_type="æ ¸å¿ƒåŠŸèƒ½å¤æ‚è·¯å¾„æµ‹è¯•",
            total_tests=total_tests,
            successful_tests=successful_tests,
            failed_tests=failed_tests,
            success_rate=success_rate,
            avg_performance_score=avg_performance_score,
            key_metrics=key_metrics,
            issues=issues,
            recommendations=recommendations
        )
    
    def analyze_hardware_compatibility_report(self, report: Dict[str, Any]) -> TestSummary:
        """åˆ†æç¡¬ä»¶å…¼å®¹æ€§æµ‹è¯•æŠ¥å‘Š"""
        test_summary = report.get('test_summary', {})
        
        total_tests = test_summary.get('total_tests', 0)
        successful_tests = test_summary.get('successful_tests', 0)
        failed_tests = test_summary.get('failed_tests', 0)
        success_rate = test_summary.get('success_rate', 0)
        avg_compatibility_score = test_summary.get('avg_compatibility_score', 0)
        
        # æå–å…³é”®æŒ‡æ ‡
        key_metrics = {
            'compatibility_rating': report.get('compatibility_rating', ''),
            'avg_compatibility_score': avg_compatibility_score,
            'hardware_platform': report.get('hardware_info', {}).get('platform', '')
        }
        
        # æå–é—®é¢˜å’Œå»ºè®®
        issues = []
        recommendations = []
        
        if failed_tests > 0:
            issues.append(f"æœ‰{failed_tests}ä¸ªç¡¬ä»¶å…¼å®¹æ€§æµ‹è¯•å¤±è´¥")
            recommendations.append("æ”¹è¿›ç¡¬ä»¶å¹³å°é€‚é…æ€§")
        
        if avg_compatibility_score < 85:
            issues.append(f"ç¡¬ä»¶å…¼å®¹æ€§è¯„åˆ†è¾ƒä½: {avg_compatibility_score:.1f}")
            recommendations.append("ä¼˜åŒ–ç¡¬ä»¶èµ„æºåˆ©ç”¨æ•ˆç‡")
        
        return TestSummary(
            test_type="ç¡¬ä»¶å¹³å°å…¼å®¹æ€§æµ‹è¯•",
            total_tests=total_tests,
            successful_tests=successful_tests,
            failed_tests=failed_tests,
            success_rate=success_rate,
            avg_performance_score=avg_compatibility_score,
            key_metrics=key_metrics,
            issues=issues,
            recommendations=recommendations
        )
    
    def analyze_performance_stress_report(self, report: Dict[str, Any]) -> TestSummary:
        """åˆ†ææ€§èƒ½å‹åŠ›æµ‹è¯•æŠ¥å‘Š"""
        test_summary = report.get('test_summary', {})
        
        total_tests = test_summary.get('total_tests', 0)
        successful_tests = test_summary.get('successful_tests', 0)
        failed_tests = test_summary.get('failed_tests', 0)
        success_rate = test_summary.get('success_rate', 0)
        avg_performance_score = test_summary.get('avg_performance_score', 0)
        
        # æå–å…³é”®æŒ‡æ ‡
        key_metrics = {
            'performance_rating': report.get('performance_rating', ''),
            'avg_performance_score': avg_performance_score,
            'peak_memory_usage': test_summary.get('peak_memory_usage', 0),
            'peak_cpu_usage': test_summary.get('peak_cpu_usage', 0)
        }
        
        # æå–é—®é¢˜å’Œå»ºè®®
        issues = []
        recommendations = []
        
        if failed_tests > 0:
            issues.append(f"æœ‰{failed_tests}ä¸ªæ€§èƒ½å‹åŠ›æµ‹è¯•å¤±è´¥")
            recommendations.append("æé«˜ç³»ç»Ÿåœ¨é«˜è´Ÿè½½ä¸‹çš„ç¨³å®šæ€§")
        
        if avg_performance_score < 80:
            issues.append(f"æ€§èƒ½å‹åŠ›æµ‹è¯•è¯„åˆ†è¾ƒä½: {avg_performance_score:.1f}")
            recommendations.append("ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½å’Œèµ„æºç®¡ç†")
        
        return TestSummary(
            test_type="æ€§èƒ½å‹åŠ›å’Œè¾¹ç•Œæ¡ä»¶æµ‹è¯•",
            total_tests=total_tests,
            successful_tests=successful_tests,
            failed_tests=failed_tests,
            success_rate=success_rate,
            avg_performance_score=avg_performance_score,
            key_metrics=key_metrics,
            issues=issues,
            recommendations=recommendations
        )
    
    def analyze_deployment_integration_report(self, report: Dict[str, Any]) -> TestSummary:
        """åˆ†æéƒ¨ç½²é›†æˆæµ‹è¯•æŠ¥å‘Š"""
        test_summary = report.get('test_summary', {})
        
        total_tests = test_summary.get('total_tests', 0)
        successful_tests = test_summary.get('successful_tests', 0)
        failed_tests = test_summary.get('failed_tests', 0)
        success_rate = test_summary.get('success_rate', 0)
        avg_availability = test_summary.get('avg_availability', 0)
        
        # æå–å…³é”®æŒ‡æ ‡
        key_metrics = {
            'integration_rating': report.get('integration_rating', ''),
            'avg_response_time': test_summary.get('avg_response_time', 0),
            'avg_throughput': test_summary.get('avg_throughput', 0),
            'avg_availability': avg_availability
        }
        
        # æå–é—®é¢˜å’Œå»ºè®®
        issues = []
        recommendations = []
        
        if failed_tests > 0:
            issues.append(f"æœ‰{failed_tests}ä¸ªéƒ¨ç½²é›†æˆæµ‹è¯•å¤±è´¥")
            recommendations.append("ä¿®å¤éƒ¨ç½²å’Œé›†æˆé—®é¢˜")
        
        if success_rate < 80:
            issues.append(f"éƒ¨ç½²é›†æˆæµ‹è¯•æˆåŠŸç‡è¾ƒä½: {success_rate:.1f}%")
            recommendations.append("æ”¹è¿›ç³»ç»Ÿé›†æˆå’Œéƒ¨ç½²æµç¨‹")
        
        return TestSummary(
            test_type="éƒ¨ç½²å’Œé›†æˆåœºæ™¯æµ‹è¯•",
            total_tests=total_tests,
            successful_tests=successful_tests,
            failed_tests=failed_tests,
            success_rate=success_rate,
            avg_performance_score=avg_availability,
            key_metrics=key_metrics,
            issues=issues,
            recommendations=recommendations
        )
    
    def generate_comprehensive_analysis(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆåˆ†æ"""
        if not self.test_summaries:
            return {'error': 'æ²¡æœ‰æµ‹è¯•æ‘˜è¦æ•°æ®'}
        
        # æ€»ä½“ç»Ÿè®¡
        total_tests = sum(s.total_tests for s in self.test_summaries)
        total_successful = sum(s.successful_tests for s in self.test_summaries)
        total_failed = sum(s.failed_tests for s in self.test_summaries)
        overall_success_rate = (total_successful / total_tests * 100) if total_tests > 0 else 0
        
        # å¹³å‡æ€§èƒ½è¯„åˆ†
        avg_performance_score = sum(s.avg_performance_score for s in self.test_summaries) / len(self.test_summaries)
        
        # æŒ‰æµ‹è¯•ç±»å‹ç»Ÿè®¡
        test_type_stats = {}
        for summary in self.test_summaries:
            test_type_stats[summary.test_type] = {
                'total_tests': summary.total_tests,
                'successful_tests': summary.successful_tests,
                'failed_tests': summary.failed_tests,
                'success_rate': summary.success_rate,
                'performance_score': summary.avg_performance_score
            }
        
        # æ±‡æ€»æ‰€æœ‰é—®é¢˜å’Œå»ºè®®
        all_issues = []
        all_recommendations = []
        
        for summary in self.test_summaries:
            all_issues.extend(summary.issues)
            all_recommendations.extend(summary.recommendations)
        
        # å»é‡å»ºè®®
        unique_recommendations = list(set(all_recommendations))
        
        # ç”Ÿæˆç³»ç»Ÿè´¨é‡è¯„çº§
        quality_rating = self._get_system_quality_rating(overall_success_rate, avg_performance_score)
        
        # ç”Ÿæˆæ”¹è¿›ä¼˜å…ˆçº§
        improvement_priorities = self._generate_improvement_priorities()
        
        return {
            'overall_statistics': {
                'total_tests': total_tests,
                'successful_tests': total_successful,
                'failed_tests': total_failed,
                'overall_success_rate': overall_success_rate,
                'avg_performance_score': avg_performance_score,
                'system_quality_rating': quality_rating
            },
            'test_type_breakdown': test_type_stats,
            'identified_issues': all_issues,
            'improvement_recommendations': unique_recommendations,
            'improvement_priorities': improvement_priorities,
            'deployment_readiness': self._assess_deployment_readiness(overall_success_rate, avg_performance_score)
        }
    
    def _get_system_quality_rating(self, success_rate: float, performance_score: float) -> str:
        """è·å–ç³»ç»Ÿè´¨é‡è¯„çº§"""
        combined_score = (success_rate + performance_score) / 2
        
        if combined_score >= 95:
            return "ä¼˜ç§€ (Excellent) - ç³»ç»Ÿè´¨é‡å“è¶Šï¼Œå¯ä»¥æŠ•å…¥ç”Ÿäº§"
        elif combined_score >= 85:
            return "è‰¯å¥½ (Good) - ç³»ç»Ÿè´¨é‡è‰¯å¥½ï¼Œå¯ä»¥éƒ¨ç½²ä½¿ç”¨"
        elif combined_score >= 70:
            return "ä¸€èˆ¬ (Fair) - ç³»ç»Ÿè´¨é‡ä¸€èˆ¬ï¼Œéœ€è¦ä¼˜åŒ–æ”¹è¿›"
        elif combined_score >= 50:
            return "è¾ƒå·® (Poor) - ç³»ç»Ÿè´¨é‡è¾ƒå·®ï¼Œéœ€è¦é‡å¤§æ”¹è¿›"
        else:
            return "ä¸åˆæ ¼ (Unacceptable) - ç³»ç»Ÿè´¨é‡ä¸åˆæ ¼ï¼Œä¸å¯æŠ•å…¥ä½¿ç”¨"
    
    def _generate_improvement_priorities(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ”¹è¿›ä¼˜å…ˆçº§"""
        priorities = []
        
        # åŸºäºæµ‹è¯•ç»“æœç¡®å®šä¼˜å…ˆçº§
        for summary in self.test_summaries:
            if summary.failed_tests > 0:
                priority_level = "é«˜" if summary.success_rate < 70 else "ä¸­"
                priorities.append({
                    'area': summary.test_type,
                    'priority': priority_level,
                    'reason': f"æˆåŠŸç‡{summary.success_rate:.1f}%ï¼Œæœ‰{summary.failed_tests}ä¸ªå¤±è´¥æµ‹è¯•",
                    'actions': summary.recommendations[:3]  # å–å‰3ä¸ªå»ºè®®
                })
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        priority_order = {'é«˜': 3, 'ä¸­': 2, 'ä½': 1}
        priorities.sort(key=lambda x: priority_order.get(x['priority'], 0), reverse=True)
        
        return priorities
    
    def _assess_deployment_readiness(self, success_rate: float, performance_score: float) -> Dict[str, Any]:
        """è¯„ä¼°éƒ¨ç½²å°±ç»ªæ€§"""
        readiness_score = (success_rate + performance_score) / 2
        
        if readiness_score >= 90:
            status = "å°±ç»ª (Ready)"
            recommendation = "ç³»ç»Ÿå·²å‡†å¤‡å¥½éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ"
            risk_level = "ä½"
        elif readiness_score >= 75:
            status = "åŸºæœ¬å°±ç»ª (Mostly Ready)"
            recommendation = "ç³»ç»ŸåŸºæœ¬å°±ç»ªï¼Œå»ºè®®ä¿®å¤å…³é”®é—®é¢˜åéƒ¨ç½²"
            risk_level = "ä¸­"
        elif readiness_score >= 60:
            status = "éœ€è¦æ”¹è¿› (Needs Improvement)"
            recommendation = "ç³»ç»Ÿéœ€è¦é‡è¦æ”¹è¿›æ‰èƒ½éƒ¨ç½²"
            risk_level = "é«˜"
        else:
            status = "æœªå°±ç»ª (Not Ready)"
            recommendation = "ç³»ç»Ÿæœªå‡†å¤‡å¥½éƒ¨ç½²ï¼Œéœ€è¦é‡å¤§æ”¹è¿›"
            risk_level = "å¾ˆé«˜"
        
        return {
            'status': status,
            'readiness_score': readiness_score,
            'recommendation': recommendation,
            'risk_level': risk_level,
            'required_actions': self._get_deployment_actions(readiness_score)
        }
    
    def _get_deployment_actions(self, readiness_score: float) -> List[str]:
        """è·å–éƒ¨ç½²è¡ŒåŠ¨å»ºè®®"""
        actions = []
        
        if readiness_score < 90:
            actions.append("ä¿®å¤æ‰€æœ‰å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹")
        
        if readiness_score < 80:
            actions.append("ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½å’Œç¨³å®šæ€§")
            actions.append("åŠ å¼ºé”™è¯¯å¤„ç†å’Œå¼‚å¸¸æ¢å¤")
        
        if readiness_score < 70:
            actions.append("é‡æ–°è®¾è®¡å…³é”®ç»„ä»¶")
            actions.append("å¢åŠ æ›´å¤šæµ‹è¯•è¦†ç›–")
        
        if readiness_score < 60:
            actions.append("è¿›è¡Œæ¶æ„é‡æ„")
            actions.append("é‡æ–°è¯„ä¼°æŠ€æœ¯é€‰å‹")
        
        # é€šç”¨å»ºè®®
        actions.extend([
            "å»ºç«‹æŒç»­é›†æˆ/æŒç»­éƒ¨ç½²æµæ°´çº¿",
            "å®æ–½ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ",
            "å‡†å¤‡å›æ»šè®¡åˆ’",
            "è¿›è¡Œç”Ÿäº§ç¯å¢ƒé¢„æ¼”"
        ])
        
        return actions[:8]  # é™åˆ¶å»ºè®®æ•°é‡
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        self.logger.info("å¼€å§‹ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š")
        
        # æŸ¥æ‰¾æµ‹è¯•æŠ¥å‘Šæ–‡ä»¶
        report_files = self.find_test_reports()
        
        if not report_files:
            return {'error': 'æœªæ‰¾åˆ°ä»»ä½•æµ‹è¯•æŠ¥å‘Šæ–‡ä»¶'}
        
        # åˆ†æå„ç±»æµ‹è¯•æŠ¥å‘Š
        for test_type, file_path in report_files.items():
            report_data = self.load_test_report(file_path)
            if report_data:
                try:
                    if test_type == 'basic_function':
                        summary = self.analyze_basic_function_report(report_data)
                    elif test_type == 'core_complex':
                        summary = self.analyze_core_complex_report(report_data)
                    elif test_type == 'hardware_compatibility':
                        summary = self.analyze_hardware_compatibility_report(report_data)
                    elif test_type == 'performance_stress':
                        summary = self.analyze_performance_stress_report(report_data)
                    elif test_type == 'deployment_integration':
                        summary = self.analyze_deployment_integration_report(report_data)
                    else:
                        continue
                    
                    self.test_summaries.append(summary)
                    self.logger.info(f"å·²åˆ†æ{test_type}æµ‹è¯•æŠ¥å‘Š")
                
                except Exception as e:
                    self.logger.error(f"åˆ†æ{test_type}æµ‹è¯•æŠ¥å‘Šå¤±è´¥: {e}")
        
        if not self.test_summaries:
            return {'error': 'æ²¡æœ‰æˆåŠŸåˆ†æçš„æµ‹è¯•æŠ¥å‘Š'}
        
        # ç”Ÿæˆç»¼åˆåˆ†æ
        comprehensive_analysis = self.generate_comprehensive_analysis()
        
        # æ„å»ºæœ€ç»ˆæŠ¥å‘Š
        final_report = {
            'report_metadata': {
                'generated_at': datetime.now().isoformat(),
                'generator_version': '1.0.0',
                'analyzed_reports': len(self.test_summaries),
                'report_files': report_files
            },
            'test_summaries': [
                {
                    'test_type': s.test_type,
                    'total_tests': s.total_tests,
                    'successful_tests': s.successful_tests,
                    'failed_tests': s.failed_tests,
                    'success_rate': s.success_rate,
                    'avg_performance_score': s.avg_performance_score,
                    'key_metrics': s.key_metrics,
                    'issues': s.issues,
                    'recommendations': s.recommendations
                } for s in self.test_summaries
            ],
            'comprehensive_analysis': comprehensive_analysis
        }
        
        return final_report
    
    def save_report(self, filename: str = None) -> str:
        """ä¿å­˜ç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'comprehensive_test_report_{timestamp}.json'
        
        report = self.generate_report()
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        return filename
    
    def print_summary(self):
        """æ‰“å°ç»¼åˆæµ‹è¯•æ‘˜è¦"""
        report = self.generate_report()
        
        if 'error' in report:
            print(f"é”™è¯¯: {report['error']}")
            return
        
        print("\n" + "="*100)
        print("YOLOSç³»ç»Ÿç»¼åˆæµ‹è¯•æŠ¥å‘Š")
        print("="*100)
        
        # æ€»ä½“ç»Ÿè®¡
        overall_stats = report['comprehensive_analysis']['overall_statistics']
        print(f"\nğŸ¯ æ€»ä½“æµ‹è¯•ç»Ÿè®¡:")
        print(f"   æ€»æµ‹è¯•æ•°: {overall_stats['total_tests']}")
        print(f"   æˆåŠŸæµ‹è¯•: {overall_stats['successful_tests']}")
        print(f"   å¤±è´¥æµ‹è¯•: {overall_stats['failed_tests']}")
        print(f"   æ€»ä½“æˆåŠŸç‡: {overall_stats['overall_success_rate']:.1f}%")
        print(f"   å¹³å‡æ€§èƒ½è¯„åˆ†: {overall_stats['avg_performance_score']:.1f}/100")
        print(f"   ç³»ç»Ÿè´¨é‡è¯„çº§: {overall_stats['system_quality_rating']}")
        
        # å„æµ‹è¯•ç±»å‹è¯¦æƒ…
        print(f"\nğŸ“Š å„æµ‹è¯•ç±»å‹è¯¦æƒ…:")
        for summary in report['test_summaries']:
            print(f"   {summary['test_type']}:")
            print(f"     - æµ‹è¯•æ•°: {summary['total_tests']} (æˆåŠŸ: {summary['successful_tests']}, å¤±è´¥: {summary['failed_tests']})")
            print(f"     - æˆåŠŸç‡: {summary['success_rate']:.1f}%")
            print(f"     - æ€§èƒ½è¯„åˆ†: {summary['avg_performance_score']:.1f}/100")
        
        # éƒ¨ç½²å°±ç»ªæ€§è¯„ä¼°
        deployment = report['comprehensive_analysis']['deployment_readiness']
        print(f"\nğŸš€ éƒ¨ç½²å°±ç»ªæ€§è¯„ä¼°:")
        print(f"   çŠ¶æ€: {deployment['status']}")
        print(f"   å°±ç»ªè¯„åˆ†: {deployment['readiness_score']:.1f}/100")
        print(f"   é£é™©ç­‰çº§: {deployment['risk_level']}")
        print(f"   å»ºè®®: {deployment['recommendation']}")
        
        # æ”¹è¿›ä¼˜å…ˆçº§
        priorities = report['comprehensive_analysis']['improvement_priorities']
        if priorities:
            print(f"\nâš¡ æ”¹è¿›ä¼˜å…ˆçº§ (å‰5é¡¹):")
            for i, priority in enumerate(priorities[:5], 1):
                print(f"   {i}. [{priority['priority']}] {priority['area']}")
                print(f"      åŸå› : {priority['reason']}")
        
        # å…³é”®é—®é¢˜
        issues = report['comprehensive_analysis']['identified_issues']
        if issues:
            print(f"\nâš ï¸  è¯†åˆ«çš„å…³é”®é—®é¢˜ (å‰5é¡¹):")
            for i, issue in enumerate(issues[:5], 1):
                print(f"   {i}. {issue}")
        
        # æ”¹è¿›å»ºè®®
        recommendations = report['comprehensive_analysis']['improvement_recommendations']
        if recommendations:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®® (å‰8é¡¹):")
            for i, rec in enumerate(recommendations[:8], 1):
                print(f"   {i}. {rec}")
        
        print("\n" + "="*100)

def main() -> int:
    """ä¸»å‡½æ•°"""
    try:
        print("å¼€å§‹ç”ŸæˆYOLOSç³»ç»Ÿç»¼åˆæµ‹è¯•æŠ¥å‘Š...")
        
        # åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨
        generator = ComprehensiveTestReportGenerator()
        
        # ç”Ÿæˆå¹¶æ‰“å°æ‘˜è¦
        generator.print_summary()
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = generator.save_report()
        print(f"\nğŸ“„ ç»¼åˆæµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        print("\nâœ… ç»¼åˆæµ‹è¯•æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        return 0
    
    except KeyboardInterrupt:
        print("\næŠ¥å‘Šç”Ÿæˆè¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"\næŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        return 1

if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code)