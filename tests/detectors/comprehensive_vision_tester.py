#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆè§†è§‰æµ‹è¯•å™¨æ¨¡å—
æä¾›å®Œæ•´çš„è§†è§‰æ£€æµ‹æµ‹è¯•åŠŸèƒ½
"""

import os
import time
import json
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

from .yolos_native_detector import YOLOSNativeDetector
from .modelscope_analyzer import ModelScopeEnhancedAnalyzer


class ComprehensiveVisionTester:
    """ç»¼åˆè§†è§‰æµ‹è¯•å™¨"""
    
    def __init__(self, test_images_dir: str = "test_images"):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.test_images_dir = Path(test_images_dir)
        self.results_dir = Path("test_results")
        self.results_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–æ£€æµ‹å™¨
        print("ğŸ”§ åˆå§‹åŒ–æ£€æµ‹å™¨...")
        self.yolo_detector = YOLOSNativeDetector()
        self.modelscope_analyzer = ModelScopeEnhancedAnalyzer()
        
        # æµ‹è¯•ç»Ÿè®¡
        self.test_stats = {
            "total_tests": 0,
            "successful_tests": 0,
            "failed_tests": 0,
            "yolo_successes": 0,
            "modelscope_successes": 0,
            "start_time": None,
            "end_time": None
        }
        
        print("âœ… ç»¼åˆè§†è§‰æµ‹è¯•å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def run_comprehensive_test(self, image_path: Optional[str] = None) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹ç»¼åˆè§†è§‰æ£€æµ‹æµ‹è¯•")
        print("="*60)
        
        self.test_stats["start_time"] = datetime.now()
        
        if image_path:
            # æµ‹è¯•å•ä¸ªå›¾åƒ
            results = self._test_single_image(image_path)
        else:
            # æµ‹è¯•ç›®å½•ä¸­çš„æ‰€æœ‰å›¾åƒ
            results = self._test_all_images()
        
        self.test_stats["end_time"] = datetime.now()
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        report = self._generate_test_report(results)
        
        # ä¿å­˜ç»“æœ
        self._save_results(results, report)
        
        print("\n" + "="*60)
        print("âœ… ç»¼åˆæµ‹è¯•å®Œæˆ")
        print("="*60)
        
        return {
            "results": results,
            "report": report,
            "stats": self.test_stats
        }
    
    def _test_single_image(self, image_path: str) -> List[Dict[str, Any]]:
        """æµ‹è¯•å•ä¸ªå›¾åƒ"""
        if not os.path.exists(image_path):
            print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            return []
        
        print(f"\nğŸ“¸ æµ‹è¯•å›¾åƒ: {image_path}")
        return [self._run_detection_pipeline(image_path)]
    
    def _test_all_images(self) -> List[Dict[str, Any]]:
        """æµ‹è¯•æ‰€æœ‰å›¾åƒ"""
        if not self.test_images_dir.exists():
            print(f"âŒ æµ‹è¯•å›¾åƒç›®å½•ä¸å­˜åœ¨: {self.test_images_dir}")
            print("ğŸ’¡ è¯·åˆ›å»ºtest_imagesç›®å½•å¹¶æ”¾å…¥æµ‹è¯•å›¾åƒ")
            return []
        
        # æ”¯æŒçš„å›¾åƒæ ¼å¼
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
        
        # æŸ¥æ‰¾æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = []
        for ext in image_extensions:
            image_files.extend(self.test_images_dir.glob(f"*{ext}"))
            image_files.extend(self.test_images_dir.glob(f"*{ext.upper()}"))
        
        if not image_files:
            print(f"âŒ åœ¨{self.test_images_dir}ä¸­æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")
            print(f"ğŸ’¡ æ”¯æŒçš„æ ¼å¼: {', '.join(image_extensions)}")
            return []
        
        print(f"\nğŸ“ æ‰¾åˆ° {len(image_files)} ä¸ªæµ‹è¯•å›¾åƒ")
        
        results = []
        for i, image_file in enumerate(image_files, 1):
            print(f"\nğŸ“¸ [{i}/{len(image_files)}] æµ‹è¯•: {image_file.name}")
            result = self._run_detection_pipeline(str(image_file))
            results.append(result)
            
            # ç®€çŸ­çš„è¿›åº¦åé¦ˆ
            if result["yolo_result"]["success"]:
                print(f"   âœ“ YOLOæ£€æµ‹æˆåŠŸ")
            else:
                print(f"   âœ— YOLOæ£€æµ‹å¤±è´¥")
            
            if result["modelscope_result"]["success"]:
                print(f"   âœ“ ModelScopeåˆ†ææˆåŠŸ")
            else:
                print(f"   âœ— ModelScopeåˆ†æå¤±è´¥")
        
        return results
    
    def _run_detection_pipeline(self, image_path: str) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ£€æµ‹æµæ°´çº¿"""
        self.test_stats["total_tests"] += 1
        
        pipeline_start = time.time()
        
        # 1. YOLOæ£€æµ‹
        print("   ğŸ” è¿è¡ŒYOLOæ£€æµ‹...")
        yolo_result = self.yolo_detector.detect_objects(image_path)
        
        if yolo_result["success"]:
            self.test_stats["yolo_successes"] += 1
        
        # 2. ModelScopeå¢å¼ºåˆ†æ
        print("   ğŸ§  è¿è¡ŒModelScopeå¢å¼ºåˆ†æ...")
        modelscope_result = self.modelscope_analyzer.analyze_with_context(
            image_path, yolo_result
        )
        
        if modelscope_result["success"]:
            self.test_stats["modelscope_successes"] += 1
        
        # 3. ç»¼åˆè¯„ä¼°
        overall_success = yolo_result["success"] and modelscope_result["success"]
        if overall_success:
            self.test_stats["successful_tests"] += 1
        else:
            self.test_stats["failed_tests"] += 1
        
        pipeline_time = time.time() - pipeline_start
        
        return {
            "image_path": image_path,
            "image_name": os.path.basename(image_path),
            "timestamp": datetime.now().isoformat(),
            "yolo_result": yolo_result,
            "modelscope_result": modelscope_result,
            "overall_success": overall_success,
            "total_pipeline_time": round(pipeline_time, 3)
        }
    
    def _generate_test_report(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        if not results:
            return {"error": "æ²¡æœ‰æµ‹è¯•ç»“æœ"}
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_tests = len(results)
        successful_tests = sum(1 for r in results if r["overall_success"])
        yolo_successes = sum(1 for r in results if r["yolo_result"]["success"])
        modelscope_successes = sum(1 for r in results if r["modelscope_result"]["success"])
        
        # è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´
        yolo_times = [r["yolo_result"].get("processing_time", 0) for r in results if r["yolo_result"]["success"]]
        modelscope_times = [r["modelscope_result"].get("processing_time", 0) for r in results if r["modelscope_result"]["success"]]
        pipeline_times = [r["total_pipeline_time"] for r in results]
        
        avg_yolo_time = sum(yolo_times) / len(yolo_times) if yolo_times else 0
        avg_modelscope_time = sum(modelscope_times) / len(modelscope_times) if modelscope_times else 0
        avg_pipeline_time = sum(pipeline_times) / len(pipeline_times) if pipeline_times else 0
        
        # æ£€æµ‹ç»Ÿè®¡
        detection_stats = self._calculate_detection_stats(results)
        
        # æµ‹è¯•æŒç»­æ—¶é—´
        duration = None
        if self.test_stats["start_time"] and self.test_stats["end_time"]:
            duration = (self.test_stats["end_time"] - self.test_stats["start_time"]).total_seconds()
        
        report = {
            "test_summary": {
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "failed_tests": total_tests - successful_tests,
                "success_rate": round(successful_tests / total_tests * 100, 2) if total_tests > 0 else 0,
                "test_duration_seconds": round(duration, 2) if duration else None
            },
            "component_performance": {
                "yolo_detector": {
                    "success_count": yolo_successes,
                    "success_rate": round(yolo_successes / total_tests * 100, 2) if total_tests > 0 else 0,
                    "avg_processing_time": round(avg_yolo_time, 3)
                },
                "modelscope_analyzer": {
                    "success_count": modelscope_successes,
                    "success_rate": round(modelscope_successes / total_tests * 100, 2) if total_tests > 0 else 0,
                    "avg_processing_time": round(avg_modelscope_time, 3),
                    "available": self.modelscope_analyzer.is_available()
                }
            },
            "performance_metrics": {
                "avg_yolo_time": round(avg_yolo_time, 3),
                "avg_modelscope_time": round(avg_modelscope_time, 3),
                "avg_pipeline_time": round(avg_pipeline_time, 3)
            },
            "detection_statistics": detection_stats,
            "timestamp": datetime.now().isoformat()
        }
        
        return report
    
    def _calculate_detection_stats(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯"""
        total_detections = 0
        class_counts = {}
        confidence_scores = []
        
        for result in results:
            if result["yolo_result"]["success"]:
                detections = result["yolo_result"].get("detections", [])
                total_detections += len(detections)
                
                for detection in detections:
                    class_name = detection.get("class", "unknown")
                    confidence = detection.get("confidence", 0)
                    
                    class_counts[class_name] = class_counts.get(class_name, 0) + 1
                    confidence_scores.append(confidence)
        
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
        
        return {
            "total_detections": total_detections,
            "avg_detections_per_image": round(total_detections / len(results), 2) if results else 0,
            "class_distribution": dict(sorted(class_counts.items(), key=lambda x: x[1], reverse=True)),
            "avg_confidence": round(avg_confidence, 3),
            "confidence_range": {
                "min": round(min(confidence_scores), 3) if confidence_scores else 0,
                "max": round(max(confidence_scores), 3) if confidence_scores else 0
            }
        }
    
    def _save_results(self, results: List[Dict[str, Any]], report: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = self.results_dir / f"test_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
        report_file = self.results_dir / f"test_report_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
        print(f"   è¯¦ç»†ç»“æœ: {results_file}")
        print(f"   æµ‹è¯•æŠ¥å‘Š: {report_file}")
    
    def print_summary(self, report: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        if "error" in report:
            print(f"âŒ {report['error']}")
            return
        
        summary = report["test_summary"]
        performance = report["component_performance"]
        metrics = report["performance_metrics"]
        
        print("\n" + "="*50)
        print("ğŸ“Š æµ‹è¯•æ‘˜è¦")
        print("="*50)
        print(f"æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"æˆåŠŸæµ‹è¯•: {summary['successful_tests']}")
        print(f"å¤±è´¥æµ‹è¯•: {summary['failed_tests']}")
        print(f"æˆåŠŸç‡: {summary['success_rate']}%")
        
        if summary.get('test_duration_seconds'):
            print(f"æµ‹è¯•è€—æ—¶: {summary['test_duration_seconds']}ç§’")
        
        print("\nğŸ“ˆ ç»„ä»¶æ€§èƒ½:")
        print(f"YOLOæ£€æµ‹å™¨: {performance['yolo_detector']['success_rate']}% æˆåŠŸç‡, å¹³å‡ {performance['yolo_detector']['avg_processing_time']}ç§’")
        print(f"ModelScopeåˆ†æå™¨: {performance['modelscope_analyzer']['success_rate']}% æˆåŠŸç‡, å¹³å‡ {performance['modelscope_analyzer']['avg_processing_time']}ç§’")
        
        print(f"\nâ±ï¸ å¹³å‡å¤„ç†æ—¶é—´: {metrics['avg_pipeline_time']}ç§’/å›¾åƒ")
        
        if "detection_statistics" in report:
            stats = report["detection_statistics"]
            print(f"\nğŸ¯ æ£€æµ‹ç»Ÿè®¡:")
            print(f"æ€»æ£€æµ‹æ•°: {stats['total_detections']}")
            print(f"å¹³å‡æ¯å›¾æ£€æµ‹æ•°: {stats['avg_detections_per_image']}")
            print(f"å¹³å‡ç½®ä¿¡åº¦: {stats['avg_confidence']}")
            
            if stats['class_distribution']:
                print("\nğŸ·ï¸ æ£€æµ‹ç±»åˆ«åˆ†å¸ƒ:")
                for class_name, count in list(stats['class_distribution'].items())[:5]:
                    print(f"   {class_name}: {count}")
    
    def get_test_stats(self) -> Dict[str, Any]:
        """è·å–æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯"""
        return self.test_stats.copy()