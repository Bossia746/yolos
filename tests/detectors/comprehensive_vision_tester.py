#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ÁªºÂêàËßÜËßâÊµãËØïÂô®Ê®°Âùó
Êèê‰æõÂÆåÊï¥ÁöÑËßÜËßâÊ£ÄÊµãÊµãËØïÂäüËÉΩ
"""

import os
import time
import json
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

from .yolos_native_detector import YOLOSNativeDetector
from .modelscope_analyzer import ModelScopeEnhancedAnalyzer


class ComprehensiveVisionTester:
    """ÁªºÂêàËßÜËßâÊµãËØïÂô®"""
    
    def __init__(self, test_images_dir: str = "test_images"):
        """ÂàùÂßãÂåñÊµãËØïÂô®"""
        self.test_images_dir = Path(test_images_dir)
        self.results_dir = Path("test_results")
        self.results_dir.mkdir(exist_ok=True)
        
        # ÂàùÂßãÂåñÊ£ÄÊµãÂô®
        print("üîß ÂàùÂßãÂåñÊ£ÄÊµãÂô®...")
        self.yolo_detector = YOLOSNativeDetector()
        self.modelscope_analyzer = ModelScopeEnhancedAnalyzer()
        
        # ÊµãËØïÁªüËÆ°
        self.test_stats = {
            "total_tests": 0,
            "successful_tests": 0,
            "failed_tests": 0,
            "yolo_successes": 0,
            "modelscope_successes": 0,
            "start_time": None,
            "end_time": None
        }
        
        print("‚úÖ ÁªºÂêàËßÜËßâÊµãËØïÂô®ÂàùÂßãÂåñÂÆåÊàê")
    
    def run_comprehensive_test(self, image_path: Optional[str] = None) -> Dict[str, Any]:
        """ËøêË°åÁªºÂêàÊµãËØï"""
        print("\n" + "="*60)
        print("üöÄ ÂºÄÂßãÁªºÂêàËßÜËßâÊ£ÄÊµãÊµãËØï")
        print("="*60)
        
        self.test_stats["start_time"] = datetime.now()
        
        if image_path:
            # ÊµãËØïÂçï‰∏™ÂõæÂÉè
            results = self._test_single_image(image_path)
        else:
            # ÊµãËØïÁõÆÂΩï‰∏≠ÁöÑÊâÄÊúâÂõæÂÉè
            results = self._test_all_images()
        
        self.test_stats["end_time"] = datetime.now()
        
        # ÁîüÊàêÊµãËØïÊä•Âëä
        report = self._generate_test_report(results)
        
        # ‰øùÂ≠òÁªìÊûú
        self._save_results(results, report)
        
        print("\n" + "="*60)
        print("‚úÖ ÁªºÂêàÊµãËØïÂÆåÊàê")
        print("="*60)
        
        return {
            "results": results,
            "report": report,
            "stats": self.test_stats
        }
    
    def _test_single_image(self, image_path: str) -> List[Dict[str, Any]]:
        """ÊµãËØïÂçï‰∏™ÂõæÂÉè"""
        if not os.path.exists(image_path):
            print(f"‚ùå ÂõæÂÉèÊñá‰ª∂‰∏çÂ≠òÂú®: {image_path}")
            return []
        
        print(f"\nüì∏ ÊµãËØïÂõæÂÉè: {image_path}")
        return [self._run_detection_pipeline(image_path)]
    
    def _test_all_images(self) -> List[Dict[str, Any]]:
        """ÊµãËØïÊâÄÊúâÂõæÂÉè"""
        if not self.test_images_dir.exists():
            print(f"‚ùå ÊµãËØïÂõæÂÉèÁõÆÂΩï‰∏çÂ≠òÂú®: {self.test_images_dir}")
            print("üí° ËØ∑ÂàõÂª∫test_imagesÁõÆÂΩïÂπ∂ÊîæÂÖ•ÊµãËØïÂõæÂÉè")
            return []
        
        # ÊîØÊåÅÁöÑÂõæÂÉèÊ†ºÂºè
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
        
        # Êü•ÊâæÊâÄÊúâÂõæÂÉèÊñá‰ª∂
        image_files = []
        for ext in image_extensions:
            image_files.extend(self.test_images_dir.glob(f"*{ext}"))
            image_files.extend(self.test_images_dir.glob(f"*{ext.upper()}"))
        
        if not image_files:
            print(f"‚ùå Âú®{self.test_images_dir}‰∏≠Êú™ÊâæÂà∞ÂõæÂÉèÊñá‰ª∂")
            print(f"üí° ÊîØÊåÅÁöÑÊ†ºÂºè: {', '.join(image_extensions)}")
            return []
        
        print(f"\nüìÅ ÊâæÂà∞ {len(image_files)} ‰∏™ÊµãËØïÂõæÂÉè")
        
        results = []
        for i, image_file in enumerate(image_files, 1):
            print(f"\nüì∏ [{i}/{len(image_files)}] ÊµãËØï: {image_file.name}")
            result = self._run_detection_pipeline(str(image_file))
            results.append(result)
            
            # ÁÆÄÁü≠ÁöÑËøõÂ∫¶ÂèçÈ¶à
            if result["yolo_result"]["success"]:
                print(f"   ‚úì YOLOÊ£ÄÊµãÊàêÂäü")
            else:
                print(f"   ‚úó YOLOÊ£ÄÊµãÂ§±Ë¥•")
            
            if result["modelscope_result"]["success"]:
                print(f"   ‚úì ModelScopeÂàÜÊûêÊàêÂäü")
            else:
                print(f"   ‚úó ModelScopeÂàÜÊûêÂ§±Ë¥•")
        
        return results
    
    def _run_detection_pipeline(self, image_path: str) -> Dict[str, Any]:
        """ËøêË°åÂÆåÊï¥ÁöÑÊ£ÄÊµãÊµÅÊ∞¥Á∫ø"""
        self.test_stats["total_tests"] += 1
        
        pipeline_start = time.time()
        
        # 1. YOLOÊ£ÄÊµã
        print("   üîç ËøêË°åYOLOÊ£ÄÊµã...")
        yolo_result = self.yolo_detector.detect_objects(image_path)
        
        if yolo_result["success"]:
            self.test_stats["yolo_successes"] += 1
        
        # 2. ModelScopeÂ¢ûÂº∫ÂàÜÊûê
        print("   üß† ËøêË°åModelScopeÂ¢ûÂº∫ÂàÜÊûê...")
        modelscope_result = self.modelscope_analyzer.analyze_with_context(
            image_path, yolo_result
        )
        
        if modelscope_result["success"]:
            self.test_stats["modelscope_successes"] += 1
        
        # 3. ÁªºÂêàËØÑ‰º∞
        overall_success = yolo_result["success"] and modelscope_result["success"]
        if overall_success:
            self.test_stats["successful_tests"] += 1
        else:
            self.test_stats["failed_tests"] += 1
        
        pipeline_time = time.time() - pipeline_start
        
        return {
            "image_path": image_path,
            "image_name": os.path.basename(image_path),
            "timestamp": datetime.now().isoformat(),
            "yolo_result": yolo_result,
            "modelscope_result": modelscope_result,
            "overall_success": overall_success,
            "total_pipeline_time": round(pipeline_time, 3)
        }
    
    def _generate_test_report(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ÁîüÊàêÊµãËØïÊä•Âëä"""
        if not results:
            return {"error": "Ê≤°ÊúâÊµãËØïÁªìÊûú"}
        
        # ËÆ°ÁÆóÁªüËÆ°‰ø°ÊÅØ
        total_tests = len(results)
        successful_tests = sum(1 for r in results if r["overall_success"])
        yolo_successes = sum(1 for r in results if r["yolo_result"]["success"])
        modelscope_successes = sum(1 for r in results if r["modelscope_result"]["success"])
        
        # ËÆ°ÁÆóÂπ≥ÂùáÂ§ÑÁêÜÊó∂Èó¥
        yolo_times = [r["yolo_result"].get("processing_time", 0) for r in results if r["yolo_result"]["success"]]
        modelscope_times = [r["modelscope_result"].get("processing_time", 0) for r in results if r["modelscope_result"]["success"]]
        pipeline_times = [r["total_pipeline_time"] for r in results]
        
        avg_yolo_time = sum(yolo_times) / len(yolo_times) if yolo_times else 0
        avg_modelscope_time = sum(modelscope_times) / len(modelscope_times) if modelscope_times else 0
        avg_pipeline_time = sum(pipeline_times) / len(pipeline_times) if pipeline_times else 0
        
        # Ê£ÄÊµãÁªüËÆ°
        detection_stats = self._calculate_detection_stats(results)
        
        # ÊµãËØïÊåÅÁª≠Êó∂Èó¥
        duration = None
        if self.test_stats["start_time"] and self.test_stats["end_time"]:
            duration = (self.test_stats["end_time"] - self.test_stats["start_time"]).total_seconds()
        
        report = {
            "test_summary": {
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "failed_tests": total_tests - successful_tests,
                "success_rate": round(successful_tests / total_tests * 100, 2) if total_tests > 0 else 0,
                "test_duration_seconds": round(duration, 2) if duration else None
            },
            "component_performance": {
                "yolo_detector": {
                    "success_count": yolo_successes,
                    "success_rate": round(yolo_successes / total_tests * 100, 2) if total_tests > 0 else 0,
                    "avg_processing_time": round(avg_yolo_time, 3)
                },
                "modelscope_analyzer": {
                    "success_count": modelscope_successes,
                    "success_rate": round(modelscope_successes / total_tests * 100, 2) if total_tests > 0 else 0,
                    "avg_processing_time": round(avg_modelscope_time, 3),
                    "available": self.modelscope_analyzer.is_available()
                }
            },
            "performance_metrics": {
                "avg_yolo_time": round(avg_yolo_time, 3),
                "avg_modelscope_time": round(avg_modelscope_time, 3),
                "avg_pipeline_time": round(avg_pipeline_time, 3)
            },
            "detection_statistics": detection_stats,
            "timestamp": datetime.now().isoformat()
        }
        
        return report
    
    def _calculate_detection_stats(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ËÆ°ÁÆóÊ£ÄÊµãÁªüËÆ°‰ø°ÊÅØ"""
        total_detections = 0
        class_counts = {}
        confidence_scores = []
        
        for result in results:
            if result["yolo_result"]["success"]:
                detections = result["yolo_result"].get("detections", [])
                total_detections += len(detections)
                
                for detection in detections:
                    class_name = detection.get("class", "unknown")
                    confidence = detection.get("confidence", 0)
                    
                    class_counts[class_name] = class_counts.get(class_name, 0) + 1
                    confidence_scores.append(confidence)
        
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
        
        return {
            "total_detections": total_detections,
            "avg_detections_per_image": round(total_detections / len(results), 2) if results else 0,
            "class_distribution": dict(sorted(class_counts.items(), key=lambda x: x[1], reverse=True)),
            "avg_confidence": round(avg_confidence, 3),
            "confidence_range": {
                "min": round(min(confidence_scores), 3) if confidence_scores else 0,
                "max": round(max(confidence_scores), 3) if confidence_scores else 0
            }
        }
    
    def _save_results(self, results: List[Dict[str, Any]], report: Dict[str, Any]):
        """‰øùÂ≠òÊµãËØïÁªìÊûú"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ‰øùÂ≠òËØ¶ÁªÜÁªìÊûú
        results_file = self.results_dir / f"test_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ‰øùÂ≠òÊµãËØïÊä•Âëä
        report_file = self.results_dir / f"test_report_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ ÁªìÊûúÂ∑≤‰øùÂ≠ò:")
        print(f"   ËØ¶ÁªÜÁªìÊûú: {results_file}")
        print(f"   ÊµãËØïÊä•Âëä: {report_file}")
    
    def print_summary(self, report: Dict[str, Any]):
        """ÊâìÂç∞ÊµãËØïÊëòË¶Å"""
        if "error" in report:
            print(f"‚ùå {report['error']}")
            return
        
        summary = report["test_summary"]
        performance = report["component_performance"]
        metrics = report["performance_metrics"]
        
        print("\n" + "="*50)
        print("üìä ÊµãËØïÊëòË¶Å")
        print("="*50)
        print(f"ÊÄªÊµãËØïÊï∞: {summary['total_tests']}")
        print(f"ÊàêÂäüÊµãËØï: {summary['successful_tests']}")
        print(f"Â§±Ë¥•ÊµãËØï: {summary['failed_tests']}")
        print(f"ÊàêÂäüÁéá: {summary['success_rate']}%")
        
        if summary.get('test_duration_seconds'):
            print(f"ÊµãËØïËÄóÊó∂: {summary['test_duration_seconds']}Áßí")
        
        print("\nüìà ÁªÑ‰ª∂ÊÄßËÉΩ:")
        print(f"YOLOÊ£ÄÊµãÂô®: {performance['yolo_detector']['success_rate']}% ÊàêÂäüÁéá, Âπ≥Âùá {performance['yolo_detector']['avg_processing_time']}Áßí")
        print(f"ModelScopeÂàÜÊûêÂô®: {performance['modelscope_analyzer']['success_rate']}% ÊàêÂäüÁéá, Âπ≥Âùá {performance['modelscope_analyzer']['avg_processing_time']}Áßí")
        
        print(f"\n‚è±Ô∏è Âπ≥ÂùáÂ§ÑÁêÜÊó∂Èó¥: {metrics['avg_pipeline_time']}Áßí/ÂõæÂÉè")
        
        if "detection_statistics" in report:
            stats = report["detection_statistics"]
            print(f"\nüéØ Ê£ÄÊµãÁªüËÆ°:")
            print(f"ÊÄªÊ£ÄÊµãÊï∞: {stats['total_detections']}")
            print(f"Âπ≥ÂùáÊØèÂõæÊ£ÄÊµãÊï∞: {stats['avg_detections_per_image']}")
            print(f"Âπ≥ÂùáÁΩÆ‰ø°Â∫¶: {stats['avg_confidence']}")
            
            if stats['class_distribution']:
                print("\nüè∑Ô∏è Ê£ÄÊµãÁ±ªÂà´ÂàÜÂ∏É:")
                for class_name, count in list(stats['class_distribution'].items())[:5]:
                    print(f"   {class_name}: {count}")
    
    def get_test_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÊµãËØïÁªüËÆ°‰ø°ÊÅØ"""
        return self.test_stats.copy()