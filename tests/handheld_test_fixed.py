#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
YOLOSæ‰‹æŒé™æ€ç‰©ä½“è¯†åˆ«ä¸“ä¸šæµ‹è¯• - ä¿®æ­£ç‰ˆ
å±•ç¤ºé¡¹ç›®çš„å®Œæ•´æ£€æµ‹èƒ½åŠ›å’Œæ€§èƒ½ä¼˜åŒ–ç‰¹æ€§
"""

import cv2
import numpy as np
import time
import json
import threading
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))
sys.path.insert(0, str(current_dir / 'src'))

# å°è¯•å¯¼å…¥ultralytics YOLO
try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
    print("âœ… Ultralytics YOLO å¯ç”¨")
except ImportError:
    ULTRALYTICS_AVAILABLE = False
    print("âŒ Ultralytics YOLO ä¸å¯ç”¨")

# å¯¼å…¥æµ‹è¯•é…ç½®
try:
    from .test_config import YOLOSTestConfig
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥YOLOSTestConfigï¼Œä½¿ç”¨æœ¬åœ°é…ç½®")

# TestConfig is now imported from .test_config as YOLOSTestConfig

@dataclass
class DetectionStats:
    """æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯"""
    model_name: str
    total_detections: int = 0
    unique_objects: int = 0
    avg_confidence: float = 0.0
    avg_inference_time: float = 0.0
    fps: float = 0.0
    object_categories: Dict[str, int] = None
    confidence_distribution: List[float] = None
    inference_times: List[float] = None
    
    def __post_init__(self):
        if self.object_categories is None:
            self.object_categories = {}
        if self.confidence_distribution is None:
            self.confidence_distribution = []
        if self.inference_times is None:
            self.inference_times = []

class YOLODetector:
    """YOLOæ£€æµ‹å™¨åŒ…è£…ç±»"""
    
    def __init__(self, model_name: str, confidence_threshold: float = 0.25, iou_threshold: float = 0.45):
        self.model_name = model_name
        self.confidence_threshold = confidence_threshold
        self.iou_threshold = iou_threshold
        
        print(f"ğŸ¤– æ­£åœ¨åŠ è½½ {model_name} æ¨¡å‹...")
        
        try:
            self.model = YOLO(f'{model_name}.pt')
            print(f"âœ… {model_name} æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ {model_name} æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def detect(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """æ‰§è¡Œæ£€æµ‹"""
        try:
            results = self.model(image, conf=self.confidence_threshold, iou=self.iou_threshold, verbose=False)
            
            detections = []
            for result in results:
                if result.boxes is not None:
                    boxes = result.boxes.xyxy.cpu().numpy()
                    confidences = result.boxes.conf.cpu().numpy()
                    class_ids = result.boxes.cls.cpu().numpy().astype(int)
                    
                    for i, (box, conf, cls_id) in enumerate(zip(boxes, confidences, class_ids)):
                        x1, y1, x2, y2 = box
                        class_name = self.model.names.get(cls_id, f"class_{cls_id}")
                        
                        detections.append({
                            'bbox': [float(x1), float(y1), float(x2), float(y2)],
                            'confidence': float(conf),
                            'class_id': int(cls_id),
                            'class_name': class_name
                        })
            
            return detections
            
        except Exception as e:
            print(f"æ£€æµ‹å¤±è´¥: {e}")
            return []

class HandheldObjectRecognitionTest:
    """æ‰‹æŒé™æ€ç‰©ä½“è¯†åˆ«æµ‹è¯•ç³»ç»Ÿ"""
    
    def __init__(self, config: YOLOSTestConfig):
        self.config = config
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–æ£€æµ‹å™¨
        self.detectors = {}
        self.detection_stats = {}
        
        # æµ‹è¯•æ•°æ®
        self.test_images = []
        self.test_results = {}
        
        # æ‘„åƒå¤´
        self.camera = None
        self.is_testing = False
        
        print("ğŸ¯ æ‰‹æŒç‰©ä½“è¯†åˆ«æµ‹è¯•ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def initialize_detectors(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ£€æµ‹å™¨"""
        print("ğŸ¤– æ­£åœ¨åˆå§‹åŒ–æ£€æµ‹å™¨...")
        
        if not ULTRALYTICS_AVAILABLE:
            print("âŒ Ultralyticsä¸å¯ç”¨ï¼Œæ— æ³•åŠ è½½YOLOæ¨¡å‹")
            return False
        
        for model_name in self.config.models_to_test:
            try:
                detector = YOLODetector(
                    model_name=model_name,
                    confidence_threshold=self.config.confidence_threshold,
                    iou_threshold=self.config.iou_threshold
                )
                
                self.detectors[model_name] = detector
                self.detection_stats[model_name] = DetectionStats(model_name=model_name)
                
            except Exception as e:
                print(f"âŒ {model_name} åŠ è½½å¤±è´¥: {e}")
        
        if not self.detectors:
            print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ£€æµ‹å™¨")
            return False
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(self.detectors)} ä¸ªæ£€æµ‹å™¨")
        return True
    
    def setup_camera(self):
        """è®¾ç½®æ‘„åƒå¤´"""
        print("ğŸ“· æ­£åœ¨åˆå§‹åŒ–æ‘„åƒå¤´...")
        
        self.camera = cv2.VideoCapture(self.config.camera_id)
        
        if not self.camera.isOpened():
            raise RuntimeError(f"æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {self.config.camera_id}")
        
        # è®¾ç½®æ‘„åƒå¤´å‚æ•°
        self.camera.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        self.camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        self.camera.set(cv2.CAP_PROP_FPS, 30)
        self.camera.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        
        # è·å–å®é™…å‚æ•°
        width = int(self.camera.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(self.camera.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = int(self.camera.get(cv2.CAP_PROP_FPS))
        
        print(f"âœ… æ‘„åƒå¤´é…ç½®: {width}x{height} @ {fps}FPS")
        
        # é¢„çƒ­æ‘„åƒå¤´
        for _ in range(10):
            ret, _ = self.camera.read()
            if not ret:
                raise RuntimeError("æ‘„åƒå¤´é¢„çƒ­å¤±è´¥")
        
        print("âœ… æ‘„åƒå¤´åˆå§‹åŒ–å®Œæˆ")
        return True
    
    def run_comprehensive_test(self):
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ‰‹æŒé™æ€ç‰©ä½“è¯†åˆ«ç»¼åˆæµ‹è¯•")
        print(f"â±ï¸ æµ‹è¯•æ—¶é•¿: {self.config.test_duration}ç§’")
        print(f"ğŸ¤– æµ‹è¯•æ¨¡å‹: {', '.join(self.config.models_to_test)}")
        
        try:
            # åˆå§‹åŒ–ç³»ç»Ÿ
            if not self.initialize_detectors():
                return False
            
            if not self.setup_camera():
                return False
            
            # åˆ›å»ºè§†é¢‘å½•åˆ¶å™¨
            video_writer = None
            if self.config.save_video:
                video_writer = self._create_video_writer()
            
            # å¼€å§‹æµ‹è¯•
            self.is_testing = True
            start_time = time.time()
            last_capture_time = 0
            frame_count = 0
            
            print("\n" + "="*80)
            print("ğŸ¯ YOLOSæ‰‹æŒé™æ€ç‰©ä½“è¯†åˆ«ä¸“ä¸šæµ‹è¯•")
            print("="*80)
            print("ğŸ“‹ æ“ä½œè¯´æ˜:")
            print("  ğŸ¤ æ‰‹æŒä¸åŒç‰©ä½“åœ¨æ‘„åƒå¤´å‰å±•ç¤º")
            print("  âŒ¨ï¸  æŒ‰ 'c' é”®æ‰‹åŠ¨æ•è·å½“å‰å¸§è¿›è¡Œåˆ†æ")
            print("  ğŸ’¾ æŒ‰ 's' é”®ä¿å­˜å½“å‰æ£€æµ‹ç»“æœ")
            print("  ğŸ“Š æŒ‰ 'p' é”®æ˜¾ç¤ºå®æ—¶æ€§èƒ½ç»Ÿè®¡")
            print("  ğŸšª æŒ‰ 'q' é”®é€€å‡ºæµ‹è¯•")
            print("="*80)
            
            while self.is_testing:
                current_time = time.time()
                elapsed_time = current_time - start_time
                
                # æ£€æŸ¥æµ‹è¯•æ—¶é—´
                if elapsed_time >= self.config.test_duration:
                    print("â° æµ‹è¯•æ—¶é—´åˆ°è¾¾ï¼Œè‡ªåŠ¨ç»“æŸ")
                    break
                
                # è¯»å–æ‘„åƒå¤´å¸§
                ret, frame = self.camera.read()
                if not ret:
                    print("âŒ æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
                    break
                
                frame_count += 1
                
                # åˆ›å»ºæ˜¾ç¤ºå¸§
                display_frame = frame.copy()
                
                # è‡ªåŠ¨æ•è·é€»è¾‘
                if (current_time - last_capture_time) >= self.config.capture_interval:
                    self._capture_and_analyze_frame(frame, f"auto_{len(self.test_images)}")
                    last_capture_time = current_time
                
                # ç»˜åˆ¶å®æ—¶ä¿¡æ¯
                self._draw_test_info(display_frame, elapsed_time, len(self.test_images))
                
                # æ˜¾ç¤ºæœ€æ–°æ£€æµ‹ç»“æœ
                if self.test_results:
                    latest_result = list(self.test_results.values())[-1]
                    self._draw_latest_detection_results(display_frame, latest_result)
                
                # å½•åˆ¶è§†é¢‘
                if video_writer:
                    video_writer.write(display_frame)
                
                # æ˜¾ç¤ºå›¾åƒ
                cv2.imshow('YOLOSæ‰‹æŒç‰©ä½“è¯†åˆ«ä¸“ä¸šæµ‹è¯•', display_frame)
                
                # å¤„ç†æŒ‰é”®
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("ğŸ‘‹ ç”¨æˆ·é€€å‡ºæµ‹è¯•")
                    break
                elif key == ord('c'):
                    self._capture_and_analyze_frame(frame, f"manual_{len(self.test_images)}")
                elif key == ord('s'):
                    self._save_current_results(frame)
                elif key == ord('p'):
                    self._print_realtime_stats()
            
            # æ¸…ç†èµ„æº
            if video_writer:
                video_writer.release()
            
            self.camera.release()
            cv2.destroyAllWindows()
            
            # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
            if self.config.generate_report:
                self._generate_comprehensive_report()
            
            print("âœ… æµ‹è¯•å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return False
        finally:
            self.is_testing = False
            if self.camera:
                self.camera.release()
            cv2.destroyAllWindows()
    
    def _capture_and_analyze_frame(self, frame: np.ndarray, frame_id: str):
        """æ•è·å¹¶åˆ†æå¸§"""
        print(f"ğŸ“¸ æ•è·å¸§è¿›è¡Œåˆ†æ: {frame_id}")
        
        # ä¿å­˜åŸå§‹å›¾åƒ
        if self.config.save_images:
            image_path = self.output_dir / f"captured_{frame_id}.jpg"
            cv2.imwrite(str(image_path), frame)
        
        # å­˜å‚¨æµ‹è¯•å›¾åƒ
        self.test_images.append((frame_id, frame.copy()))
        
        # å¯¹æ‰€æœ‰æ¨¡å‹è¿›è¡Œæ£€æµ‹
        frame_results = {}
        
        for model_name, detector in self.detectors.items():
            try:
                start_time = time.time()
                
                # æ‰§è¡Œæ£€æµ‹
                detections = detector.detect(frame)
                
                inference_time = time.time() - start_time
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self._update_detection_stats(model_name, detections, inference_time)
                
                # å­˜å‚¨ç»“æœ
                frame_results[model_name] = {
                    'detections': detections,
                    'inference_time': inference_time,
                    'timestamp': time.time()
                }
                
                print(f"ğŸ” {model_name}: {len(detections)}ä¸ªæ£€æµ‹, {inference_time:.3f}s")
                
            except Exception as e:
                print(f"âŒ {model_name} æ£€æµ‹å¤±è´¥: {e}")
                frame_results[model_name] = {
                    'detections': [],
                    'inference_time': 0,
                    'error': str(e)
                }
        
        # å­˜å‚¨å¸§ç»“æœ
        self.test_results[frame_id] = frame_results
        
        # åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–
        if len(frame_results) > 1:
            self._create_detection_comparison(frame, frame_results, frame_id)
    
    def _update_detection_stats(self, model_name: str, detections: List[Dict], inference_time: float):
        """æ›´æ–°æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.detection_stats[model_name]
        
        # åŸºç¡€ç»Ÿè®¡
        stats.total_detections += len(detections)
        
        # æ¨ç†æ—¶é—´ç»Ÿè®¡
        stats.inference_times.append(inference_time)
        stats.avg_inference_time = np.mean(stats.inference_times)
        stats.fps = 1.0 / stats.avg_inference_time if stats.avg_inference_time > 0 else 0
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        if detections:
            confidences = [d['confidence'] for d in detections]
            stats.confidence_distribution.extend(confidences)
            stats.avg_confidence = np.mean(stats.confidence_distribution)
            
            # ç±»åˆ«ç»Ÿè®¡
            for detection in detections:
                class_name = detection['class_name']
                stats.object_categories[class_name] = stats.object_categories.get(class_name, 0) + 1
            
            stats.unique_objects = len(stats.object_categories)
    
    def _create_detection_comparison(self, frame: np.ndarray, results: Dict, frame_id: str):
        """åˆ›å»ºæ£€æµ‹ç»“æœå¯¹æ¯”å›¾"""
        num_models = len(results)
        fig, axes = plt.subplots(1, num_models, figsize=(5*num_models, 5))
        
        if num_models == 1:
            axes = [axes]
        
        for idx, (model_name, result) in enumerate(results.items()):
            if idx >= len(axes):
                break
                
            # ç»˜åˆ¶æ£€æµ‹ç»“æœ
            annotated_frame = self._draw_detections_on_frame(
                frame.copy(), result['detections'], model_name
            )
            
            # è½¬æ¢BGRåˆ°RGB
            annotated_frame_rgb = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)
            
            axes[idx].imshow(annotated_frame_rgb)
            axes[idx].set_title(f"{model_name}\næ£€æµ‹æ•°: {len(result['detections'])}, "
                              f"æ—¶é—´: {result['inference_time']:.3f}s")
            axes[idx].axis('off')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / f"comparison_{frame_id}.png", dpi=150, bbox_inches='tight')
        plt.close()
    
    def _draw_detections_on_frame(self, frame: np.ndarray, detections: List[Dict], model_name: str) -> np.ndarray:
        """åœ¨å¸§ä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ"""
        colors = [
            (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),
            (255, 0, 255), (0, 255, 255), (128, 0, 128), (255, 165, 0)
        ]
        
        for i, detection in enumerate(detections):
            bbox = detection['bbox']
            x1, y1, x2, y2 = map(int, bbox)
            
            # é€‰æ‹©é¢œè‰²
            color = colors[detection['class_id'] % len(colors)]
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            
            # ç»˜åˆ¶æ ‡ç­¾
            label = f"{detection['class_name']} {detection['confidence']:.2f}"
            
            # è®¡ç®—æ–‡æœ¬å¤§å°
            (text_width, text_height), _ = cv2.getTextSize(
                label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1
            )
            
            # ç»˜åˆ¶æ–‡æœ¬èƒŒæ™¯
            cv2.rectangle(frame, (x1, y1 - text_height - 5), 
                         (x1 + text_width, y1), color, -1)
            
            # ç»˜åˆ¶æ–‡æœ¬
            cv2.putText(frame, label, (x1, y1 - 5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        
        return frame
    
    def _draw_test_info(self, frame: np.ndarray, elapsed_time: float, captured_frames: int):
        """ç»˜åˆ¶æµ‹è¯•ä¿¡æ¯"""
        info_lines = [
            f"æµ‹è¯•æ—¶é—´: {elapsed_time:.1f}s / {self.config.test_duration}s",
            f"å·²æ•è·å¸§æ•°: {captured_frames}",
            f"æ´»è·ƒæ¨¡å‹: {len(self.detectors)}",
            f"æŒ‰ 'c' æ‰‹åŠ¨æ•è·, 'q' é€€å‡º"
        ]
        
        # ç»˜åˆ¶åŠé€æ˜èƒŒæ™¯
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (400, 120), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # ç»˜åˆ¶æ–‡æœ¬
        for i, line in enumerate(info_lines):
            cv2.putText(frame, line, (20, 35 + i * 25),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)
    
    def _draw_latest_detection_results(self, frame: np.ndarray, latest_result: Dict):
        """ç»˜åˆ¶æœ€æ–°æ£€æµ‹ç»“æœæ‘˜è¦"""
        y_offset = 150
        
        # ç»˜åˆ¶èƒŒæ™¯
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, y_offset), (500, y_offset + 200), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # æ ‡é¢˜
        cv2.putText(frame, "æœ€æ–°æ£€æµ‹ç»“æœ:", (20, y_offset + 25),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        
        # å„æ¨¡å‹ç»“æœ
        line_y = y_offset + 50
        for model_name, result in latest_result.items():
            if 'error' in result:
                text = f"{model_name}: é”™è¯¯"
                color = (0, 0, 255)
            else:
                detections = result['detections']
                inference_time = result['inference_time']
                text = f"{model_name}: {len(detections)}ä¸ªç›®æ ‡, {inference_time:.3f}s"
                color = (0, 255, 0)
            
            cv2.putText(frame, text, (30, line_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            line_y += 25
    
    def _create_video_writer(self):
        """åˆ›å»ºè§†é¢‘å½•åˆ¶å™¨"""
        if not self.config.save_video:
            return None
        
        # è·å–æ‘„åƒå¤´å‚æ•°
        width = int(self.camera.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(self.camera.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = 20  # å½•åˆ¶FPS
        
        # åˆ›å»ºè§†é¢‘æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        video_path = self.output_dir / f"test_video_{timestamp}.mp4"
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        video_writer = cv2.VideoWriter(str(video_path), fourcc, fps, (width, height))
        
        print(f"ğŸ¥ å¼€å§‹å½•åˆ¶è§†é¢‘: {video_path}")
        return video_writer
    
    def _save_current_results(self, frame: np.ndarray):
        """ä¿å­˜å½“å‰ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å½“å‰å¸§
        frame_path = self.output_dir / f"manual_save_{timestamp}.jpg"
        cv2.imwrite(str(frame_path), frame)
        
        # ä¿å­˜æ£€æµ‹ç»“æœ
        if self.test_results:
            latest_result = list(self.test_results.values())[-1]
            result_path = self.output_dir / f"detection_result_{timestamp}.json"
            
            with open(result_path, 'w', encoding='utf-8') as f:
                json.dump(latest_result, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {frame_path}")
    
    def _print_realtime_stats(self):
        """æ‰“å°å®æ—¶ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*80)
        print("ğŸ“Š å®æ—¶æ€§èƒ½ç»Ÿè®¡")
        print("="*80)
        
        for model_name, stats in self.detection_stats.items():
            print(f"\nğŸ” {model_name}:")
            print(f"  ğŸ“ˆ æ€»æ£€æµ‹æ•°: {stats.total_detections}")
            print(f"  â±ï¸  å¹³å‡æ¨ç†æ—¶é—´: {stats.avg_inference_time:.3f}s")
            print(f"  ğŸš€ FPS: {stats.fps:.1f}")
            print(f"  ğŸ¯ å¹³å‡ç½®ä¿¡åº¦: {stats.avg_confidence:.3f}")
            print(f"  ğŸ·ï¸  æ£€æµ‹ç±»åˆ«æ•°: {stats.unique_objects}")
            
            if stats.object_categories:
                print("  ğŸ” æ£€æµ‹åˆ°çš„ç‰©ä½“:")
                for obj_name, count in sorted(stats.object_categories.items(), 
                                            key=lambda x: x[1], reverse=True)[:5]:
                    print(f"    â€¢ {obj_name}: {count}æ¬¡")
        
        print("="*80)
    
    def _generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        print("ğŸ“‹ æ­£åœ¨ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_dir = self.output_dir / f"report_{timestamp}"
        report_dir.mkdir(exist_ok=True)
        
        # 1. ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾è¡¨
        self._create_performance_charts(report_dir)
        
        # 2. ç”ŸæˆHTMLæŠ¥å‘Š
        self._create_html_report(report_dir, timestamp)
        
        # 3. ä¿å­˜åŸå§‹æ•°æ®
        self._save_raw_data(report_dir)
        
        print(f"ğŸ“‹ æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {report_dir}")
        print(f"ğŸŒ è¯·æ‰“å¼€ {report_dir}/test_report.html æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š")
    
    def _create_performance_charts(self, report_dir: Path):
        """åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾è¡¨"""
        try:
            # è®¾ç½®ä¸­æ–‡å­—ä½“
            plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']
            plt.rcParams['axes.unicode_minus'] = False
            
            # 1. æ¨ç†æ—¶é—´å¯¹æ¯”
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
            
            model_names = list(self.detection_stats.keys())
            inference_times = [stats.avg_inference_time for stats in self.detection_stats.values()]
            fps_values = [stats.fps for stats in self.detection_stats.values()]
            total_detections = [stats.total_detections for stats in self.detection_stats.values()]
            avg_confidences = [stats.avg_confidence for stats in self.detection_stats.values()]
            
            # æ¨ç†æ—¶é—´å¯¹æ¯”
            bars1 = ax1.bar(model_names, inference_times, color='skyblue')
            ax1.set_title('Average Inference Time Comparison')
            ax1.set_ylabel('Time (seconds)')
            ax1.tick_params(axis='x', rotation=45)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, time_val in zip(bars1, inference_times):
                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                        f'{time_val:.3f}s', ha='center', va='bottom')
            
            # FPSå¯¹æ¯”
            bars2 = ax2.bar(model_names, fps_values, color='lightgreen')
            ax2.set_title('FPS Performance Comparison')
            ax2.set_ylabel('FPS')
            ax2.tick_params(axis='x', rotation=45)
            
            for bar, fps_val in zip(bars2, fps_values):
                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                        f'{fps_val:.1f}', ha='center', va='bottom')
            
            # æ£€æµ‹æ•°é‡å¯¹æ¯”
            bars3 = ax3.bar(model_names, total_detections, color='orange')
            ax3.set_title('Total Detections Comparison')
            ax3.set_ylabel('Detection Count')
            ax3.tick_params(axis='x', rotation=45)
            
            for bar, det_count in zip(bars3, total_detections):
                ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                        f'{det_count}', ha='center', va='bottom')
            
            # å¹³å‡ç½®ä¿¡åº¦å¯¹æ¯”
            bars4 = ax4.bar(model_names, avg_confidences, color='pink')
            ax4.set_title('Average Confidence Comparison')
            ax4.set_ylabel('Confidence')
            ax4.tick_params(axis='x', rotation=45)
            
            for bar, conf_val in zip(bars4, avg_confidences):
                ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{conf_val:.3f}', ha='center', va='bottom')
            
            plt.tight_layout()
            plt.savefig(report_dir / 'performance_comparison.png', dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
    
    def _create_html_report(self, report_dir: Path, timestamp: str):
        """åˆ›å»ºHTMLæµ‹è¯•æŠ¥å‘Š"""
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>YOLOSæ‰‹æŒç‰©ä½“è¯†åˆ«æµ‹è¯•æŠ¥å‘Š</title>
    <style>
        body {{ font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 10px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; color: #333; border-bottom: 2px solid #4CAF50; padding-bottom: 20px; }}
        .section {{ margin: 30px 0; }}
        .stats-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; }}
        .stat-card {{ background: #f9f9f9; padding: 15px; border-radius: 8px; border-left: 4px solid #4CAF50; }}
        .chart-container {{ text-align: center; margin: 20px 0; }}
        .chart-container img {{ max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background-color: #4CAF50; color: white; }}
        .highlight {{ color: #4CAF50; font-weight: bold; }}
        .config-section {{ background: #e8f5e8; padding: 15px; border-radius: 8px; }}
        .emoji {{ font-size: 1.2em; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1><span class="emoji">ğŸ¯</span> YOLOSæ‰‹æŒç‰©ä½“è¯†åˆ«ä¸“ä¸šæµ‹è¯•æŠ¥å‘Š</h1>
            <p>æµ‹è¯•æ—¶é—´: {timestamp}</p>
            <p>æµ‹è¯•æ—¶é•¿: {self.config.test_duration}ç§’ | æ•è·å¸§æ•°: {len(self.test_images)}</p>
        </div>
        
        <div class="section">
            <h2><span class="emoji">ğŸ“‹</span> æµ‹è¯•é…ç½®</h2>
            <div class="config-section">
                <p><strong>æµ‹è¯•æ¨¡å‹:</strong> {', '.join(self.config.models_to_test)}</p>
                <p><strong>ç½®ä¿¡åº¦é˜ˆå€¼:</strong> {self.config.confidence_threshold}</p>
                <p><strong>IoUé˜ˆå€¼:</strong> {self.config.iou_threshold}</p>
                <p><strong>åŠç²¾åº¦æ¨ç†:</strong> {'å¯ç”¨' if self.config.enable_half_precision else 'ç¦ç”¨'}</p>
                <p><strong>æ•è·é—´éš”:</strong> {self.config.capture_interval}ç§’</p>
            </div>
        </div>
        
        <div class="section">
            <h2><span class="emoji">ğŸ“Š</span> æ€§èƒ½ç»Ÿè®¡</h2>
            <div class="stats-grid">
        """
        
        # æ·»åŠ å„æ¨¡å‹ç»Ÿè®¡å¡ç‰‡
        for model_name, stats in self.detection_stats.items():
            html_content += f"""
                <div class="stat-card">
                    <h3><span class="emoji">ğŸ¤–</span> {model_name}</h3>
                    <p><strong>æ€»æ£€æµ‹æ•°:</strong> <span class="highlight">{stats.total_detections}</span></p>
                    <p><strong>å¹³å‡æ¨ç†æ—¶é—´:</strong> <span class="highlight">{stats.avg_inference_time:.3f}s</span></p>
                    <p><strong>FPS:</strong> <span class="highlight">{stats.fps:.1f}</span></p>
                    <p><strong>å¹³å‡ç½®ä¿¡åº¦:</strong> <span class="highlight">{stats.avg_confidence:.3f}</span></p>
                    <p><strong>æ£€æµ‹ç±»åˆ«æ•°:</strong> <span class="highlight">{stats.unique_objects}</span></p>
                </div>
            """
        
        html_content += """
            </div>
        </div>
        """
        
        # å¦‚æœæœ‰æ€§èƒ½å›¾è¡¨ï¼Œæ·»åŠ å®ƒ
        if (report_dir / 'performance_comparison.png').exists():
            html_content += """
        <div class="section">
            <h2><span class="emoji">ğŸ“ˆ</span> æ€§èƒ½å¯¹æ¯”å›¾è¡¨</h2>
            <div class="chart-container">
                <img src="performance_comparison.png" alt="æ€§èƒ½å¯¹æ¯”å›¾è¡¨">
            </div>
        </div>
            """
        
        html_content += """
        <div class="section">
            <h2><span class="emoji">ğŸ”</span> è¯¦ç»†æ£€æµ‹ç»“æœ</h2>
            <table>
                <thead>
                    <tr>
                        <th>æ¨¡å‹</th>
                        <th>æ€»æ£€æµ‹æ•°</th>
                        <th>å¹³å‡æ¨ç†æ—¶é—´</th>
                        <th>FPS</th>
                        <th>å¹³å‡ç½®ä¿¡åº¦</th>
                        <th>ä¸»è¦æ£€æµ‹ç±»åˆ«</th>
                    </tr>
                </thead>
                <tbody>
        """
        
        # æ·»åŠ è¯¦ç»†ç»Ÿè®¡è¡¨æ ¼
        for model_name, stats in self.detection_stats.items():
            top_categories = sorted(stats.object_categories.items(), 
                                  key=lambda x: x[1], reverse=True)[:3]
            top_categories_str = ', '.join([f"{cat}({count})" for cat, count in top_categories])
            
            html_content += f"""
                    <tr>
                        <td>{model_name}</td>
                        <td>{stats.total_detections}</td>
                        <td>{stats.avg_inference_time:.3f}s</td>
                        <td>{stats.fps:.1f}</td>
                        <td>{stats.avg_confidence:.3f}</td>
                        <td>{top_categories_str}</td>
                    </tr>
            """
        
        html_content += """
                </tbody>
            </table>
        </div>
        
        <div class="section">
            <h2><span class="emoji">ğŸ’¡</span> æµ‹è¯•æ€»ç»“</h2>
            <div class="config-section">
        """
        
        # ç”Ÿæˆæµ‹è¯•æ€»ç»“
        if self.detection_stats:
            best_fps_model = max(self.detection_stats.items(), key=lambda x: x[1].fps)
            best_accuracy_model = max(self.detection_stats.items(), key=lambda x: x[1].avg_confidence)
            most_detections_model = max(self.detection_stats.items(), key=lambda x: x[1].total_detections)
            
            html_content += f"""
                <p><strong><span class="emoji">ğŸš€</span> æœ€å¿«æ¨¡å‹:</strong> {best_fps_model[0]} (FPS: {best_fps_model[1].fps:.1f})</p>
                <p><strong><span class="emoji">ğŸ¯</span> æœ€é«˜ç½®ä¿¡åº¦:</strong> {best_accuracy_model[0]} (ç½®ä¿¡åº¦: {best_accuracy_model[1].avg_confidence:.3f})</p>
                <p><strong><span class="emoji">ğŸ”</span> æ£€æµ‹æ•°æœ€å¤š:</strong> {most_detections_model[0]} (æ£€æµ‹æ•°: {most_detections_model[1].total_detections})</p>
            """
        
        html_content += f"""
                <p><strong><span class="emoji">ğŸ“Š</span> æµ‹è¯•å®Œæˆåº¦:</strong> æˆåŠŸæµ‹è¯•äº† {len(self.detectors)} ä¸ªæ¨¡å‹ï¼Œæ•è·äº† {len(self.test_images)} å¸§å›¾åƒ</p>
            </div>
        </div>
        
        <div class="section">
            <p style="text-align: center; color: #666; margin-top: 40px;">
                <span class="emoji">â°</span> æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | 
                <span class="emoji">ğŸ¯</span> YOLOSæ™ºèƒ½è§†é¢‘è¯†åˆ«ç³»ç»Ÿ
            </p>
        </div>
    </div>
</body>
</html>
        """
        
        # ä¿å­˜HTMLæŠ¥å‘Š
        with open(report_dir / 'test_report.html', 'w', encoding='utf-8') as f:
            f.write(html_content)
    
    def _save_raw_data(self, report_dir: Path):
        """ä¿å­˜åŸå§‹æµ‹è¯•æ•°æ®"""
        # ä¿å­˜ç»Ÿè®¡æ•°æ®
        stats_data = {}
        for model_name, stats in self.detection_stats.items():
            stats_data[model_name] = asdict(stats)
        
        with open(report_dir / 'detection_stats.json', 'w', encoding='utf-8') as f:
            json.dump(stats_data, f, ensure_ascii=False, indent=2, default=str)
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        with open(report_dir / 'test_results.json', 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, ensure_ascii=False, indent=2, default=str)
        
        # ä¿å­˜æµ‹è¯•é…ç½®
        with open(report_dir / 'test_config.json', 'w', encoding='utf-8') as f:
            json.dump(asdict(self.config), f, ensure_ascii=False, indent=2)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ YOLOSæ‰‹æŒé™æ€ç‰©ä½“è¯†åˆ«ä¸“ä¸šæµ‹è¯•")
    print("="*80)
    
    # æ£€æŸ¥åŸºæœ¬ä¾èµ–
    if not ULTRALYTICS_AVAILABLE:
        print("âŒ Ultralytics YOLO ä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install ultralytics")
        return
    
    # æ£€æŸ¥æ‘„åƒå¤´
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´ï¼Œè¯·æ£€æŸ¥æ‘„åƒå¤´è¿æ¥")
        return
    cap.release()
    
    print("âœ… ç³»ç»Ÿæ£€æŸ¥é€šè¿‡")
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    config = TestConfig(
        test_duration=90,  # 90ç§’æµ‹è¯•
        capture_interval=4.0,  # æ¯4ç§’è‡ªåŠ¨æ•è·
        models_to_test=['yolov8n', 'yolov8s'],  # ä½¿ç”¨å¯é çš„æ¨¡å‹
        confidence_threshold=0.25,
        enable_half_precision=True,
        save_images=True,
        save_video=True,
        generate_report=True
    )
    
    # åˆ›å»ºæµ‹è¯•ç³»ç»Ÿ
    test_system = HandheldObjectRecognitionTest(config)
    
    try:
        # è¿è¡Œæµ‹è¯•
        success = test_system.run_comprehensive_test()
        
        if success:
            print("\nâœ… æµ‹è¯•å®Œæˆï¼")
            print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {test_system.output_dir}")
            print("ğŸ“‹ è¯·æŸ¥çœ‹ç”Ÿæˆçš„HTMLæŠ¥å‘Šè·å–è¯¦ç»†åˆ†æç»“æœ")
        else:
            print("\nâŒ æµ‹è¯•æœªèƒ½å®Œæˆ")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()