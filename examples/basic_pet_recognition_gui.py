#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºç¡€å® ç‰©è¯†åˆ«GUI - ä½¿ç”¨OpenCVçº§è”åˆ†ç±»å™¨å’Œé¢œè‰²æ£€æµ‹
ä¸ä¾èµ–YOLOï¼Œé€‚ç”¨äºç½‘ç»œå—é™ç¯å¢ƒ
"""

import cv2
import numpy as np
import time
import json
import logging
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
import sys
import os

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pet_recognition.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class BasicPetRecognizer:
    """åŸºç¡€å® ç‰©è¯†åˆ«å™¨ - ä½¿ç”¨ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰æ–¹æ³•"""
    
    def __init__(self):
        # å°è¯•åŠ è½½Haarçº§è”åˆ†ç±»å™¨
        self.face_cascade = None
        self.body_cascade = None
        
        # åŠ è½½çº§è”åˆ†ç±»å™¨
        self.load_cascades()
        
        # é¢œè‰²èŒƒå›´å®šä¹‰ (HSV)
        self.color_ranges = {
            'brown': [(10, 50, 20), (20, 255, 200)],    # æ£•è‰² (ç‹—ã€çŒ«å¸¸è§)
            'black': [(0, 0, 0), (180, 255, 30)],       # é»‘è‰²
            'white': [(0, 0, 200), (180, 30, 255)],     # ç™½è‰²
            'gray': [(0, 0, 50), (180, 30, 200)],       # ç°è‰²
            'orange': [(5, 50, 50), (15, 255, 255)],    # æ©™è‰² (æ©˜çŒ«)
            'yellow': [(20, 50, 50), (30, 255, 255)]    # é»„è‰² (é‡‘æ¯›ç­‰)
        }
        
        # è¿åŠ¨æ£€æµ‹
        self.bg_subtractor = cv2.createBackgroundSubtractorMOG2(detectShadows=True)
        self.motion_threshold = 500  # è¿åŠ¨åŒºåŸŸæœ€å°é¢ç§¯
        
        # æ£€æµ‹å†å²
        self.detection_history = []
        self.max_history = 10
    
    def load_cascades(self):
        """åŠ è½½Haarçº§è”åˆ†ç±»å™¨"""
        try:
            # å°è¯•åŠ è½½äººè„¸æ£€æµ‹å™¨ (å¯èƒ½æ£€æµ‹åˆ°å® ç‰©è„¸éƒ¨)
            face_cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            if os.path.exists(face_cascade_path):
                self.face_cascade = cv2.CascadeClassifier(face_cascade_path)
                logger.info("Face cascade loaded successfully")
            
            # å°è¯•åŠ è½½å…¨èº«æ£€æµ‹å™¨
            body_cascade_path = cv2.data.haarcascades + 'haarcascade_fullbody.xml'
            if os.path.exists(body_cascade_path):
                self.body_cascade = cv2.CascadeClassifier(body_cascade_path)
                logger.info("Body cascade loaded successfully")
                
        except Exception as e:
            logger.warning(f"Failed to load cascades: {e}")
    
    def detect_by_color(self, frame: np.ndarray) -> List[Dict]:
        """åŸºäºé¢œè‰²çš„æ£€æµ‹"""
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        detections = []
        
        for color_name, (lower, upper) in self.color_ranges.items():
            # åˆ›å»ºé¢œè‰²æ©ç 
            lower = np.array(lower)
            upper = np.array(upper)
            mask = cv2.inRange(hsv, lower, upper)
            
            # å½¢æ€å­¦æ“ä½œæ¸…ç†æ©ç 
            kernel = np.ones((5, 5), np.uint8)
            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
            
            # æŸ¥æ‰¾è½®å»“
            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            for contour in contours:
                area = cv2.contourArea(contour)
                if area > 1000:  # æœ€å°é¢ç§¯é˜ˆå€¼
                    # è·å–è¾¹ç•Œæ¡†
                    x, y, w, h = cv2.boundingRect(contour)
                    
                    # è®¡ç®—å½¢çŠ¶ç‰¹å¾
                    aspect_ratio = w / h
                    extent = area / (w * h)
                    
                    # åŸºäºå½¢çŠ¶ç‰¹å¾æ¨æ–­å¯èƒ½çš„å® ç‰©ç±»å‹
                    pet_type = self.classify_by_shape(aspect_ratio, extent, area)
                    
                    detection = {
                        'type': 'color_based',
                        'species': pet_type,
                        'color': color_name,
                        'confidence': min(0.8, extent + 0.2),  # åŸºäºå½¢çŠ¶åŒ¹é…åº¦çš„ç½®ä¿¡åº¦
                        'bbox': (x, y, w, h),
                        'area': area,
                        'aspect_ratio': aspect_ratio,
                        'extent': extent
                    }
                    
                    detections.append(detection)
        
        return detections
    
    def detect_by_motion(self, frame: np.ndarray) -> List[Dict]:
        """åŸºäºè¿åŠ¨çš„æ£€æµ‹"""
        # èƒŒæ™¯å‡é™¤
        fg_mask = self.bg_subtractor.apply(frame)
        
        # å½¢æ€å­¦æ“ä½œ
        kernel = np.ones((5, 5), np.uint8)
        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, kernel)
        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)
        
        # æŸ¥æ‰¾è¿åŠ¨åŒºåŸŸ
        contours, _ = cv2.findContours(fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        detections = []
        for contour in contours:
            area = cv2.contourArea(contour)
            if area > self.motion_threshold:
                x, y, w, h = cv2.boundingRect(contour)
                
                # åˆ†æè¿åŠ¨æ¨¡å¼
                motion_type = self.analyze_motion_pattern(x, y, w, h, area)
                
                detection = {
                    'type': 'motion_based',
                    'species': 'moving_object',
                    'motion_pattern': motion_type,
                    'confidence': min(0.7, area / 5000),  # åŸºäºè¿åŠ¨åŒºåŸŸå¤§å°
                    'bbox': (x, y, w, h),
                    'area': area
                }
                
                detections.append(detection)
        
        return detections
    
    def detect_by_cascade(self, frame: np.ndarray) -> List[Dict]:
        """ä½¿ç”¨çº§è”åˆ†ç±»å™¨æ£€æµ‹"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        detections = []
        
        # äººè„¸æ£€æµ‹ (å¯èƒ½æ£€æµ‹åˆ°å® ç‰©è„¸éƒ¨)
        if self.face_cascade is not None:
            faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
            for (x, y, w, h) in faces:
                detection = {
                    'type': 'cascade_face',
                    'species': 'pet_face',
                    'confidence': 0.6,
                    'bbox': (x, y, w, h),
                    'area': w * h
                }
                detections.append(detection)
        
        # å…¨èº«æ£€æµ‹
        if self.body_cascade is not None:
            bodies = self.body_cascade.detectMultiScale(gray, 1.1, 4)
            for (x, y, w, h) in bodies:
                detection = {
                    'type': 'cascade_body',
                    'species': 'pet_body',
                    'confidence': 0.7,
                    'bbox': (x, y, w, h),
                    'area': w * h
                }
                detections.append(detection)
        
        return detections
    
    def classify_by_shape(self, aspect_ratio: float, extent: float, area: int) -> str:
        """åŸºäºå½¢çŠ¶ç‰¹å¾åˆ†ç±»å® ç‰©ç±»å‹"""
        # ç®€å•çš„å½¢çŠ¶åˆ†ç±»è§„åˆ™
        if aspect_ratio > 1.5:  # é•¿æ¡å½¢
            if area > 5000:
                return "dog_lying"
            else:
                return "cat_lying"
        elif aspect_ratio < 0.7:  # é«˜ç˜¦å½¢
            if area > 3000:
                return "dog_sitting"
            else:
                return "cat_sitting"
        else:  # æ¥è¿‘æ­£æ–¹å½¢
            if extent > 0.7:  # å¡«å……åº¦é«˜
                if area > 4000:
                    return "dog"
                else:
                    return "cat"
            else:
                return "bird_or_small_pet"
    
    def analyze_motion_pattern(self, x: int, y: int, w: int, h: int, area: int) -> str:
        """åˆ†æè¿åŠ¨æ¨¡å¼"""
        aspect_ratio = w / h
        
        if aspect_ratio > 2.0:
            return "horizontal_movement"  # æ°´å¹³ç§»åŠ¨ (å¯èƒ½æ˜¯è·‘åŠ¨)
        elif aspect_ratio < 0.5:
            return "vertical_movement"    # å‚ç›´ç§»åŠ¨ (å¯èƒ½æ˜¯è·³è·ƒ)
        elif area > 3000:
            return "large_movement"       # å¤§èŒƒå›´ç§»åŠ¨
        else:
            return "small_movement"       # å°èŒƒå›´ç§»åŠ¨
    
    def detect_pets(self, frame: np.ndarray) -> List[Dict]:
        """ç»¼åˆæ£€æµ‹æ–¹æ³•"""
        all_detections = []
        
        # é¢œè‰²æ£€æµ‹
        color_detections = self.detect_by_color(frame)
        all_detections.extend(color_detections)
        
        # è¿åŠ¨æ£€æµ‹
        motion_detections = self.detect_by_motion(frame)
        all_detections.extend(motion_detections)
        
        # çº§è”åˆ†ç±»å™¨æ£€æµ‹
        cascade_detections = self.detect_by_cascade(frame)
        all_detections.extend(cascade_detections)
        
        # å»é‡å’Œèåˆ
        filtered_detections = self.filter_and_merge_detections(all_detections)
        
        # æ›´æ–°æ£€æµ‹å†å²
        self.update_detection_history(filtered_detections)
        
        return filtered_detections
    
    def filter_and_merge_detections(self, detections: List[Dict]) -> List[Dict]:
        """è¿‡æ»¤å’Œåˆå¹¶é‡å çš„æ£€æµ‹"""
        if not detections:
            return []
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        detections.sort(key=lambda x: x['confidence'], reverse=True)
        
        filtered = []
        for detection in detections:
            # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰æ£€æµ‹é‡å 
            is_duplicate = False
            for existing in filtered:
                if self.calculate_iou(detection['bbox'], existing['bbox']) > 0.3:
                    is_duplicate = True
                    # å¦‚æœæ–°æ£€æµ‹ç½®ä¿¡åº¦æ›´é«˜ï¼Œæ›¿æ¢
                    if detection['confidence'] > existing['confidence']:
                        filtered.remove(existing)
                        filtered.append(detection)
                    break
            
            if not is_duplicate:
                filtered.append(detection)
        
        return filtered
    
    def calculate_iou(self, box1: Tuple[int, int, int, int], box2: Tuple[int, int, int, int]) -> float:
        """è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„IoU"""
        x1, y1, w1, h1 = box1
        x2, y2, w2, h2 = box2
        
        # è®¡ç®—äº¤é›†
        xi1 = max(x1, x2)
        yi1 = max(y1, y2)
        xi2 = min(x1 + w1, x2 + w2)
        yi2 = min(y1 + h1, y2 + h2)
        
        if xi2 <= xi1 or yi2 <= yi1:
            return 0.0
        
        inter_area = (xi2 - xi1) * (yi2 - yi1)
        box1_area = w1 * h1
        box2_area = w2 * h2
        union_area = box1_area + box2_area - inter_area
        
        return inter_area / union_area if union_area > 0 else 0.0
    
    def update_detection_history(self, detections: List[Dict]):
        """æ›´æ–°æ£€æµ‹å†å²"""
        self.detection_history.append(len(detections))
        if len(self.detection_history) > self.max_history:
            self.detection_history.pop(0)


class BasicPetRecognitionGUI:
    """åŸºç¡€å® ç‰©è¯†åˆ«GUI"""
    
    def __init__(self):
        # æ‘„åƒå¤´ç›¸å…³
        self.cap = None
        self.frame_width = 640
        self.frame_height = 480
        
        # è¯†åˆ«å™¨
        self.pet_recognizer = BasicPetRecognizer()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.frame_count = 0
        self.start_time = time.time()
        self.fps = 0
        
        # æ£€æµ‹æ§åˆ¶
        self.detection_interval = 5  # æ¯5å¸§æ£€æµ‹ä¸€æ¬¡
        self.display_interval = 2    # æ¯2å¸§æ›´æ–°æ˜¾ç¤º
        self.result_hold_frames = 20 # ç»“æœä¿æŒ20å¸§
        
        # ç»“æœç¼“å­˜
        self.cached_results = []
        self.result_cache_time = 0
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_detections': 0,
            'detection_types': {},
            'session_start': time.time()
        }
        
        self.initialize_camera()
    
    def initialize_camera(self) -> bool:
        """åˆå§‹åŒ–æ‘„åƒå¤´ - ä¼˜å…ˆä½¿ç”¨å†…ç½®æ‘„åƒå¤´"""
        # é¦–å…ˆå°è¯•å†…ç½®æ‘„åƒå¤´ (index 0)
        logger.info("Trying builtin camera (index 0)")
        cap = cv2.VideoCapture(0)
        if cap.isOpened():
            ret, frame = cap.read()
            if ret and frame is not None:
                self.cap = cap
                logger.info(f"Builtin camera OK: {frame.shape}")
            else:
                cap.release()
        
        # å¦‚æœå†…ç½®æ‘„åƒå¤´å¤±è´¥ï¼Œå°è¯•å¤–éƒ¨æ‘„åƒå¤´
        if self.cap is None:
            for index in [1, 2, 3, 4]:
                logger.info(f"Trying camera index {index}")
                cap = cv2.VideoCapture(index)
                if cap.isOpened():
                    ret, frame = cap.read()
                    if ret and frame is not None:
                        self.cap = cap
                        logger.info(f"Camera {index} OK: {frame.shape}")
                        break
                    else:
                        cap.release()
                else:
                    cap.release()
        
        if self.cap is None:
            logger.error("No camera available")
            return False
        
        # è®¾ç½®æ‘„åƒå¤´å‚æ•°
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        
        # è·å–å®é™…å°ºå¯¸
        self.frame_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.frame_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        logger.info(f"Camera setup complete: {self.frame_width}x{self.frame_height}")
        return True
    
    def draw_info_panel(self, frame: np.ndarray, results: List[Dict]) -> np.ndarray:
        """ç»˜åˆ¶ä¿¡æ¯é¢æ¿"""
        # å·¦ä¸Šè§’ä¿¡æ¯é¢æ¿
        panel_width = 200
        panel_height = 80
        
        # åŠé€æ˜èƒŒæ™¯
        overlay = frame.copy()
        cv2.rectangle(overlay, (5, 5), (panel_width, panel_height), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # ç»Ÿè®¡ä¿¡æ¯
        detection_types = set(r['type'] for r in results)
        avg_detections = np.mean(self.pet_recognizer.detection_history) if self.pet_recognizer.detection_history else 0
        
        # æ˜¾ç¤ºä¿¡æ¯
        font_scale = 0.4
        thickness = 1
        y_offset = 18
        
        cv2.putText(frame, f"Detections: {len(results)}", 
                   (8, y_offset), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), thickness)
        
        cv2.putText(frame, f"Methods: {len(detection_types)}", 
                   (8, y_offset + 15), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), thickness)
        
        cv2.putText(frame, f"Avg: {avg_detections:.1f}", 
                   (8, y_offset + 30), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), thickness)
        
        cv2.putText(frame, f"FPS: {self.fps:.1f} | Frame: {self.frame_count}", 
                   (8, y_offset + 45), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), thickness)
        
        return frame
    
    def draw_legend(self, frame: np.ndarray) -> np.ndarray:
        """ç»˜åˆ¶å›¾ä¾‹"""
        # åº•éƒ¨å›¾ä¾‹
        legend_height = 30
        legend_y = self.frame_height - legend_height
        
        # åŠé€æ˜èƒŒæ™¯
        overlay = frame.copy()
        cv2.rectangle(overlay, (0, legend_y), (self.frame_width, self.frame_height), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)
        
        # å›¾ä¾‹æ–‡æœ¬
        legend_text = "Detection: ColorğŸ¨ MotionğŸƒ ShapeğŸ“ | Colors: BrownğŸ¤ Blackâš« Whiteâšª GrayğŸ”˜"
        font_scale = 0.35
        thickness = 1
        
        cv2.putText(frame, legend_text, (5, legend_y + 20), 
                   cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), thickness)
        
        return frame
    
    def draw_detections(self, frame: np.ndarray, results: List[Dict]) -> np.ndarray:
        """ç»˜åˆ¶æ£€æµ‹ç»“æœ"""
        for detection in results:
            detection_type = detection['type']
            species = detection['species']
            confidence = detection['confidence']
            x, y, w, h = detection['bbox']
            
            # æ ¹æ®æ£€æµ‹ç±»å‹é€‰æ‹©é¢œè‰²
            if detection_type == 'color_based':
                color = (0, 255, 255)  # é»„è‰²
            elif detection_type == 'motion_based':
                color = (255, 0, 255)  # æ´‹çº¢
            elif detection_type.startswith('cascade'):
                color = (0, 255, 0)    # ç»¿è‰²
            else:
                color = (128, 128, 128)  # ç°è‰²
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
            
            # ç»˜åˆ¶æ ‡ç­¾
            label_parts = [species, f"{confidence:.2f}"]
            
            if 'color' in detection:
                label_parts.append(detection['color'])
            
            if 'motion_pattern' in detection:
                label_parts.append(detection['motion_pattern'])
            
            label = " ".join(label_parts)
            
            # æ ‡ç­¾èƒŒæ™¯
            (label_w, label_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)
            cv2.rectangle(frame, (x, y - label_h - 8), (x + label_w + 4, y), color, -1)
            cv2.putText(frame, label, (x + 2, y - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
            
            # ç»˜åˆ¶æ£€æµ‹ç±»å‹æ ‡è¯†
            type_text = detection_type.replace('_', ' ').title()
            cv2.putText(frame, type_text, (x, y + h + 15), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.3, color, 1)
            
            # ç»˜åˆ¶ä¸­å¿ƒç‚¹
            center_x = x + w // 2
            center_y = y + h // 2
            cv2.circle(frame, (center_x, center_y), 3, color, -1)
        
        return frame
    
    def log_detections(self, results: List[Dict]):
        """è®°å½•æ£€æµ‹ç»“æœ"""
        for detection in results:
            detection_type = detection['type']
            species = detection['species']
            confidence = detection['confidence']
            
            log_msg = f"[PET DETECTED] Type: {detection_type}, Species: {species}, Confidence: {confidence:.3f}"
            
            if 'color' in detection:
                log_msg += f", Color: {detection['color']}"
            
            if 'motion_pattern' in detection:
                log_msg += f", Motion: {detection['motion_pattern']}"
            
            logger.info(log_msg)
            print(f"ğŸ¾ {log_msg}")
            
            # æ›´æ–°ç»Ÿè®¡
            if detection_type not in self.stats['detection_types']:
                self.stats['detection_types'][detection_type] = 0
            self.stats['detection_types'][detection_type] += 1
            self.stats['total_detections'] += 1
    
    def run(self):
        """è¿è¡Œä¸»å¾ªç¯"""
        if not self.cap:
            logger.error("Camera not initialized")
            return
        
        logger.info("Starting basic pet recognition GUI...")
        logger.info("Detection methods: Color analysis, Motion detection, Shape recognition")
        logger.info("Controls: 'q' or ESC to quit, 's' to save screenshot, 'r' to reset stats")
        
        # ç¨³å®šåŒ–ç­‰å¾…
        logger.info("Camera stabilizing... (30 frames)")
        for i in range(30):
            ret, frame = self.cap.read()
            if not ret:
                logger.error("Failed to read frame during stabilization")
                return
            cv2.imshow('Pet Recognition - Stabilizing...', frame)
            cv2.waitKey(1)
        
        try:
            while True:
                ret, frame = self.cap.read()
                if not ret:
                    logger.error("Failed to read frame from camera")
                    break
                
                self.frame_count += 1
                current_time = time.time()
                
                # è®¡ç®—FPS
                if self.frame_count % 30 == 0:
                    elapsed = current_time - self.start_time
                    self.fps = self.frame_count / elapsed if elapsed > 0 else 0
                
                # æ£€æµ‹æ§åˆ¶
                should_detect = (self.frame_count % self.detection_interval == 0)
                should_update_display = (self.frame_count % self.display_interval == 0)
                
                # æ‰§è¡Œæ£€æµ‹
                if should_detect:
                    results = self.pet_recognizer.detect_pets(frame)
                    
                    if results:
                        self.cached_results = results
                        self.result_cache_time = self.frame_count
                        self.log_detections(results)
                
                # ä½¿ç”¨ç¼“å­˜ç»“æœ
                display_results = []
                if self.cached_results and (self.frame_count - self.result_cache_time) < self.result_hold_frames:
                    display_results = self.cached_results
                
                # ç»˜åˆ¶ç»“æœ
                if display_results and should_update_display:
                    frame = self.draw_detections(frame, display_results)
                
                # ç»˜åˆ¶ç•Œé¢å…ƒç´ 
                frame = self.draw_info_panel(frame, display_results)
                frame = self.draw_legend(frame)
                
                # æ˜¾ç¤ºç”»é¢
                cv2.imshow('Basic Pet Recognition - Traditional CV Methods', frame)
                
                # å¤„ç†æŒ‰é”®
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q') or key == 27:  # 'q' or ESC
                    break
                elif key == ord('s'):  # ä¿å­˜æˆªå›¾
                    timestamp = time.strftime("%Y%m%d_%H%M%S")
                    filename = f"basic_pet_screenshot_{timestamp}.jpg"
                    cv2.imwrite(filename, frame)
                    logger.info(f"Screenshot saved: {filename}")
                    print(f"ğŸ“¸ Screenshot saved: {filename}")
                elif key == ord('r'):  # é‡ç½®ç»Ÿè®¡
                    self.stats = {
                        'total_detections': 0,
                        'detection_types': {},
                        'session_start': time.time()
                    }
                    self.pet_recognizer.detection_history = []
                    logger.info("Statistics reset")
                    print("ğŸ“Š Statistics reset")
        
        except KeyboardInterrupt:
            logger.info("Interrupted by user")
        except Exception as e:
            logger.error(f"Error in main loop: {e}")
        finally:
            self.cleanup()
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("Cleaning up resources...")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        session_duration = time.time() - self.stats['session_start']
        
        stats_text = f"""
ğŸ¾ Basic Pet Recognition Session Summary
========================================
Duration: {session_duration:.1f} seconds
Total Frames: {self.frame_count}
Average FPS: {self.fps:.1f}
Total Detections: {self.stats['total_detections']}

Detection Methods:
"""
        
        for method, count in self.stats['detection_types'].items():
            stats_text += f"  {method.replace('_', ' ').title()}: {count} detections\n"
        
        print(stats_text)
        logger.info("Session completed")
        
        # é‡Šæ”¾æ‘„åƒå¤´
        if self.cap:
            self.cap.release()
        
        # å…³é—­çª—å£
        cv2.destroyAllWindows()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¾ Basic Pet Recognition System")
    print("Using Traditional Computer Vision Methods")
    print("=" * 45)
    print("Features:")
    print("  ğŸ¨ Color-based detection")
    print("  ğŸƒ Motion analysis")
    print("  ğŸ“ Shape classification")
    print("  ğŸ” Cascade classifiers")
    print()
    
    try:
        gui = BasicPetRecognitionGUI()
        gui.run()
        
    except Exception as e:
        logger.error(f"Failed to start pet recognition system: {e}")
        print(f"âŒ Error: {e}")


if __name__ == "__main__":
    main()