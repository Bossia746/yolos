#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–é«˜æ€§èƒ½å¤šæ¨¡æ€è¯†åˆ«GUIç³»ç»Ÿ
åŸºäºæœ€ä½³å®è·µï¼Œè§£å†³å¡é¡¿å’Œå¤šæ‰‹æ£€æµ‹é—®é¢˜
é¿å…å¤æ‚ä¾èµ–å¯¼å…¥é—®é¢˜

å‚è€ƒæœ€ä½³å®è·µ:
- OpenCVå¤šçº¿ç¨‹ä¼˜åŒ–
- MediaPipeæ€§èƒ½ä¼˜åŒ–
- GUIæ€§èƒ½ä¼˜åŒ–
"""

import cv2
import numpy as np
import threading
import queue
import time
import logging
from typing import Optional, Tuple, List, Dict, Any
from dataclasses import dataclass
from collections import deque
import mediapipe as mp
from concurrent.futures import ThreadPoolExecutor
import sys
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('simple_performance_gui.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class FrameData:
    """å¸§æ•°æ®ç»“æ„"""
    frame: np.ndarray
    timestamp: float
    frame_id: int

@dataclass
class DetectionResult:
    """æ£€æµ‹ç»“æœç»“æ„"""
    frame: np.ndarray
    hands: List[Dict[str, Any]]
    faces: List[Dict[str, Any]]
    poses: List[Dict[str, Any]]
    fps: float
    processing_time: float
    frame_id: int

class ThreadSafeCamera:
    """çº¿ç¨‹å®‰å…¨çš„æ‘„åƒå¤´ç±»"""
    
    def __init__(self, camera_id: int = 0, buffer_size: int = 2):
        self.camera_id = camera_id
        self.buffer_size = buffer_size
        self.cap = None
        self.frame_queue = queue.Queue(maxsize=buffer_size)
        self.running = False
        self.capture_thread = None
        self.frame_count = 0
        self.fps_counter = deque(maxlen=30)
        
    def start(self) -> bool:
        """å¯åŠ¨æ‘„åƒå¤´æ•è·"""
        try:
            self.cap = cv2.VideoCapture(self.camera_id)
            if not self.cap.isOpened():
                logger.error(f"æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {self.camera_id}")
                return False
                
            # ä¼˜åŒ–æ‘„åƒå¤´è®¾ç½®
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
            self.cap.set(cv2.CAP_PROP_FPS, 30)
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # å‡å°‘ç¼“å†²åŒºå»¶è¿Ÿ
            
            self.running = True
            self.capture_thread = threading.Thread(target=self._capture_frames, daemon=True)
            self.capture_thread.start()
            
            logger.info(f"ğŸ“¹ æ‘„åƒå¤´ {self.camera_id} å¯åŠ¨æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"æ‘„åƒå¤´å¯åŠ¨å¤±è´¥: {e}")
            return False
    
    def _capture_frames(self):
        """å¸§æ•è·çº¿ç¨‹"""
        last_time = time.time()
        
        while self.running:
            try:
                ret, frame = self.cap.read()
                if not ret:
                    logger.warning("æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
                    continue
                
                current_time = time.time()
                self.fps_counter.append(current_time - last_time)
                last_time = current_time
                
                frame_data = FrameData(
                    frame=frame,
                    timestamp=current_time,
                    frame_id=self.frame_count
                )
                
                # éé˜»å¡æ”¾å…¥é˜Ÿåˆ—
                try:
                    self.frame_queue.put_nowait(frame_data)
                    self.frame_count += 1
                except queue.Full:
                    # é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€æ—§çš„å¸§
                    try:
                        self.frame_queue.get_nowait()
                        self.frame_queue.put_nowait(frame_data)
                    except queue.Empty:
                        pass
                        
            except Exception as e:
                logger.error(f"å¸§æ•è·é”™è¯¯: {e}")
                time.sleep(0.01)
    
    def get_frame(self) -> Optional[FrameData]:
        """è·å–æœ€æ–°å¸§"""
        try:
            return self.frame_queue.get_nowait()
        except queue.Empty:
            return None
    
    def get_fps(self) -> float:
        """è·å–å®é™…FPS"""
        if len(self.fps_counter) < 2:
            return 0.0
        avg_interval = sum(self.fps_counter) / len(self.fps_counter)
        return 1.0 / avg_interval if avg_interval > 0 else 0.0
    
    def stop(self):
        """åœæ­¢æ‘„åƒå¤´"""
        self.running = False
        if self.capture_thread:
            self.capture_thread.join(timeout=1.0)
        if self.cap:
            self.cap.release()
        logger.info("æ‘„åƒå¤´å·²åœæ­¢")

class SimpleMultiModalDetector:
    """ç®€åŒ–å¤šæ¨¡æ€æ£€æµ‹å™¨ - ç›´æ¥ä½¿ç”¨MediaPipe"""
    
    def __init__(self):
        self.mp_hands = mp.solutions.hands
        self.mp_face = mp.solutions.face_detection
        self.mp_pose = mp.solutions.pose
        self.mp_drawing = mp.solutions.drawing_utils
        
        self.hands = None
        self.face_detection = None
        self.pose = None
        
        self.executor = ThreadPoolExecutor(max_workers=3)
        self.detection_stats = {
            'total_detections': 0,
            'avg_processing_time': 0.0,
            'last_fps': 0.0
        }
        
    def initialize(self) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰æ£€æµ‹å™¨"""
        try:
            logger.info("ğŸ”§ åˆå§‹åŒ–ç®€åŒ–å¤šæ¨¡æ€æ£€æµ‹å™¨...")
            
            # åˆå§‹åŒ–æ‰‹åŠ¿æ£€æµ‹å™¨ - ä¼˜åŒ–é…ç½®
            self.hands = self.mp_hands.Hands(
                static_image_mode=False,
                max_num_hands=4,  # æ”¯æŒå¤šæ‰‹æ£€æµ‹
                min_detection_confidence=0.6,  # é™ä½é˜ˆå€¼æé«˜æ£€æµ‹ç‡
                min_tracking_confidence=0.4,   # é™ä½è·Ÿè¸ªé˜ˆå€¼
                model_complexity=0  # ä½¿ç”¨è½»é‡çº§æ¨¡å‹
            )
            
            # åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨
            self.face_detection = self.mp_face.FaceDetection(
                model_selection=0,  # çŸ­è·ç¦»æ¨¡å‹
                min_detection_confidence=0.6
            )
            
            # åˆå§‹åŒ–å§¿åŠ¿æ£€æµ‹å™¨
            self.pose = self.mp_pose.Pose(
                static_image_mode=False,
                model_complexity=0,  # è½»é‡çº§æ¨¡å‹
                enable_segmentation=False,
                min_detection_confidence=0.6,
                min_tracking_confidence=0.4
            )
            
            logger.info("âœ… ç®€åŒ–å¤šæ¨¡æ€æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def detect_parallel(self, frame: np.ndarray) -> DetectionResult:
        """å¹¶è¡Œæ£€æµ‹"""
        start_time = time.time()
        
        # è½¬æ¢é¢œè‰²ç©ºé—´
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # æäº¤å¹¶è¡Œä»»åŠ¡
        hand_future = self.executor.submit(self._detect_hands, rgb_frame)
        face_future = self.executor.submit(self._detect_faces, rgb_frame)
        pose_future = self.executor.submit(self._detect_poses, rgb_frame)
        
        # æ”¶é›†ç»“æœ
        try:
            hands = hand_future.result(timeout=0.1)  # 100msè¶…æ—¶
        except:
            hands = []
            
        try:
            faces = face_future.result(timeout=0.1)
        except:
            faces = []
            
        try:
            poses = pose_future.result(timeout=0.1)
        except:
            poses = []
        
        processing_time = time.time() - start_time
        fps = 1.0 / processing_time if processing_time > 0 else 0.0
        
        # æ›´æ–°ç»Ÿè®¡
        self._update_stats(processing_time, fps)
        
        return DetectionResult(
            frame=frame,
            hands=hands,
            faces=faces,
            poses=poses,
            fps=fps,
            processing_time=processing_time,
            frame_id=0
        )
    
    def _detect_hands(self, rgb_frame: np.ndarray) -> List[Dict[str, Any]]:
        """æ‰‹åŠ¿æ£€æµ‹"""
        try:
            if self.hands:
                results = self.hands.process(rgb_frame)
                hands_data = []
                
                if results.multi_hand_landmarks and results.multi_handedness:
                    for i, (hand_landmarks, handedness) in enumerate(zip(results.multi_hand_landmarks, results.multi_handedness)):
                        # æå–å…³é”®ç‚¹
                        landmarks = []
                        h, w = rgb_frame.shape[:2]
                        for lm in hand_landmarks.landmark:
                            landmarks.append((int(lm.x * w), int(lm.y * h)))
                        
                        # è®¡ç®—è¾¹ç•Œæ¡†
                        x_coords = [lm[0] for lm in landmarks]
                        y_coords = [lm[1] for lm in landmarks]
                        bbox = (min(x_coords), min(y_coords), max(x_coords), max(y_coords))
                        
                        # ç®€å•æ‰‹åŠ¿è¯†åˆ«
                        gesture = self._simple_gesture_recognition(landmarks)
                        
                        hands_data.append({
                            'type': 'hand',
                            'landmarks': landmarks,
                            'bbox': bbox,
                            'handedness': handedness.classification[0].label,
                            'handedness_confidence': handedness.classification[0].score,
                            'gesture': gesture,
                            'confidence': handedness.classification[0].score
                        })
                
                return hands_data
        except Exception as e:
            logger.debug(f"æ‰‹åŠ¿æ£€æµ‹é”™è¯¯: {e}")
        return []
    
    def _simple_gesture_recognition(self, landmarks: List[Tuple[int, int]]) -> Dict[str, Any]:
        """ç®€å•æ‰‹åŠ¿è¯†åˆ«"""
        try:
            # åŸºäºå…³é”®ç‚¹ä½ç½®çš„ç®€å•æ‰‹åŠ¿è¯†åˆ«
            # æ‹‡æŒ‡å°–ã€é£ŸæŒ‡å°–ã€ä¸­æŒ‡å°–ã€æ— åæŒ‡å°–ã€å°æŒ‡å°–
            thumb_tip = landmarks[4]
            index_tip = landmarks[8]
            middle_tip = landmarks[12]
            ring_tip = landmarks[16]
            pinky_tip = landmarks[20]
            
            # æ‰‹è…•
            wrist = landmarks[0]
            
            # è®¡ç®—æ‰‹æŒ‡æ˜¯å¦ä¼¸å±•
            fingers_up = []
            
            # æ‹‡æŒ‡ (æ¯”è¾ƒxåæ ‡)
            if thumb_tip[0] > landmarks[3][0]:  # æ‹‡æŒ‡å°– > æ‹‡æŒ‡å…³èŠ‚
                fingers_up.append(1)
            else:
                fingers_up.append(0)
            
            # å…¶ä»–å››æŒ‡ (æ¯”è¾ƒyåæ ‡)
            for tip_id in [8, 12, 16, 20]:
                if landmarks[tip_id][1] < landmarks[tip_id - 2][1]:  # æŒ‡å°– < å…³èŠ‚
                    fingers_up.append(1)
                else:
                    fingers_up.append(0)
            
            total_fingers = sum(fingers_up)
            
            # ç®€å•æ‰‹åŠ¿åˆ†ç±»
            if total_fingers == 0:
                gesture = "Fist"
                confidence = 0.8
            elif total_fingers == 5:
                gesture = "Open Palm"
                confidence = 0.8
            elif total_fingers == 1 and fingers_up[1] == 1:  # åªæœ‰é£ŸæŒ‡
                gesture = "Pointing"
                confidence = 0.7
            elif total_fingers == 2 and fingers_up[1] == 1 and fingers_up[2] == 1:  # é£ŸæŒ‡å’Œä¸­æŒ‡
                gesture = "Peace Sign"
                confidence = 0.7
            elif total_fingers == 1 and fingers_up[0] == 1:  # åªæœ‰æ‹‡æŒ‡
                gesture = "Thumbs Up"
                confidence = 0.7
            else:
                gesture = f"{total_fingers} Fingers"
                confidence = 0.5
            
            return {
                'gesture': gesture,
                'confidence': confidence,
                'fingers_up': fingers_up,
                'total_fingers': total_fingers
            }
            
        except Exception as e:
            logger.debug(f"æ‰‹åŠ¿è¯†åˆ«é”™è¯¯: {e}")
            return {'gesture': 'Unknown', 'confidence': 0.0}
    
    def _detect_faces(self, rgb_frame: np.ndarray) -> List[Dict[str, Any]]:
        """äººè„¸æ£€æµ‹"""
        try:
            if self.face_detection:
                results = self.face_detection.process(rgb_frame)
                faces_data = []
                
                if results.detections:
                    h, w = rgb_frame.shape[:2]
                    for detection in results.detections:
                        bbox_rel = detection.location_data.relative_bounding_box
                        bbox = (
                            int(bbox_rel.xmin * w),
                            int(bbox_rel.ymin * h),
                            int((bbox_rel.xmin + bbox_rel.width) * w),
                            int((bbox_rel.ymin + bbox_rel.height) * h)
                        )
                        
                        faces_data.append({
                            'type': 'face',
                            'bbox': bbox,
                            'confidence': detection.score[0]
                        })
                
                return faces_data
        except Exception as e:
            logger.debug(f"äººè„¸æ£€æµ‹é”™è¯¯: {e}")
        return []
    
    def _detect_poses(self, rgb_frame: np.ndarray) -> List[Dict[str, Any]]:
        """å§¿åŠ¿æ£€æµ‹"""
        try:
            if self.pose:
                results = self.pose.process(rgb_frame)
                poses_data = []
                
                if results.pose_landmarks:
                    h, w = rgb_frame.shape[:2]
                    keypoints = []
                    for lm in results.pose_landmarks.landmark:
                        keypoints.append((int(lm.x * w), int(lm.y * h)))
                    
                    poses_data.append({
                        'type': 'pose',
                        'keypoints': keypoints,
                        'confidence': 0.8  # MediaPipeä¸ç›´æ¥æä¾›ç½®ä¿¡åº¦
                    })
                
                return poses_data
        except Exception as e:
            logger.debug(f"å§¿åŠ¿æ£€æµ‹é”™è¯¯: {e}")
        return []
    
    def _update_stats(self, processing_time: float, fps: float):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.detection_stats['total_detections'] += 1
        self.detection_stats['avg_processing_time'] = (
            (self.detection_stats['avg_processing_time'] * (self.detection_stats['total_detections'] - 1) + processing_time) /
            self.detection_stats['total_detections']
        )
        self.detection_stats['last_fps'] = fps
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.detection_stats.copy()
    
    def close(self):
        """å…³é—­æ£€æµ‹å™¨"""
        self.executor.shutdown(wait=True)
        if self.hands:
            self.hands.close()
        if self.face_detection:
            self.face_detection.close()
        if self.pose:
            self.pose.close()

class SimplePerformanceGUI:
    """ç®€åŒ–é«˜æ€§èƒ½GUIä¸»ç±»"""
    
    def __init__(self):
        self.camera = None
        self.detector = None
        self.running = False
        self.display_thread = None
        self.result_queue = queue.Queue(maxsize=5)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.frame_times = deque(maxlen=30)
        self.total_frames = 0
        self.start_time = time.time()
        
        # GUIè®¾ç½®
        self.window_name = "ç®€åŒ–é«˜æ€§èƒ½å¤šæ¨¡æ€è¯†åˆ«ç³»ç»Ÿ"
        self.window_size = (1280, 720)
        
    def initialize(self) -> bool:
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        logger.info("ğŸš€ å¯åŠ¨ç®€åŒ–é«˜æ€§èƒ½å¤šæ¨¡æ€è¯†åˆ«GUIç³»ç»Ÿ")
        
        # åˆå§‹åŒ–æ‘„åƒå¤´
        self.camera = ThreadSafeCamera(camera_id=0, buffer_size=2)
        if not self.camera.start():
            logger.error("æ‘„åƒå¤´åˆå§‹åŒ–å¤±è´¥")
            return False
        
        # åˆå§‹åŒ–æ£€æµ‹å™¨
        self.detector = SimpleMultiModalDetector()
        if not self.detector.initialize():
            logger.error("æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥")
            return False
        
        # åˆ›å»ºGUIçª—å£
        cv2.namedWindow(self.window_name, cv2.WINDOW_NORMAL)
        cv2.resizeWindow(self.window_name, *self.window_size)
        cv2.setWindowProperty(self.window_name, cv2.WND_PROP_TOPMOST, 1)
        
        logger.info("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return True
    
    def run(self):
        """è¿è¡Œä¸»å¾ªç¯"""
        if not self.initialize():
            logger.error("ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return
        
        self.running = True
        
        # å¯åŠ¨æ˜¾ç¤ºçº¿ç¨‹
        self.display_thread = threading.Thread(target=self._display_loop, daemon=True)
        self.display_thread.start()
        
        logger.info("ğŸ¯ å¼€å§‹ä¸»å¤„ç†å¾ªç¯")
        
        try:
            self._main_loop()
        except KeyboardInterrupt:
            logger.info("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        except Exception as e:
            logger.error(f"ä¸»å¾ªç¯é”™è¯¯: {e}")
        finally:
            self._cleanup()
    
    def _main_loop(self):
        """ä¸»å¤„ç†å¾ªç¯"""
        frame_skip = 0  # å¸§è·³è·ƒè®¡æ•°
        
        while self.running:
            frame_start = time.time()
            
            # è·å–æœ€æ–°å¸§
            frame_data = self.camera.get_frame()
            if frame_data is None:
                time.sleep(0.001)  # 1msç­‰å¾…
                continue
            
            # å¸§è·³è·ƒä¼˜åŒ– - æ¯2å¸§å¤„ç†ä¸€æ¬¡æ£€æµ‹
            if frame_skip % 2 == 0:
                # æ‰§è¡Œæ£€æµ‹
                detection_result = self.detector.detect_parallel(frame_data.frame)
                detection_result.frame_id = frame_data.frame_id
                
                # éé˜»å¡æ”¾å…¥æ˜¾ç¤ºé˜Ÿåˆ—
                try:
                    self.result_queue.put_nowait(detection_result)
                except queue.Full:
                    # é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€æ—§ç»“æœ
                    try:
                        self.result_queue.get_nowait()
                        self.result_queue.put_nowait(detection_result)
                    except queue.Empty:
                        pass
            else:
                # è·³è¿‡æ£€æµ‹ï¼Œç›´æ¥æ˜¾ç¤ºåŸå§‹å¸§
                simple_result = DetectionResult(
                    frame=frame_data.frame,
                    hands=[], faces=[], poses=[],
                    fps=0.0, processing_time=0.0,
                    frame_id=frame_data.frame_id
                )
                try:
                    self.result_queue.put_nowait(simple_result)
                except queue.Full:
                    pass
            
            frame_skip += 1
            self.total_frames += 1
            
            # è®°å½•å¸§æ—¶é—´
            frame_time = time.time() - frame_start
            self.frame_times.append(frame_time)
            
            # æ¯50å¸§è¾“å‡ºä¸€æ¬¡ç»Ÿè®¡
            if self.total_frames % 50 == 0:
                self._log_performance_stats()
            
            # æ§åˆ¶å¸§ç‡ - ç›®æ ‡30FPS
            target_frame_time = 1.0 / 30.0
            if frame_time < target_frame_time:
                time.sleep(target_frame_time - frame_time)
    
    def _display_loop(self):
        """æ˜¾ç¤ºå¾ªç¯çº¿ç¨‹"""
        logger.info("ğŸ–¥ï¸ æ˜¾ç¤ºçº¿ç¨‹å¯åŠ¨")
        
        while self.running:
            try:
                # è·å–æ£€æµ‹ç»“æœ
                result = self.result_queue.get(timeout=0.1)
                
                # ç»˜åˆ¶æ³¨é‡Š
                annotated_frame = self._draw_annotations(result)
                
                # æ˜¾ç¤ºå¸§
                cv2.imshow(self.window_name, annotated_frame)
                
                # æ£€æŸ¥æŒ‰é”®
                key = cv2.waitKey(1) & 0xFF
                if key == 27:  # ESCé”®
                    self.running = False
                    break
                elif key == ord('q'):
                    self.running = False
                    break
                    
            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"æ˜¾ç¤ºå¾ªç¯é”™è¯¯: {e}")
                time.sleep(0.01)
    
    def _draw_annotations(self, result: DetectionResult) -> np.ndarray:
        """ç»˜åˆ¶æ³¨é‡Šä¿¡æ¯"""
        frame = result.frame.copy()
        
        # ç»˜åˆ¶æ‰‹åŠ¿
        for hand in result.hands:
            if hand['type'] == 'hand':
                self._draw_hand_info(frame, hand)
        
        # ç»˜åˆ¶äººè„¸
        for face in result.faces:
            if face['type'] == 'face':
                self._draw_face_info(frame, face)
        
        # ç»˜åˆ¶å§¿åŠ¿
        for pose in result.poses:
            if pose['type'] == 'pose':
                self._draw_pose_info(frame, pose)
        
        # ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯
        self._draw_performance_info(frame, result)
        
        return frame
    
    def _draw_hand_info(self, frame: np.ndarray, hand_data: Dict[str, Any]):
        """ç»˜åˆ¶æ‰‹éƒ¨ä¿¡æ¯"""
        try:
            # ç»˜åˆ¶å…³é”®ç‚¹
            if 'landmarks' in hand_data:
                for i, (x, y) in enumerate(hand_data['landmarks']):
                    cv2.circle(frame, (int(x), int(y)), 3, (0, 255, 0), -1)
                    if i in [4, 8, 12, 16, 20]:  # æŒ‡å°–
                        cv2.circle(frame, (int(x), int(y)), 5, (0, 255, 255), 2)
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            if 'bbox' in hand_data:
                x1, y1, x2, y2 = hand_data['bbox']
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                
                # ç»˜åˆ¶æ‰‹åŠ¿æ ‡ç­¾
                gesture_info = hand_data.get('gesture', {})
                gesture_name = gesture_info.get('gesture', 'Unknown')
                confidence = gesture_info.get('confidence', 0.0)
                handedness = hand_data.get('handedness', 'Unknown')
                
                label = f"{handedness} {gesture_name} ({confidence:.2f})"
                cv2.putText(frame, label, (x1, y1 - 10),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                
        except Exception as e:
            logger.debug(f"ç»˜åˆ¶æ‰‹éƒ¨ä¿¡æ¯é”™è¯¯: {e}")
    
    def _draw_face_info(self, frame: np.ndarray, face_data: Dict[str, Any]):
        """ç»˜åˆ¶äººè„¸ä¿¡æ¯"""
        try:
            if 'bbox' in face_data:
                x1, y1, x2, y2 = face_data['bbox']
                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)
                cv2.putText(frame, f"Face ({face_data.get('confidence', 0.0):.2f})",
                           (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
        except Exception as e:
            logger.debug(f"ç»˜åˆ¶äººè„¸ä¿¡æ¯é”™è¯¯: {e}")
    
    def _draw_pose_info(self, frame: np.ndarray, pose_data: Dict[str, Any]):
        """ç»˜åˆ¶å§¿åŠ¿ä¿¡æ¯"""
        try:
            if 'keypoints' in pose_data:
                # åªç»˜åˆ¶ä¸»è¦å…³é”®ç‚¹
                important_points = [0, 11, 12, 13, 14, 15, 16]  # é¼»å­ã€è‚©è†€ã€æ‰‹è‚˜ã€æ‰‹è…•
                for i, (x, y) in enumerate(pose_data['keypoints']):
                    if i in important_points:
                        cv2.circle(frame, (int(x), int(y)), 4, (0, 0, 255), -1)
        except Exception as e:
            logger.debug(f"ç»˜åˆ¶å§¿åŠ¿ä¿¡æ¯é”™è¯¯: {e}")
    
    def _draw_performance_info(self, frame: np.ndarray, result: DetectionResult):
        """ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯"""
        try:
            # è®¡ç®—FPS
            camera_fps = self.camera.get_fps()
            avg_frame_time = sum(self.frame_times) / len(self.frame_times) if self.frame_times else 0
            display_fps = 1.0 / avg_frame_time if avg_frame_time > 0 else 0
            
            # æ€§èƒ½æ–‡æœ¬
            perf_text = [
                f"Camera FPS: {camera_fps:.1f}",
                f"Display FPS: {display_fps:.1f}",
                f"Processing: {result.processing_time*1000:.1f}ms",
                f"Hands: {len(result.hands)}",
                f"Faces: {len(result.faces)}",
                f"Poses: {len(result.poses)}",
                f"Total Frames: {self.total_frames}"
            ]
            
            # ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯èƒŒæ™¯
            overlay = frame.copy()
            cv2.rectangle(overlay, (5, 5), (350, 200), (0, 0, 0), -1)
            cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
            
            # ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯
            y_offset = 30
            for i, text in enumerate(perf_text):
                cv2.putText(frame, text, (10, y_offset + i * 25),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
            
        except Exception as e:
            logger.debug(f"ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯é”™è¯¯: {e}")
    
    def _log_performance_stats(self):
        """è®°å½•æ€§èƒ½ç»Ÿè®¡"""
        try:
            runtime = time.time() - self.start_time
            avg_fps = self.total_frames / runtime if runtime > 0 else 0
            camera_fps = self.camera.get_fps()
            detector_stats = self.detector.get_stats()
            
            logger.info(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡ - æ€»å¸§æ•°: {self.total_frames}, "
                       f"å¹³å‡FPS: {avg_fps:.1f}, æ‘„åƒå¤´FPS: {camera_fps:.1f}, "
                       f"æ£€æµ‹FPS: {detector_stats['last_fps']:.1f}")
        except Exception as e:
            logger.debug(f"æ€§èƒ½ç»Ÿè®¡é”™è¯¯: {e}")
    
    def _cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†ç³»ç»Ÿèµ„æº...")
        
        self.running = False
        
        if self.display_thread:
            self.display_thread.join(timeout=1.0)
        
        if self.camera:
            self.camera.stop()
        
        if self.detector:
            self.detector.close()
        
        cv2.destroyAllWindows()
        
        # æœ€ç»ˆç»Ÿè®¡
        runtime = time.time() - self.start_time
        avg_fps = self.total_frames / runtime if runtime > 0 else 0
        logger.info(f"ğŸ ç³»ç»Ÿè¿è¡Œå®Œæˆ - æ€»è¿è¡Œæ—¶é—´: {runtime:.1f}s, "
                   f"æ€»å¸§æ•°: {self.total_frames}, å¹³å‡FPS: {avg_fps:.1f}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        gui = SimplePerformanceGUI()
        gui.run()
    except Exception as e:
        logger.error(f"ç¨‹åºè¿è¡Œé”™è¯¯: {e}")
        return 1
    return 0

if __name__ == "__main__":
    sys.exit(main())