#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¦»çº¿é«˜æ€§èƒ½å¤šæ¨¡æ€è¯†åˆ«GUIç³»ç»Ÿ
åŸºäºæœ€ä½³å®è·µï¼Œè§£å†³ç½‘ç»œä¾èµ–å’Œå¡é¡¿é—®é¢˜
ä½¿ç”¨OpenCVå†…ç½®åŠŸèƒ½ï¼Œé¿å…MediaPipeç½‘ç»œä¸‹è½½

å‚è€ƒæœ€ä½³å®è·µ:
- OpenCVå¤šçº¿ç¨‹ä¼˜åŒ–
- ç¦»çº¿æ¨¡å‹ä½¿ç”¨
- GUIæ€§èƒ½ä¼˜åŒ–
"""

import cv2
import numpy as np
import threading
import queue
import time
import logging
from typing import Optional, Tuple, List, Dict, Any
from dataclasses import dataclass
from collections import deque
from concurrent.futures import ThreadPoolExecutor
import sys
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('offline_performance_gui.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class FrameData:
    """å¸§æ•°æ®ç»“æ„"""
    frame: np.ndarray
    timestamp: float
    frame_id: int

@dataclass
class DetectionResult:
    """æ£€æµ‹ç»“æœç»“æ„"""
    frame: np.ndarray
    hands: List[Dict[str, Any]]
    faces: List[Dict[str, Any]]
    poses: List[Dict[str, Any]]
    fps: float
    processing_time: float
    frame_id: int

class ThreadSafeCamera:
    """çº¿ç¨‹å®‰å…¨çš„æ‘„åƒå¤´ç±»"""
    
    def __init__(self, camera_id: int = 0, buffer_size: int = 2):
        self.camera_id = camera_id
        self.buffer_size = buffer_size
        self.cap = None
        self.frame_queue = queue.Queue(maxsize=buffer_size)
        self.running = False
        self.capture_thread = None
        self.frame_count = 0
        self.fps_counter = deque(maxlen=30)
        
    def start(self) -> bool:
        """å¯åŠ¨æ‘„åƒå¤´æ•è·"""
        try:
            self.cap = cv2.VideoCapture(self.camera_id)
            if not self.cap.isOpened():
                logger.error(f"æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {self.camera_id}")
                return False
                
            # ä¼˜åŒ–æ‘„åƒå¤´è®¾ç½®
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
            self.cap.set(cv2.CAP_PROP_FPS, 30)
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # å‡å°‘ç¼“å†²åŒºå»¶è¿Ÿ
            
            self.running = True
            self.capture_thread = threading.Thread(target=self._capture_frames, daemon=True)
            self.capture_thread.start()
            
            logger.info(f"ğŸ“¹ æ‘„åƒå¤´ {self.camera_id} å¯åŠ¨æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"æ‘„åƒå¤´å¯åŠ¨å¤±è´¥: {e}")
            return False
    
    def _capture_frames(self):
        """å¸§æ•è·çº¿ç¨‹"""
        last_time = time.time()
        
        while self.running:
            try:
                ret, frame = self.cap.read()
                if not ret:
                    logger.warning("æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
                    continue
                
                current_time = time.time()
                self.fps_counter.append(current_time - last_time)
                last_time = current_time
                
                frame_data = FrameData(
                    frame=frame,
                    timestamp=current_time,
                    frame_id=self.frame_count
                )
                
                # éé˜»å¡æ”¾å…¥é˜Ÿåˆ—
                try:
                    self.frame_queue.put_nowait(frame_data)
                    self.frame_count += 1
                except queue.Full:
                    # é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€æ—§çš„å¸§
                    try:
                        self.frame_queue.get_nowait()
                        self.frame_queue.put_nowait(frame_data)
                    except queue.Empty:
                        pass
                        
            except Exception as e:
                logger.error(f"å¸§æ•è·é”™è¯¯: {e}")
                time.sleep(0.01)
    
    def get_frame(self) -> Optional[FrameData]:
        """è·å–æœ€æ–°å¸§"""
        try:
            return self.frame_queue.get_nowait()
        except queue.Empty:
            return None
    
    def get_fps(self) -> float:
        """è·å–å®é™…FPS"""
        if len(self.fps_counter) < 2:
            return 0.0
        avg_interval = sum(self.fps_counter) / len(self.fps_counter)
        return 1.0 / avg_interval if avg_interval > 0 else 0.0
    
    def stop(self):
        """åœæ­¢æ‘„åƒå¤´"""
        self.running = False
        if self.capture_thread:
            self.capture_thread.join(timeout=1.0)
        if self.cap:
            self.cap.release()
        logger.info("æ‘„åƒå¤´å·²åœæ­¢")

class OfflineMultiModalDetector:
    """ç¦»çº¿å¤šæ¨¡æ€æ£€æµ‹å™¨ - ä½¿ç”¨OpenCVå†…ç½®åŠŸèƒ½"""
    
    def __init__(self):
        self.face_cascade = None
        self.hand_cascade = None
        self.body_cascade = None
        
        self.executor = ThreadPoolExecutor(max_workers=3)
        self.detection_stats = {
            'total_detections': 0,
            'avg_processing_time': 0.0,
            'last_fps': 0.0
        }
        
        # æ‰‹åŠ¿è·Ÿè¸ªçŠ¶æ€
        self.hand_tracker = HandTracker()
        
    def initialize(self) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰æ£€æµ‹å™¨"""
        try:
            logger.info("ğŸ”§ åˆå§‹åŒ–ç¦»çº¿å¤šæ¨¡æ€æ£€æµ‹å™¨...")
            
            # åŠ è½½OpenCVå†…ç½®çš„Haarçº§è”åˆ†ç±»å™¨
            cascade_path = cv2.data.haarcascades
            
            # äººè„¸æ£€æµ‹å™¨
            face_cascade_file = os.path.join(cascade_path, 'haarcascade_frontalface_default.xml')
            if os.path.exists(face_cascade_file):
                self.face_cascade = cv2.CascadeClassifier(face_cascade_file)
                logger.info("âœ… äººè„¸æ£€æµ‹å™¨åŠ è½½æˆåŠŸ")
            else:
                logger.warning("âš ï¸ äººè„¸æ£€æµ‹å™¨æ–‡ä»¶æœªæ‰¾åˆ°")
            
            # å°è¯•åŠ è½½å…¶ä»–çº§è”åˆ†ç±»å™¨
            try:
                # å…¨èº«æ£€æµ‹å™¨
                body_cascade_file = os.path.join(cascade_path, 'haarcascade_fullbody.xml')
                if os.path.exists(body_cascade_file):
                    self.body_cascade = cv2.CascadeClassifier(body_cascade_file)
                    logger.info("âœ… å…¨èº«æ£€æµ‹å™¨åŠ è½½æˆåŠŸ")
            except:
                logger.info("å…¨èº«æ£€æµ‹å™¨ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ›¿ä»£æ–¹æ³•")
            
            logger.info("âœ… ç¦»çº¿å¤šæ¨¡æ€æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def detect_parallel(self, frame: np.ndarray) -> DetectionResult:
        """å¹¶è¡Œæ£€æµ‹"""
        start_time = time.time()
        
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒä»¥æé«˜æ€§èƒ½
        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # æäº¤å¹¶è¡Œä»»åŠ¡
        hand_future = self.executor.submit(self._detect_hands, frame, gray_frame)
        face_future = self.executor.submit(self._detect_faces, frame, gray_frame)
        pose_future = self.executor.submit(self._detect_poses, frame, gray_frame)
        
        # æ”¶é›†ç»“æœ
        try:
            hands = hand_future.result(timeout=0.1)  # 100msè¶…æ—¶
        except:
            hands = []
            
        try:
            faces = face_future.result(timeout=0.1)
        except:
            faces = []
            
        try:
            poses = pose_future.result(timeout=0.1)
        except:
            poses = []
        
        processing_time = time.time() - start_time
        fps = 1.0 / processing_time if processing_time > 0 else 0.0
        
        # æ›´æ–°ç»Ÿè®¡
        self._update_stats(processing_time, fps)
        
        return DetectionResult(
            frame=frame,
            hands=hands,
            faces=faces,
            poses=poses,
            fps=fps,
            processing_time=processing_time,
            frame_id=0
        )
    
    def _detect_hands(self, frame: np.ndarray, gray_frame: np.ndarray) -> List[Dict[str, Any]]:
        """æ‰‹åŠ¿æ£€æµ‹ - ä½¿ç”¨è½®å»“å’Œé¢œè‰²æ£€æµ‹"""
        try:
            hands_data = []
            
            # ä½¿ç”¨è‚¤è‰²æ£€æµ‹
            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
            
            # å®šä¹‰è‚¤è‰²èŒƒå›´ (HSV)
            lower_skin = np.array([0, 20, 70], dtype=np.uint8)
            upper_skin = np.array([20, 255, 255], dtype=np.uint8)
            
            # åˆ›å»ºè‚¤è‰²æ©ç 
            skin_mask = cv2.inRange(hsv, lower_skin, upper_skin)
            
            # å½¢æ€å­¦æ“ä½œå»å™ª
            kernel = np.ones((3, 3), np.uint8)
            skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_OPEN, kernel)
            skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_CLOSE, kernel)
            
            # æŸ¥æ‰¾è½®å»“
            contours, _ = cv2.findContours(skin_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            # ç­›é€‰æ‰‹éƒ¨è½®å»“
            for contour in contours:
                area = cv2.contourArea(contour)
                if 1000 < area < 50000:  # æ‰‹éƒ¨é¢ç§¯èŒƒå›´
                    # è®¡ç®—è¾¹ç•Œæ¡†
                    x, y, w, h = cv2.boundingRect(contour)
                    
                    # è®¡ç®—è½®å»“ä¸­å¿ƒ
                    M = cv2.moments(contour)
                    if M["m00"] != 0:
                        cx = int(M["m10"] / M["m00"])
                        cy = int(M["m01"] / M["m00"])
                    else:
                        cx, cy = x + w//2, y + h//2
                    
                    # è®¡ç®—å‡¸åŒ…
                    hull = cv2.convexHull(contour)
                    hull_area = cv2.contourArea(hull)
                    
                    # è®¡ç®—å‡¸æ€§ç¼ºé™·
                    defects = []
                    if len(contour) > 10:
                        hull_indices = cv2.convexHull(contour, returnPoints=False)
                        if len(hull_indices) > 3:
                            defects_raw = cv2.convexityDefects(contour, hull_indices)
                            if defects_raw is not None:
                                for i in range(defects_raw.shape[0]):
                                    s, e, f, d = defects_raw[i, 0]
                                    if d > 8000:  # æ·±åº¦é˜ˆå€¼
                                        defects.append((s, e, f, d))
                    
                    # ç®€å•æ‰‹åŠ¿è¯†åˆ«
                    gesture = self._analyze_hand_gesture(contour, hull, defects, area, hull_area)
                    
                    # æ›´æ–°æ‰‹éƒ¨è·Ÿè¸ª
                    hand_id = self.hand_tracker.update_hand(cx, cy, area)
                    
                    hands_data.append({
                        'type': 'hand',
                        'bbox': (x, y, x + w, y + h),
                        'center': (cx, cy),
                        'area': area,
                        'contour': contour,
                        'hull': hull,
                        'defects': defects,
                        'gesture': gesture,
                        'hand_id': hand_id,
                        'confidence': min(0.9, area / 10000)  # åŸºäºé¢ç§¯çš„ç½®ä¿¡åº¦
                    })
            
            return hands_data[:4]  # æœ€å¤šè¿”å›4åªæ‰‹
            
        except Exception as e:
            logger.debug(f"æ‰‹åŠ¿æ£€æµ‹é”™è¯¯: {e}")
            return []
    
    def _analyze_hand_gesture(self, contour, hull, defects, area, hull_area) -> Dict[str, Any]:
        """åˆ†ææ‰‹åŠ¿"""
        try:
            # è®¡ç®—å‡¸æ€§æ¯”ç‡
            solidity = area / hull_area if hull_area > 0 else 0
            
            # åŸºäºå‡¸æ€§ç¼ºé™·æ•°é‡åˆ¤æ–­æ‰‹åŠ¿
            defect_count = len(defects)
            
            if solidity > 0.95:  # å¾ˆå‡¸ï¼Œå¯èƒ½æ˜¯æ‹³å¤´
                gesture = "Fist"
                confidence = 0.8
            elif defect_count == 0:
                gesture = "Closed Hand"
                confidence = 0.7
            elif defect_count == 1:
                gesture = "Pointing"
                confidence = 0.6
            elif defect_count == 2:
                gesture = "Peace Sign"
                confidence = 0.6
            elif defect_count >= 3:
                gesture = "Open Hand"
                confidence = 0.7
            else:
                gesture = f"{defect_count} Fingers"
                confidence = 0.5
            
            return {
                'gesture': gesture,
                'confidence': confidence,
                'defect_count': defect_count,
                'solidity': solidity
            }
            
        except Exception as e:
            logger.debug(f"æ‰‹åŠ¿åˆ†æé”™è¯¯: {e}")
            return {'gesture': 'Unknown', 'confidence': 0.0}
    
    def _detect_faces(self, frame: np.ndarray, gray_frame: np.ndarray) -> List[Dict[str, Any]]:
        """äººè„¸æ£€æµ‹"""
        try:
            faces_data = []
            
            if self.face_cascade is not None:
                # ä½¿ç”¨Haarçº§è”æ£€æµ‹äººè„¸
                faces = self.face_cascade.detectMultiScale(
                    gray_frame,
                    scaleFactor=1.1,
                    minNeighbors=5,
                    minSize=(30, 30),
                    flags=cv2.CASCADE_SCALE_IMAGE
                )
                
                for (x, y, w, h) in faces:
                    faces_data.append({
                        'type': 'face',
                        'bbox': (x, y, x + w, y + h),
                        'confidence': 0.8  # Haarçº§è”ä¸æä¾›ç½®ä¿¡åº¦
                    })
            
            return faces_data
            
        except Exception as e:
            logger.debug(f"äººè„¸æ£€æµ‹é”™è¯¯: {e}")
            return []
    
    def _detect_poses(self, frame: np.ndarray, gray_frame: np.ndarray) -> List[Dict[str, Any]]:
        """å§¿åŠ¿æ£€æµ‹ - ç®€åŒ–ç‰ˆæœ¬"""
        try:
            poses_data = []
            
            if self.body_cascade is not None:
                # ä½¿ç”¨å…¨èº«æ£€æµ‹å™¨
                bodies = self.body_cascade.detectMultiScale(
                    gray_frame,
                    scaleFactor=1.1,
                    minNeighbors=3,
                    minSize=(50, 100)
                )
                
                for (x, y, w, h) in bodies:
                    # ä¼°ç®—å…³é”®ç‚¹ä½ç½®
                    keypoints = self._estimate_keypoints(x, y, w, h)
                    
                    poses_data.append({
                        'type': 'pose',
                        'bbox': (x, y, x + w, y + h),
                        'keypoints': keypoints,
                        'confidence': 0.6
                    })
            
            return poses_data
            
        except Exception as e:
            logger.debug(f"å§¿åŠ¿æ£€æµ‹é”™è¯¯: {e}")
            return []
    
    def _estimate_keypoints(self, x: int, y: int, w: int, h: int) -> List[Tuple[int, int]]:
        """ä¼°ç®—å…³é”®ç‚¹ä½ç½®"""
        # åŸºäºè¾¹ç•Œæ¡†ä¼°ç®—ä¸»è¦å…³é”®ç‚¹
        keypoints = [
            (x + w//2, y + h//8),      # å¤´éƒ¨
            (x + w//4, y + h//3),      # å·¦è‚©
            (x + 3*w//4, y + h//3),    # å³è‚©
            (x + w//4, y + 2*h//3),    # å·¦è‚˜
            (x + 3*w//4, y + 2*h//3),  # å³è‚˜
            (x + w//4, y + h),         # å·¦æ‰‹
            (x + 3*w//4, y + h),       # å³æ‰‹
        ]
        return keypoints
    
    def _update_stats(self, processing_time: float, fps: float):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.detection_stats['total_detections'] += 1
        self.detection_stats['avg_processing_time'] = (
            (self.detection_stats['avg_processing_time'] * (self.detection_stats['total_detections'] - 1) + processing_time) /
            self.detection_stats['total_detections']
        )
        self.detection_stats['last_fps'] = fps
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.detection_stats.copy()
    
    def close(self):
        """å…³é—­æ£€æµ‹å™¨"""
        self.executor.shutdown(wait=True)

class HandTracker:
    """æ‰‹éƒ¨è·Ÿè¸ªå™¨"""
    
    def __init__(self, max_distance: float = 100.0):
        self.hands = {}  # hand_id -> (x, y, last_seen)
        self.next_id = 0
        self.max_distance = max_distance
        self.max_age = 30  # æœ€å¤§å¸§æ•°
    
    def update_hand(self, x: int, y: int, area: float) -> int:
        """æ›´æ–°æ‰‹éƒ¨ä½ç½®ï¼Œè¿”å›æ‰‹éƒ¨ID"""
        current_time = time.time()
        
        # æŸ¥æ‰¾æœ€è¿‘çš„å·²çŸ¥æ‰‹éƒ¨
        best_id = None
        best_distance = float('inf')
        
        for hand_id, (hx, hy, last_seen, _) in self.hands.items():
            distance = np.sqrt((x - hx)**2 + (y - hy)**2)
            if distance < self.max_distance and distance < best_distance:
                best_distance = distance
                best_id = hand_id
        
        if best_id is not None:
            # æ›´æ–°ç°æœ‰æ‰‹éƒ¨
            self.hands[best_id] = (x, y, current_time, area)
            return best_id
        else:
            # åˆ›å»ºæ–°æ‰‹éƒ¨
            new_id = self.next_id
            self.next_id += 1
            self.hands[new_id] = (x, y, current_time, area)
            return new_id
    
    def cleanup_old_hands(self):
        """æ¸…ç†æ—§çš„æ‰‹éƒ¨è·Ÿè¸ª"""
        current_time = time.time()
        to_remove = []
        
        for hand_id, (_, _, last_seen, _) in self.hands.items():
            if current_time - last_seen > 1.0:  # 1ç§’æœªè§
                to_remove.append(hand_id)
        
        for hand_id in to_remove:
            del self.hands[hand_id]

class OfflinePerformanceGUI:
    """ç¦»çº¿é«˜æ€§èƒ½GUIä¸»ç±»"""
    
    def __init__(self):
        self.camera = None
        self.detector = None
        self.running = False
        self.display_thread = None
        self.result_queue = queue.Queue(maxsize=5)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.frame_times = deque(maxlen=30)
        self.total_frames = 0
        self.start_time = time.time()
        
        # GUIè®¾ç½®
        self.window_name = "ç¦»çº¿é«˜æ€§èƒ½å¤šæ¨¡æ€è¯†åˆ«ç³»ç»Ÿ"
        self.window_size = (1280, 720)
        
    def initialize(self) -> bool:
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        logger.info("ğŸš€ å¯åŠ¨ç¦»çº¿é«˜æ€§èƒ½å¤šæ¨¡æ€è¯†åˆ«GUIç³»ç»Ÿ")
        
        # åˆå§‹åŒ–æ‘„åƒå¤´
        self.camera = ThreadSafeCamera(camera_id=0, buffer_size=2)
        if not self.camera.start():
            logger.error("æ‘„åƒå¤´åˆå§‹åŒ–å¤±è´¥")
            return False
        
        # åˆå§‹åŒ–æ£€æµ‹å™¨
        self.detector = OfflineMultiModalDetector()
        if not self.detector.initialize():
            logger.error("æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥")
            return False
        
        # åˆ›å»ºGUIçª—å£
        cv2.namedWindow(self.window_name, cv2.WINDOW_NORMAL)
        cv2.resizeWindow(self.window_name, *self.window_size)
        cv2.setWindowProperty(self.window_name, cv2.WND_PROP_TOPMOST, 1)
        
        logger.info("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return True
    
    def run(self):
        """è¿è¡Œä¸»å¾ªç¯"""
        if not self.initialize():
            logger.error("ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return
        
        self.running = True
        
        # å¯åŠ¨æ˜¾ç¤ºçº¿ç¨‹
        self.display_thread = threading.Thread(target=self._display_loop, daemon=True)
        self.display_thread.start()
        
        logger.info("ğŸ¯ å¼€å§‹ä¸»å¤„ç†å¾ªç¯")
        
        try:
            self._main_loop()
        except KeyboardInterrupt:
            logger.info("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        except Exception as e:
            logger.error(f"ä¸»å¾ªç¯é”™è¯¯: {e}")
        finally:
            self._cleanup()
    
    def _main_loop(self):
        """ä¸»å¤„ç†å¾ªç¯"""
        frame_skip = 0  # å¸§è·³è·ƒè®¡æ•°
        
        while self.running:
            frame_start = time.time()
            
            # è·å–æœ€æ–°å¸§
            frame_data = self.camera.get_frame()
            if frame_data is None:
                time.sleep(0.001)  # 1msç­‰å¾…
                continue
            
            # å¸§è·³è·ƒä¼˜åŒ– - æ¯2å¸§å¤„ç†ä¸€æ¬¡æ£€æµ‹
            if frame_skip % 2 == 0:
                # æ‰§è¡Œæ£€æµ‹
                detection_result = self.detector.detect_parallel(frame_data.frame)
                detection_result.frame_id = frame_data.frame_id
                
                # éé˜»å¡æ”¾å…¥æ˜¾ç¤ºé˜Ÿåˆ—
                try:
                    self.result_queue.put_nowait(detection_result)
                except queue.Full:
                    # é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€æ—§ç»“æœ
                    try:
                        self.result_queue.get_nowait()
                        self.result_queue.put_nowait(detection_result)
                    except queue.Empty:
                        pass
            else:
                # è·³è¿‡æ£€æµ‹ï¼Œç›´æ¥æ˜¾ç¤ºåŸå§‹å¸§
                simple_result = DetectionResult(
                    frame=frame_data.frame,
                    hands=[], faces=[], poses=[],
                    fps=0.0, processing_time=0.0,
                    frame_id=frame_data.frame_id
                )
                try:
                    self.result_queue.put_nowait(simple_result)
                except queue.Full:
                    pass
            
            frame_skip += 1
            self.total_frames += 1
            
            # è®°å½•å¸§æ—¶é—´
            frame_time = time.time() - frame_start
            self.frame_times.append(frame_time)
            
            # æ¯50å¸§è¾“å‡ºä¸€æ¬¡ç»Ÿè®¡
            if self.total_frames % 50 == 0:
                self._log_performance_stats()
                # æ¸…ç†æ—§çš„æ‰‹éƒ¨è·Ÿè¸ª
                self.detector.hand_tracker.cleanup_old_hands()
            
            # æ§åˆ¶å¸§ç‡ - ç›®æ ‡30FPS
            target_frame_time = 1.0 / 30.0
            if frame_time < target_frame_time:
                time.sleep(target_frame_time - frame_time)
    
    def _display_loop(self):
        """æ˜¾ç¤ºå¾ªç¯çº¿ç¨‹"""
        logger.info("ğŸ–¥ï¸ æ˜¾ç¤ºçº¿ç¨‹å¯åŠ¨")
        
        while self.running:
            try:
                # è·å–æ£€æµ‹ç»“æœ
                result = self.result_queue.get(timeout=0.1)
                
                # ç»˜åˆ¶æ³¨é‡Š
                annotated_frame = self._draw_annotations(result)
                
                # æ˜¾ç¤ºå¸§
                cv2.imshow(self.window_name, annotated_frame)
                
                # æ£€æŸ¥æŒ‰é”®
                key = cv2.waitKey(1) & 0xFF
                if key == 27:  # ESCé”®
                    self.running = False
                    break
                elif key == ord('q'):
                    self.running = False
                    break
                    
            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"æ˜¾ç¤ºå¾ªç¯é”™è¯¯: {e}")
                time.sleep(0.01)
    
    def _draw_annotations(self, result: DetectionResult) -> np.ndarray:
        """ç»˜åˆ¶æ³¨é‡Šä¿¡æ¯"""
        frame = result.frame.copy()
        
        # ç»˜åˆ¶æ‰‹åŠ¿
        for hand in result.hands:
            if hand['type'] == 'hand':
                self._draw_hand_info(frame, hand)
        
        # ç»˜åˆ¶äººè„¸
        for face in result.faces:
            if face['type'] == 'face':
                self._draw_face_info(frame, face)
        
        # ç»˜åˆ¶å§¿åŠ¿
        for pose in result.poses:
            if pose['type'] == 'pose':
                self._draw_pose_info(frame, pose)
        
        # ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯
        self._draw_performance_info(frame, result)
        
        return frame
    
    def _draw_hand_info(self, frame: np.ndarray, hand_data: Dict[str, Any]):
        """ç»˜åˆ¶æ‰‹éƒ¨ä¿¡æ¯"""
        try:
            # ç»˜åˆ¶è½®å»“
            if 'contour' in hand_data:
                cv2.drawContours(frame, [hand_data['contour']], -1, (0, 255, 0), 2)
            
            # ç»˜åˆ¶å‡¸åŒ…
            if 'hull' in hand_data:
                cv2.drawContours(frame, [hand_data['hull']], -1, (0, 255, 255), 2)
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            if 'bbox' in hand_data:
                x1, y1, x2, y2 = hand_data['bbox']
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                
                # ç»˜åˆ¶æ‰‹åŠ¿æ ‡ç­¾
                gesture_info = hand_data.get('gesture', {})
                gesture_name = gesture_info.get('gesture', 'Unknown')
                confidence = gesture_info.get('confidence', 0.0)
                hand_id = hand_data.get('hand_id', 0)
                
                label = f"Hand{hand_id}: {gesture_name} ({confidence:.2f})"
                cv2.putText(frame, label, (x1, y1 - 10),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            
            # ç»˜åˆ¶ä¸­å¿ƒç‚¹
            if 'center' in hand_data:
                cx, cy = hand_data['center']
                cv2.circle(frame, (cx, cy), 5, (255, 0, 0), -1)
                
        except Exception as e:
            logger.debug(f"ç»˜åˆ¶æ‰‹éƒ¨ä¿¡æ¯é”™è¯¯: {e}")
    
    def _draw_face_info(self, frame: np.ndarray, face_data: Dict[str, Any]):
        """ç»˜åˆ¶äººè„¸ä¿¡æ¯"""
        try:
            if 'bbox' in face_data:
                x1, y1, x2, y2 = face_data['bbox']
                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)
                cv2.putText(frame, f"Face ({face_data.get('confidence', 0.0):.2f})",
                           (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
        except Exception as e:
            logger.debug(f"ç»˜åˆ¶äººè„¸ä¿¡æ¯é”™è¯¯: {e}")
    
    def _draw_pose_info(self, frame: np.ndarray, pose_data: Dict[str, Any]):
        """ç»˜åˆ¶å§¿åŠ¿ä¿¡æ¯"""
        try:
            if 'keypoints' in pose_data:
                # ç»˜åˆ¶å…³é”®ç‚¹
                for i, (x, y) in enumerate(pose_data['keypoints']):
                    cv2.circle(frame, (int(x), int(y)), 4, (0, 0, 255), -1)
                    cv2.putText(frame, str(i), (int(x)+5, int(y)),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 255), 1)
        except Exception as e:
            logger.debug(f"ç»˜åˆ¶å§¿åŠ¿ä¿¡æ¯é”™è¯¯: {e}")
    
    def _draw_performance_info(self, frame: np.ndarray, result: DetectionResult):
        """ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯"""
        try:
            # è®¡ç®—FPS
            camera_fps = self.camera.get_fps()
            avg_frame_time = sum(self.frame_times) / len(self.frame_times) if self.frame_times else 0
            display_fps = 1.0 / avg_frame_time if avg_frame_time > 0 else 0
            
            # æ€§èƒ½æ–‡æœ¬
            perf_text = [
                f"Camera FPS: {camera_fps:.1f}",
                f"Display FPS: {display_fps:.1f}",
                f"Processing: {result.processing_time*1000:.1f}ms",
                f"Hands: {len(result.hands)}",
                f"Faces: {len(result.faces)}",
                f"Poses: {len(result.poses)}",
                f"Total Frames: {self.total_frames}",
                f"Mode: Offline OpenCV"
            ]
            
            # ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯èƒŒæ™¯
            overlay = frame.copy()
            cv2.rectangle(overlay, (5, 5), (350, 220), (0, 0, 0), -1)
            cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
            
            # ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯
            y_offset = 30
            for i, text in enumerate(perf_text):
                cv2.putText(frame, text, (10, y_offset + i * 25),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
            
        except Exception as e:
            logger.debug(f"ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯é”™è¯¯: {e}")
    
    def _log_performance_stats(self):
        """è®°å½•æ€§èƒ½ç»Ÿè®¡"""
        try:
            runtime = time.time() - self.start_time
            avg_fps = self.total_frames / runtime if runtime > 0 else 0
            camera_fps = self.camera.get_fps()
            detector_stats = self.detector.get_stats()
            
            logger.info(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡ - æ€»å¸§æ•°: {self.total_frames}, "
                       f"å¹³å‡FPS: {avg_fps:.1f}, æ‘„åƒå¤´FPS: {camera_fps:.1f}, "
                       f"æ£€æµ‹FPS: {detector_stats['last_fps']:.1f}")
        except Exception as e:
            logger.debug(f"æ€§èƒ½ç»Ÿè®¡é”™è¯¯: {e}")
    
    def _cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†ç³»ç»Ÿèµ„æº...")
        
        self.running = False
        
        if self.display_thread:
            self.display_thread.join(timeout=1.0)
        
        if self.camera:
            self.camera.stop()
        
        if self.detector:
            self.detector.close()
        
        cv2.destroyAllWindows()
        
        # æœ€ç»ˆç»Ÿè®¡
        runtime = time.time() - self.start_time
        avg_fps = self.total_frames / runtime if runtime > 0 else 0
        logger.info(f"ğŸ ç³»ç»Ÿè¿è¡Œå®Œæˆ - æ€»è¿è¡Œæ—¶é—´: {runtime:.1f}s, "
                   f"æ€»å¸§æ•°: {self.total_frames}, å¹³å‡FPS: {avg_fps:.1f}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        gui = OfflinePerformanceGUI()
        gui.run()
    except Exception as e:
        logger.error(f"ç¨‹åºè¿è¡Œé”™è¯¯: {e}")
        return 1
    return 0

if __name__ == "__main__":
    sys.exit(main())