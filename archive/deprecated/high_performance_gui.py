#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""é«˜æ€§èƒ½å¤šæ¨¡æ€è¯†åˆ«GUI - ä¸“é—¨è§£å†³å¡é¡¿å’Œæ‰‹åŠ¿è¯†åˆ«é—®é¢˜"""

import sys
import os
import cv2
import numpy as np
import time
import logging
import threading
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from collections import deque
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

# è®¾ç½®ç¼–ç 
os.environ['PYTHONIOENCODING'] = 'utf-8'

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,  # å‡å°‘æ—¥å¿—çº§åˆ«æå‡æ€§èƒ½
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('high_performance_gui.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceResult:
    """æ€§èƒ½ä¼˜åŒ–çš„æ£€æµ‹ç»“æœ"""
    timestamp: float
    face_count: int = 0
    hand_count: int = 0
    pose_count: int = 0
    fps: float = 0.0
    processing_time: float = 0.0
    hands_landmarks: List = None
    faces_boxes: List = None
    poses_keypoints: List = None

class HighPerformanceDetector:
    """é«˜æ€§èƒ½å¤šæ¨¡æ€æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.face_recognizer = None
        self.gesture_recognizer = None
        self.pose_recognizer = None
        
        # æ€§èƒ½ä¼˜åŒ–å‚æ•°
        self.frame_skip = 1  # ä¸è·³å¸§ï¼Œç¡®ä¿æµç•…
        self.frame_counter = 0
        self.detection_interval = 2  # æ¯2å¸§è¿›è¡Œä¸€æ¬¡å®Œæ•´æ£€æµ‹
        self.last_result = PerformanceResult(timestamp=time.time())
        
        # FPSè®¡ç®—
        self.fps_history = deque(maxlen=30)
        self.last_fps_time = time.time()
        
        self._init_recognizers()
    
    def _init_recognizers(self):
        """åˆå§‹åŒ–è¯†åˆ«å™¨"""
        try:
            # å¯¼å…¥ä¼˜åŒ–çš„æ£€æµ‹å™¨
            from recognition.optimized_multimodal_detector import create_optimized_multimodal_detector
            
            logger.info("æ­£åœ¨åˆå§‹åŒ–é«˜æ€§èƒ½æ£€æµ‹å™¨...")
            detector = create_optimized_multimodal_detector()
            
            self.face_recognizer = detector.face_recognizer
            self.gesture_recognizer = detector.gesture_recognizer  
            self.pose_recognizer = detector.pose_recognizer
            
            logger.info("âœ… é«˜æ€§èƒ½æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def detect_fast(self, frame: np.ndarray) -> Tuple[np.ndarray, PerformanceResult]:
        """é«˜æ€§èƒ½æ£€æµ‹ - ä¼˜åŒ–ç‰ˆæœ¬"""
        start_time = time.time()
        self.frame_counter += 1
        
        # è®¡ç®—FPS
        current_time = time.time()
        if current_time - self.last_fps_time > 0:
            fps = 1.0 / (current_time - self.last_fps_time)
            self.fps_history.append(fps)
        self.last_fps_time = current_time
        
        # åˆ›å»ºç»“æœå¯¹è±¡
        result = PerformanceResult(
            timestamp=start_time,
            fps=np.mean(self.fps_history) if self.fps_history else 0
        )
        
        # æ™ºèƒ½æ£€æµ‹ç­–ç•¥ï¼šä¸æ˜¯æ¯å¸§éƒ½åšå®Œæ•´æ£€æµ‹
        do_full_detection = (self.frame_counter % self.detection_interval == 0)
        
        if do_full_detection:
            # å¹¶è¡Œæ£€æµ‹ä»¥æå‡æ€§èƒ½
            detection_results = self._parallel_detection(frame)
            
            result.face_count = detection_results.get('faces', 0)
            result.hand_count = detection_results.get('hands', 0) 
            result.pose_count = detection_results.get('poses', 0)
            result.hands_landmarks = detection_results.get('hands_landmarks', [])
            result.faces_boxes = detection_results.get('faces_boxes', [])
            result.poses_keypoints = detection_results.get('poses_keypoints', [])
            
            self.last_result = result
        else:
            # ä½¿ç”¨ä¸Šæ¬¡æ£€æµ‹ç»“æœï¼Œåªæ›´æ–°æ—¶é—´æˆ³å’ŒFPS
            result = PerformanceResult(
                timestamp=start_time,
                face_count=self.last_result.face_count,
                hand_count=self.last_result.hand_count,
                pose_count=self.last_result.pose_count,
                fps=result.fps,
                hands_landmarks=self.last_result.hands_landmarks,
                faces_boxes=self.last_result.faces_boxes,
                poses_keypoints=self.last_result.poses_keypoints
            )
        
        result.processing_time = time.time() - start_time
        
        # ç»˜åˆ¶é«˜æ€§èƒ½æ³¨é‡Š
        annotated_frame = self._draw_fast_annotations(frame, result)
        
        return annotated_frame, result
    
    def _parallel_detection(self, frame: np.ndarray) -> Dict:
        """å¹¶è¡Œæ£€æµ‹ä»¥æå‡æ€§èƒ½"""
        results = {'faces': 0, 'hands': 0, 'poses': 0, 
                  'hands_landmarks': [], 'faces_boxes': [], 'poses_keypoints': []}
        
        try:
            # é¢éƒ¨æ£€æµ‹
            if self.face_recognizer:
                face_result = self.face_recognizer.detect_faces(frame)
                if hasattr(face_result, 'faces_detected'):
                    results['faces'] = face_result.faces_detected
                    if hasattr(face_result, 'faces') and face_result.faces:
                        results['faces_boxes'] = []
                        for face in face_result.faces:
                            if hasattr(face, 'bbox') and face.bbox is not None:
                                results['faces_boxes'].append(face.bbox)
            
            # æ‰‹åŠ¿æ£€æµ‹ - é‡ç‚¹ä¼˜åŒ–å¤šç‚¹è¯†åˆ«
            if self.gesture_recognizer:
                gesture_result = self.gesture_recognizer.detect_gestures(frame)
                if hasattr(gesture_result, 'hands_detected'):
                    results['hands'] = gesture_result.hands_detected
                    if hasattr(gesture_result, 'hands_data') and gesture_result.hands_data:
                        results['hands_landmarks'] = gesture_result.hands_data
            
            # å§¿åŠ¿æ£€æµ‹
            if self.pose_recognizer:
                pose_result = self.pose_recognizer.detect_poses(frame)
                if hasattr(pose_result, 'persons_detected'):
                    results['poses'] = pose_result.persons_detected
                    if hasattr(pose_result, 'persons_data') and pose_result.persons_data:
                        results['poses_keypoints'] = pose_result.persons_data
                        
        except Exception as e:
            logger.warning(f"æ£€æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
        
        return results
    
    def _draw_fast_annotations(self, frame: np.ndarray, result: PerformanceResult) -> np.ndarray:
        """é«˜æ€§èƒ½æ³¨é‡Šç»˜åˆ¶"""
        try:
            annotated = frame.copy()
            
            # ç»˜åˆ¶é¢éƒ¨æ¡†
            if result.faces_boxes:
                for i, bbox in enumerate(result.faces_boxes):
                    if bbox is not None and len(bbox) >= 4:
                        x1, y1, x2, y2 = map(int, bbox[:4])
                        cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 255, 0), 2)
                        cv2.putText(annotated, f"Face {i+1}", (x1, y1-10), 
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            
            # ç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹ - æ”¯æŒå¤šç‚¹è¯†åˆ«
            if result.hands_landmarks:
                colors = [(255, 0, 0), (0, 255, 255), (255, 0, 255), (0, 128, 255)]  # å¤šç§é¢œè‰²åŒºåˆ†ä¸åŒæ‰‹
                for hand_idx, hand_data in enumerate(result.hands_landmarks):
                    color = colors[hand_idx % len(colors)]
                    try:
                        # å¤„ç†MediaPipeæ ¼å¼çš„æ‰‹éƒ¨å…³é”®ç‚¹
                        if hasattr(hand_data, 'landmark'):
                            landmarks = hand_data.landmark
                            h, w = frame.shape[:2]
                            
                            # ç»˜åˆ¶å…³é”®ç‚¹
                            for landmark in landmarks:
                                x = int(landmark.x * w)
                                y = int(landmark.y * h)
                                cv2.circle(annotated, (x, y), 3, color, -1)
                            
                            # ç»˜åˆ¶è¿æ¥çº¿ï¼ˆç®€åŒ–ç‰ˆæœ¬ä»¥æå‡æ€§èƒ½ï¼‰
                            connections = [(0, 1), (1, 2), (2, 3), (3, 4),  # æ‹‡æŒ‡
                                         (0, 5), (5, 6), (6, 7), (7, 8),  # é£ŸæŒ‡
                                         (0, 17), (5, 9), (9, 10), (10, 11), (11, 12),  # ä¸­æŒ‡
                                         (13, 14), (14, 15), (15, 16),  # æ— åæŒ‡
                                         (17, 18), (18, 19), (19, 20)]  # å°æŒ‡
                            
                            for start_idx, end_idx in connections:
                                if start_idx < len(landmarks) and end_idx < len(landmarks):
                                    start_point = (int(landmarks[start_idx].x * w), 
                                                 int(landmarks[start_idx].y * h))
                                    end_point = (int(landmarks[end_idx].x * w), 
                                               int(landmarks[end_idx].y * h))
                                    cv2.line(annotated, start_point, end_point, color, 2)
                        
                        # æ˜¾ç¤ºæ‰‹éƒ¨æ ‡ç­¾
                        cv2.putText(annotated, f"Hand {hand_idx+1}", (10, 150 + hand_idx*25), 
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
                                  
                    except Exception as e:
                        logger.debug(f"ç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹å¤±è´¥: {e}")
            
            # ç»˜åˆ¶å§¿åŠ¿å…³é”®ç‚¹
            if result.poses_keypoints:
                for pose_idx, pose_data in enumerate(result.poses_keypoints):
                    try:
                        if hasattr(pose_data, 'keypoints') and pose_data.keypoints is not None:
                            keypoints = pose_data.keypoints
                            h, w = frame.shape[:2]
                            
                            # ç»˜åˆ¶å…³é”®ç‚¹
                            for i in range(0, len(keypoints), 3):  # x, y, confidence
                                if i+2 < len(keypoints) and keypoints[i+2] > 0.5:  # ç½®ä¿¡åº¦é˜ˆå€¼
                                    x, y = int(keypoints[i] * w), int(keypoints[i+1] * h)
                                    cv2.circle(annotated, (x, y), 4, (0, 0, 255), -1)
                        
                        cv2.putText(annotated, f"Pose {pose_idx+1}", (10, 250 + pose_idx*20), 
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
                                  
                    except Exception as e:
                        logger.debug(f"ç»˜åˆ¶å§¿åŠ¿å…³é”®ç‚¹å¤±è´¥: {e}")
            
            # ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯
            self._draw_performance_info(annotated, result)
            
            return annotated
            
        except Exception as e:
            logger.error(f"ç»˜åˆ¶æ³¨é‡Šå¤±è´¥: {e}")
            return frame
    
    def _draw_performance_info(self, frame: np.ndarray, result: PerformanceResult):
        """ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯"""
        # åŠé€æ˜èƒŒæ™¯
        overlay = frame.copy()
        cv2.rectangle(overlay, (0, 0), (frame.shape[1], 120), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # æ€§èƒ½æ–‡æœ¬
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.7
        color = (0, 255, 0)
        thickness = 2
        
        # æ˜¾ç¤ºæ£€æµ‹ç»Ÿè®¡
        cv2.putText(frame, f"Faces: {result.face_count}", (10, 30), font, font_scale, color, thickness)
        cv2.putText(frame, f"Hands: {result.hand_count}", (10, 60), font, font_scale, color, thickness)
        cv2.putText(frame, f"Poses: {result.pose_count}", (10, 90), font, font_scale, color, thickness)
        
        # æ˜¾ç¤ºFPS
        cv2.putText(frame, f"FPS: {result.fps:.1f}", (200, 30), font, font_scale, (0, 255, 255), thickness)
        
        # æ˜¾ç¤ºå¤„ç†æ—¶é—´
        cv2.putText(frame, f"Time: {result.processing_time*1000:.1f}ms", (200, 60), font, font_scale, (255, 255, 0), thickness)
        
        # æ˜¾ç¤ºå¸§æ•°
        cv2.putText(frame, f"Frame: {self.frame_counter}", (200, 90), font, font_scale, (255, 0, 255), thickness)

class HighPerformanceGUI:
    """é«˜æ€§èƒ½GUIç•Œé¢"""
    
    def __init__(self):
        self.detector = HighPerformanceDetector()
        self.cap = None
        self.running = False
        self.frame_count = 0
        
        # æ€§èƒ½ä¼˜åŒ–è®¾ç½®
        self.target_fps = 30
        self.frame_time = 1.0 / self.target_fps
    
    def initialize_camera(self, camera_id: int = 0) -> bool:
        """åˆå§‹åŒ–æ‘„åƒå¤´"""
        try:
            self.cap = cv2.VideoCapture(camera_id)
            if not self.cap.isOpened():
                logger.error("æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
                return False
            
            # ä¼˜åŒ–æ‘„åƒå¤´è®¾ç½®
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            self.cap.set(cv2.CAP_PROP_FPS, 30)
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # å‡å°‘ç¼“å†²åŒºå»¶è¿Ÿ
            
            logger.info("âœ… é«˜æ€§èƒ½æ‘„åƒå¤´åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"æ‘„åƒå¤´åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def run(self):
        """è¿è¡Œé«˜æ€§èƒ½GUI"""
        logger.info("ğŸš€ å¯åŠ¨é«˜æ€§èƒ½å¤šæ¨¡æ€è¯†åˆ«GUI")
        
        if not self.initialize_camera():
            return False
        
        window_name = "High Performance Multimodal Recognition"
        cv2.namedWindow(window_name, cv2.WINDOW_AUTOSIZE)
        cv2.setWindowProperty(window_name, cv2.WND_PROP_TOPMOST, 1)
        cv2.moveWindow(window_name, 100, 100)
        
        logger.info(f"åˆ›å»ºé«˜æ€§èƒ½GUIçª—å£: {window_name}")
        
        self.running = True
        last_frame_time = time.time()
        
        try:
            while self.running:
                frame_start = time.time()
                
                ret, frame = self.cap.read()
                if not ret or frame is None:
                    logger.warning("æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
                    continue
                
                self.frame_count += 1
                
                # é«˜æ€§èƒ½æ£€æµ‹
                annotated_frame, result = self.detector.detect_fast(frame)
                
                # æ˜¾ç¤ºå¸§
                cv2.imshow(window_name, annotated_frame)
                
                # æ€§èƒ½æ—¥å¿—ï¼ˆæ¯100å¸§è¾“å‡ºä¸€æ¬¡ï¼‰
                if self.frame_count % 100 == 0:
                    logger.info(f"é«˜æ€§èƒ½GUIè¿è¡Œæ­£å¸¸ - å¸§æ•°:{self.frame_count}, FPS:{result.fps:.1f}, "
                              f"æ£€æµ‹: é¢éƒ¨{result.face_count} æ‰‹åŠ¿{result.hand_count} å§¿åŠ¿{result.pose_count}")
                
                # å¤„ç†æŒ‰é”®
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q') or key == 27:  # 'q' æˆ– ESC
                    logger.info("ç”¨æˆ·è¯·æ±‚é€€å‡º")
                    break
                elif key == ord('s'):  # ä¿å­˜æˆªå›¾
                    self._save_screenshot(annotated_frame)
                elif key == ord('r'):  # é‡ç½®ç»Ÿè®¡
                    self._reset_stats()
                
                # æ£€æŸ¥çª—å£çŠ¶æ€
                if cv2.getWindowProperty(window_name, cv2.WND_PROP_VISIBLE) < 1:
                    break
                
                # å¸§ç‡æ§åˆ¶ï¼ˆå¯é€‰ï¼‰
                frame_time = time.time() - frame_start
                if frame_time < self.frame_time:
                    time.sleep(self.frame_time - frame_time)
            
            logger.info("âœ… é«˜æ€§èƒ½GUIæµ‹è¯•å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"é«˜æ€§èƒ½GUIè¿è¡Œé”™è¯¯: {e}")
            return False
        
        finally:
            self._cleanup()
    
    def _save_screenshot(self, frame: np.ndarray):
        """ä¿å­˜æˆªå›¾"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"high_performance_screenshot_{timestamp}.jpg"
            cv2.imwrite(filename, frame)
            logger.info(f"æˆªå›¾å·²ä¿å­˜: {filename}")
        except Exception as e:
            logger.error(f"ä¿å­˜æˆªå›¾å¤±è´¥: {e}")
    
    def _reset_stats(self):
        """é‡ç½®ç»Ÿè®¡"""
        self.frame_count = 0
        self.detector.frame_counter = 0
        self.detector.fps_history.clear()
        logger.info("ç»Ÿè®¡å·²é‡ç½®")
    
    def _cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.running = False
        if self.cap:
            self.cap.release()
        cv2.destroyAllWindows()
        logger.info("é«˜æ€§èƒ½GUIèµ„æºæ¸…ç†å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("=== é«˜æ€§èƒ½å¤šæ¨¡æ€è¯†åˆ«GUIç³»ç»Ÿ ===")
    
    try:
        gui = HighPerformanceGUI()
        success = gui.run()
        
        if success:
            logger.info("é«˜æ€§èƒ½ç¨‹åºæ­£å¸¸ç»“æŸ")
        else:
            logger.error("é«˜æ€§èƒ½ç¨‹åºå¼‚å¸¸ç»“æŸ")
            
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        logger.error(f"é«˜æ€§èƒ½ç¨‹åºè¿è¡Œé”™è¯¯: {e}")
    
    return 0

if __name__ == '__main__':
    sys.exit(main())