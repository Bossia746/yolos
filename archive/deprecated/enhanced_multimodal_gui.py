#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""å¢å¼ºç‰ˆå¤šæ¨¡æ€è¯†åˆ«GUI - è§£å†³ä¹±ç é—®é¢˜å¹¶å±•ç¤ºå®Œæ•´åŠŸèƒ½"""

import sys
import os
import cv2
import numpy as np
import time
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
from collections import deque
import math

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

# è®¾ç½®ç¼–ç 
os.environ['PYTHONIOENCODING'] = 'utf-8'

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('enhanced_multimodal_test.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


class TrajectoryTracker:
    """è½¨è¿¹è¿½è¸ªå™¨"""
    
    def __init__(self, max_points=30):
        self.max_points = max_points
        self.face_trajectories = {}  # é¢éƒ¨è½¨è¿¹
        self.hand_trajectories = {}  # æ‰‹éƒ¨è½¨è¿¹
        self.pose_trajectories = {}  # å§¿åŠ¿å…³é”®ç‚¹è½¨è¿¹
        
    def update_face_trajectory(self, face_id, center_point):
        """æ›´æ–°é¢éƒ¨è½¨è¿¹"""
        if face_id not in self.face_trajectories:
            self.face_trajectories[face_id] = deque(maxlen=self.max_points)
        self.face_trajectories[face_id].append(center_point)
    
    def update_hand_trajectory(self, hand_id, wrist_point):
        """æ›´æ–°æ‰‹éƒ¨è½¨è¿¹"""
        if hand_id not in self.hand_trajectories:
            self.hand_trajectories[hand_id] = deque(maxlen=self.max_points)
        self.hand_trajectories[hand_id].append(wrist_point)
    
    def update_pose_trajectory(self, keypoint_name, point):
        """æ›´æ–°å§¿åŠ¿å…³é”®ç‚¹è½¨è¿¹"""
        if keypoint_name not in self.pose_trajectories:
            self.pose_trajectories[keypoint_name] = deque(maxlen=self.max_points)
        self.pose_trajectories[keypoint_name].append(point)
    
    def draw_trajectories(self, frame):
        """ç»˜åˆ¶æ‰€æœ‰è½¨è¿¹"""
        # ç»˜åˆ¶é¢éƒ¨è½¨è¿¹
        for face_id, trajectory in self.face_trajectories.items():
            if len(trajectory) > 1:
                points = list(trajectory)
                for i in range(1, len(points)):
                    alpha = i / len(points)
                    color = (int(255 * alpha), int(100 * alpha), int(50 * alpha))
                    cv2.line(frame, points[i-1], points[i], color, 2)
        
        # ç»˜åˆ¶æ‰‹éƒ¨è½¨è¿¹
        for hand_id, trajectory in self.hand_trajectories.items():
            if len(trajectory) > 1:
                points = list(trajectory)
                for i in range(1, len(points)):
                    alpha = i / len(points)
                    color = (int(50 * alpha), int(255 * alpha), int(100 * alpha))
                    cv2.line(frame, points[i-1], points[i], color, 2)
        
        # ç»˜åˆ¶å…³é”®å§¿åŠ¿ç‚¹è½¨è¿¹
        for keypoint_name, trajectory in self.pose_trajectories.items():
            if len(trajectory) > 1:
                points = list(trajectory)
                for i in range(1, len(points)):
                    alpha = i / len(points)
                    color = (int(100 * alpha), int(50 * alpha), int(255 * alpha))
                    cv2.line(frame, points[i-1], points[i], color, 1)


class EnhancedMultimodalTester:
    """å¢å¼ºç‰ˆå¤šæ¨¡æ€è¯†åˆ«æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.face_recognizer = None
        self.gesture_recognizer = None
        self.pose_recognizer = None
        self.cap = None
        self.running = False
        self.frame_count = 0
        self.trajectory_tracker = TrajectoryTracker()
        
        # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        self.detection_stats = {
            'faces_detected': 0,
            'faces_recognized': 0,
            'hands_detected': 0,
            'gestures_classified': 0,
            'poses_detected': 0,
            'keypoints_tracked': 0,
            'trajectories_active': 0
        }
        
        # å½“å‰æ£€æµ‹çŠ¶æ€
        self.current_detections = {
            'faces': [],
            'hands': [],
            'pose_keypoints': [],
            'gestures': [],
            'pose_classification': None
        }
        
        # æ€§èƒ½ä¼˜åŒ–è®¾ç½®
        self.detection_interval = 2  # æ¯2å¸§æ£€æµ‹ä¸€æ¬¡ä»¥æé«˜é€Ÿåº¦
        self.frame_skip_counter = 0
        
    def initialize_recognizers(self):
        """åˆå§‹åŒ–è¯†åˆ«å™¨"""
        logger.info("ğŸš€ åˆå§‹åŒ–å¢å¼ºç‰ˆè¯†åˆ«å™¨...")
        
        # åˆå§‹åŒ–é¢éƒ¨è¯†åˆ«å™¨
        try:
            from recognition.face_recognizer import FaceRecognizer
            self.face_recognizer = FaceRecognizer(min_detection_confidence=0.6)
            logger.info("âœ… é¢éƒ¨è¯†åˆ«å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ é¢éƒ¨è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # åˆå§‹åŒ–æ‰‹åŠ¿è¯†åˆ«å™¨
        try:
            from recognition.gesture_recognizer import GestureRecognizer
            self.gesture_recognizer = GestureRecognizer(
                min_detection_confidence=0.6,
                min_tracking_confidence=0.5,
                max_num_hands=2
            )
            logger.info("âœ… æ‰‹åŠ¿è¯†åˆ«å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ æ‰‹åŠ¿è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # åˆå§‹åŒ–å§¿åŠ¿è¯†åˆ«å™¨
        try:
            from recognition.pose_recognizer import PoseRecognizer
            self.pose_recognizer = PoseRecognizer(
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5,
                model_complexity=1  # ä½¿ç”¨ä¸­ç­‰å¤æ‚åº¦ä»¥å¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦
            )
            logger.info("âœ… å§¿åŠ¿è¯†åˆ«å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ å§¿åŠ¿è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        return True
    
    def initialize_camera(self):
        """åˆå§‹åŒ–æ‘„åƒå¤´"""
        logger.info("ğŸ“· åˆå§‹åŒ–æ‘„åƒå¤´...")
        
        try:
            self.cap = cv2.VideoCapture(0)
            
            if not self.cap.isOpened():
                logger.error("âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
                return False
            
            # è®¾ç½®æ‘„åƒå¤´å‚æ•°
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            self.cap.set(cv2.CAP_PROP_FPS, 30)
            
            # æµ‹è¯•è¯»å–
            ret, frame = self.cap.read()
            if ret and frame is not None:
                logger.info(f"âœ… æ‘„åƒå¤´åˆå§‹åŒ–æˆåŠŸ: {frame.shape}")
                return True
            else:
                logger.error("âŒ æ‘„åƒå¤´æµ‹è¯•è¯»å–å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"âŒ æ‘„åƒå¤´åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def detect_faces_enhanced(self, frame):
        """å¢å¼ºç‰ˆé¢éƒ¨æ£€æµ‹"""
        faces_info = []
        
        if self.face_recognizer:
            try:
                annotated_frame, face_results = self.face_recognizer.detect_faces(frame)
                
                for i, face in enumerate(face_results):
                    face_info = {
                        'id': i,
                        'bbox': face.get('bbox', None),
                        'identity': face.get('identity', 'Unknown'),
                        'confidence': face.get('confidence', 0.0),
                        'landmarks': face.get('landmarks', None)
                    }
                    
                    # è®¡ç®—é¢éƒ¨ä¸­å¿ƒç‚¹ç”¨äºè½¨è¿¹è¿½è¸ª
                    if face_info['bbox']:
                        x1, y1, x2, y2 = face_info['bbox']
                        center = (int((x1 + x2) / 2), int((y1 + y2) / 2))
                        face_info['center'] = center
                        self.trajectory_tracker.update_face_trajectory(i, center)
                    
                    faces_info.append(face_info)
                    
                    # æ›´æ–°ç»Ÿè®¡
                    self.detection_stats['faces_detected'] += 1
                    if face_info['identity'] != 'Unknown':
                        self.detection_stats['faces_recognized'] += 1
                
                return annotated_frame, faces_info
                
            except Exception as e:
                logger.debug(f"é¢éƒ¨æ£€æµ‹é”™è¯¯: {e}")
        
        return frame, faces_info
    
    def detect_gestures_enhanced(self, frame):
        """å¢å¼ºç‰ˆæ‰‹åŠ¿æ£€æµ‹"""
        hands_info = []
        gestures_info = []
        
        if self.gesture_recognizer:
            try:
                annotated_frame, gesture_results = self.gesture_recognizer.detect_hands(frame)
                
                for i, hand in enumerate(gesture_results):
                    hand_info = {
                        'id': i,
                        'hand_label': hand.get('hand_label', 'Unknown'),
                        'gesture': hand.get('gesture', 'unknown'),
                        'gesture_name': hand.get('gesture_name', 'Unknown'),
                        'landmarks': hand.get('landmarks', []),
                        'bbox': hand.get('bbox', None)
                    }
                    
                    # æå–æ‰‹è…•ä½ç½®ç”¨äºè½¨è¿¹è¿½è¸ª
                    if hand_info['landmarks'] and len(hand_info['landmarks']) > 0:
                        wrist_point = hand_info['landmarks'][0]  # æ‰‹è…•æ˜¯ç¬¬0ä¸ªå…³é”®ç‚¹
                        self.trajectory_tracker.update_hand_trajectory(i, wrist_point)
                    
                    hands_info.append(hand_info)
                    
                    # å¦‚æœæ£€æµ‹åˆ°æœ‰æ•ˆæ‰‹åŠ¿ï¼Œæ·»åŠ åˆ°æ‰‹åŠ¿ä¿¡æ¯
                    if hand_info['gesture'] != 'unknown':
                        gestures_info.append({
                            'hand_id': i,
                            'gesture': hand_info['gesture'],
                            'gesture_name': hand_info['gesture_name'],
                            'hand_label': hand_info['hand_label']
                        })
                        self.detection_stats['gestures_classified'] += 1
                    
                    self.detection_stats['hands_detected'] += 1
                
                return annotated_frame, hands_info, gestures_info
                
            except Exception as e:
                logger.debug(f"æ‰‹åŠ¿æ£€æµ‹é”™è¯¯: {e}")
        
        return frame, hands_info, gestures_info
    
    def detect_pose_enhanced(self, frame):
        """å¢å¼ºç‰ˆå§¿åŠ¿æ£€æµ‹"""
        pose_info = {}
        keypoints_info = []
        
        if self.pose_recognizer:
            try:
                annotated_frame, pose_result = self.pose_recognizer.detect_pose(frame)
                
                if pose_result and pose_result.get('pose_detected', False):
                    pose_info = {
                        'detected': True,
                        'pose_type': pose_result.get('pose_type', 'unknown'),
                        'pose_name': pose_result.get('pose_name', 'Unknown'),
                        'confidence': pose_result.get('confidence', 0.0),
                        'angles': pose_result.get('angles', {}),
                        'body_ratios': pose_result.get('body_ratios', {})
                    }
                    
                    # å¤„ç†å…³é”®ç‚¹ä¿¡æ¯
                    landmarks = pose_result.get('landmarks', [])
                    if landmarks:
                        # MediaPipeå§¿åŠ¿å…³é”®ç‚¹åç§°æ˜ å°„
                        keypoint_names = [
                            'nose', 'left_eye_inner', 'left_eye', 'left_eye_outer',
                            'right_eye_inner', 'right_eye', 'right_eye_outer',
                            'left_ear', 'right_ear', 'mouth_left', 'mouth_right',
                            'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',
                            'left_wrist', 'right_wrist', 'left_pinky', 'right_pinky',
                            'left_index', 'right_index', 'left_thumb', 'right_thumb',
                            'left_hip', 'right_hip', 'left_knee', 'right_knee',
                            'left_ankle', 'right_ankle', 'left_heel', 'right_heel',
                            'left_foot_index', 'right_foot_index'
                        ]
                        
                        for i, landmark in enumerate(landmarks):
                            if i < len(keypoint_names) and len(landmark) >= 4:
                                x, y, z, visibility = landmark
                                if visibility > 0.5:  # åªå¤„ç†å¯è§çš„å…³é”®ç‚¹
                                    keypoint_info = {
                                        'name': keypoint_names[i],
                                        'position': (int(x), int(y)),
                                        'visibility': visibility,
                                        'z': z
                                    }
                                    keypoints_info.append(keypoint_info)
                                    
                                    # æ›´æ–°é‡è¦å…³é”®ç‚¹çš„è½¨è¿¹
                                    if keypoint_names[i] in ['left_wrist', 'right_wrist', 'nose', 'left_shoulder', 'right_shoulder']:
                                        self.trajectory_tracker.update_pose_trajectory(
                                            keypoint_names[i], (int(x), int(y))
                                        )
                    
                    self.detection_stats['poses_detected'] += 1
                    self.detection_stats['keypoints_tracked'] += len(keypoints_info)
                
                return annotated_frame, pose_info, keypoints_info
                
            except Exception as e:
                logger.debug(f"å§¿åŠ¿æ£€æµ‹é”™è¯¯: {e}")
        
        return frame, pose_info, keypoints_info
    
    def detect_multimodal_enhanced(self, frame):
        """å¢å¼ºç‰ˆå¤šæ¨¡æ€æ£€æµ‹"""
        # æ€§èƒ½ä¼˜åŒ–ï¼šè·³å¸§å¤„ç†
        self.frame_skip_counter += 1
        if self.frame_skip_counter % self.detection_interval != 0:
            # éæ£€æµ‹å¸§ï¼Œåªç»˜åˆ¶è½¨è¿¹å’Œä¹‹å‰çš„æ£€æµ‹ç»“æœ
            annotated_frame = frame.copy()
            self.draw_previous_detections(annotated_frame)
            self.trajectory_tracker.draw_trajectories(annotated_frame)
            return annotated_frame, self.current_detections
        
        # æ‰§è¡Œæ£€æµ‹
        annotated_frame = frame.copy()
        
        # é¢éƒ¨æ£€æµ‹
        face_frame, faces = self.detect_faces_enhanced(annotated_frame)
        annotated_frame = face_frame
        
        # æ‰‹åŠ¿æ£€æµ‹
        gesture_frame, hands, gestures = self.detect_gestures_enhanced(annotated_frame)
        annotated_frame = gesture_frame
        
        # å§¿åŠ¿æ£€æµ‹
        pose_frame, pose_info, keypoints = self.detect_pose_enhanced(annotated_frame)
        annotated_frame = pose_frame
        
        # æ›´æ–°å½“å‰æ£€æµ‹çŠ¶æ€
        self.current_detections = {
            'faces': faces,
            'hands': hands,
            'pose_keypoints': keypoints,
            'gestures': gestures,
            'pose_classification': pose_info
        }
        
        # ç»˜åˆ¶è½¨è¿¹
        self.trajectory_tracker.draw_trajectories(annotated_frame)
        
        # ç»˜åˆ¶å¢å¼ºä¿¡æ¯
        self.draw_enhanced_info(annotated_frame)
        
        return annotated_frame, self.current_detections
    
    def draw_previous_detections(self, frame):
        """ç»˜åˆ¶ä¹‹å‰çš„æ£€æµ‹ç»“æœï¼ˆç”¨äºè·³å¸§æ—¶ä¿æŒæ˜¾ç¤ºï¼‰"""
        # ç»˜åˆ¶é¢éƒ¨æ¡†
        for face in self.current_detections.get('faces', []):
            if face.get('bbox'):
                x1, y1, x2, y2 = face['bbox']
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                # ä½¿ç”¨è‹±æ–‡é¿å…ä¹±ç 
                label = f"Face {face['id']}"
                if face['identity'] != 'Unknown':
                    label += f" ({face['confidence']:.2f})"
                cv2.putText(frame, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        
        # ç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹
        for hand in self.current_detections.get('hands', []):
            landmarks = hand.get('landmarks', [])
            for point in landmarks:
                if len(point) >= 2:
                    cv2.circle(frame, (int(point[0]), int(point[1])), 3, (255, 0, 0), -1)
        
        # ç»˜åˆ¶å§¿åŠ¿å…³é”®ç‚¹
        for keypoint in self.current_detections.get('pose_keypoints', []):
            pos = keypoint.get('position')
            if pos:
                cv2.circle(frame, pos, 4, (0, 0, 255), -1)
    
    def draw_enhanced_info(self, frame):
        """ç»˜åˆ¶å¢å¼ºä¿¡æ¯æ˜¾ç¤º"""
        # åˆ›å»ºä¿¡æ¯é¢æ¿
        info_height = 200
        overlay = np.zeros((info_height, frame.shape[1], 3), dtype=np.uint8)
        
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.5
        color = (255, 255, 255)
        thickness = 1
        
        y_offset = 20
        line_height = 15
        
        # å½“å‰æ£€æµ‹çŠ¶æ€
        cv2.putText(overlay, "=== Real-time Detection Status ===", (10, y_offset), font, 0.6, (0, 255, 255), 2)
        y_offset += 25
        
        # é¢éƒ¨ä¿¡æ¯
        faces = self.current_detections.get('faces', [])
        cv2.putText(overlay, f"Faces Detected: {len(faces)}", (10, y_offset), font, font_scale, color, thickness)
        y_offset += line_height
        
        for i, face in enumerate(faces[:2]):  # æœ€å¤šæ˜¾ç¤º2ä¸ªé¢éƒ¨
            identity = face.get('identity', 'Unknown')
            confidence = face.get('confidence', 0.0)
            # ä½¿ç”¨è‹±æ–‡é¿å…ä¹±ç 
            face_text = f"  Face {i+1}: "
            if identity != 'Unknown':
                face_text += f"Recognized ({confidence:.2f})"
            else:
                face_text += "Unregistered"
            cv2.putText(overlay, face_text, (20, y_offset), font, font_scale, (0, 255, 0), thickness)
            y_offset += line_height
        
        # æ‰‹åŠ¿ä¿¡æ¯
        hands = self.current_detections.get('hands', [])
        gestures = self.current_detections.get('gestures', [])
        cv2.putText(overlay, f"Hands Detected: {len(hands)}", (10, y_offset), font, font_scale, color, thickness)
        y_offset += line_height
        
        for gesture in gestures[:2]:  # æœ€å¤šæ˜¾ç¤º2ä¸ªæ‰‹åŠ¿
            hand_label = gesture.get('hand_label', 'Unknown')
            gesture_name = gesture.get('gesture_name', 'Unknown')
            # ä½¿ç”¨è‹±æ–‡æ‰‹åŠ¿åç§°
            gesture_text = f"  {hand_label} Hand: {gesture.get('gesture', 'unknown')}"
            cv2.putText(overlay, gesture_text, (20, y_offset), font, font_scale, (255, 0, 0), thickness)
            y_offset += line_height
        
        # å§¿åŠ¿ä¿¡æ¯
        pose_info = self.current_detections.get('pose_classification', {})
        keypoints = self.current_detections.get('pose_keypoints', [])
        cv2.putText(overlay, f"Pose Keypoints: {len(keypoints)}", (10, y_offset), font, font_scale, color, thickness)
        y_offset += line_height
        
        if pose_info.get('detected', False):
            pose_type = pose_info.get('pose_type', 'unknown')
            confidence = pose_info.get('confidence', 0.0)
            pose_text = f"  Pose: {pose_type} ({confidence:.2f})"
            cv2.putText(overlay, pose_text, (20, y_offset), font, font_scale, (0, 0, 255), thickness)
            y_offset += line_height
        
        # è½¨è¿¹ä¿¡æ¯
        active_trajectories = (len(self.trajectory_tracker.face_trajectories) + 
                             len(self.trajectory_tracker.hand_trajectories) + 
                             len(self.trajectory_tracker.pose_trajectories))
        cv2.putText(overlay, f"Active Trajectories: {active_trajectories}", (10, y_offset), font, font_scale, (255, 255, 0), thickness)
        
        # å°†ä¿¡æ¯é¢æ¿å åŠ åˆ°ä¸»ç”»é¢
        frame[frame.shape[0]-info_height:, :] = cv2.addWeighted(
            frame[frame.shape[0]-info_height:, :], 0.3, overlay, 0.7, 0
        )
    
    def draw_main_overlay(self, frame, fps):
        """ç»˜åˆ¶ä¸»è¦ä¿¡æ¯è¦†ç›–å±‚"""
        # é¡¶éƒ¨çŠ¶æ€æ 
        overlay = frame.copy()
        cv2.rectangle(overlay, (0, 0), (frame.shape[1], 80), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6
        color = (255, 255, 255)
        thickness = 1
        
        # æ—¶é—´å’ŒFPS
        current_time = datetime.now().strftime("%H:%M:%S")
        cv2.putText(frame, f"Time: {current_time}", (10, 25), font, font_scale, color, thickness)
        cv2.putText(frame, f"FPS: {fps:.1f}", (10, 50), font, font_scale, color, thickness)
        
        # æ€»ä½“ç»Ÿè®¡
        total_detections = (self.detection_stats['faces_detected'] + 
                          self.detection_stats['hands_detected'] + 
                          self.detection_stats['poses_detected'])
        cv2.putText(frame, f"Frame: {self.frame_count} | Total Detections: {total_detections}", 
                   (200, 25), font, 0.5, color, thickness)
        
        # è¯¦ç»†ç»Ÿè®¡
        stats_text = (f"F:{self.detection_stats['faces_detected']} "
                     f"H:{self.detection_stats['hands_detected']} "
                     f"P:{self.detection_stats['poses_detected']} "
                     f"G:{self.detection_stats['gestures_classified']}")
        cv2.putText(frame, stats_text, (200, 50), font, 0.5, color, thickness)
        
        # æ§åˆ¶è¯´æ˜
        control_text = "Controls: [Q]uit | [S]ave | [R]eset | [SPACE]Pause | [T]rajectory"
        cv2.putText(frame, control_text, (10, frame.shape[0] - 10), font, 0.4, (0, 255, 0), 1)
    
    def save_screenshot(self, frame):
        """ä¿å­˜æˆªå›¾"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"enhanced_multimodal_{timestamp}.jpg"
            cv2.imwrite(filename, frame)
            logger.info(f"ğŸ“¸ æˆªå›¾å·²ä¿å­˜: {filename}")
        except Exception as e:
            logger.error(f"ä¿å­˜æˆªå›¾å¤±è´¥: {e}")
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡"""
        self.detection_stats = {
            'faces_detected': 0,
            'faces_recognized': 0,
            'hands_detected': 0,
            'gestures_classified': 0,
            'poses_detected': 0,
            'keypoints_tracked': 0,
            'trajectories_active': 0
        }
        self.frame_count = 0
        # æ¸…ç©ºè½¨è¿¹
        self.trajectory_tracker = TrajectoryTracker()
        logger.info("ğŸ“Š ç»Ÿè®¡å’Œè½¨è¿¹å·²é‡ç½®")
    
    def run_test(self):
        """è¿è¡Œå¢å¼ºç‰ˆæµ‹è¯•"""
        logger.info("ğŸ–¥ï¸ å¼€å§‹å¢å¼ºç‰ˆå¤šæ¨¡æ€è¯†åˆ«GUIæµ‹è¯•")
        
        # åˆå§‹åŒ–è¯†åˆ«å™¨
        if not self.initialize_recognizers():
            logger.error("âŒ è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥")
            return False
        
        # åˆå§‹åŒ–æ‘„åƒå¤´
        if not self.initialize_camera():
            logger.error("âŒ æ‘„åƒå¤´åˆå§‹åŒ–å¤±è´¥")
            return False
        
        # åˆ›å»ºçª—å£
        window_name = "Enhanced Multimodal Recognition - Full Feature Demo"
        cv2.namedWindow(window_name, cv2.WINDOW_AUTOSIZE)
        
        logger.info("ğŸ¬ å¼€å§‹å¢å¼ºç‰ˆå®æ—¶æ£€æµ‹...")
        
        self.running = True
        paused = False
        show_trajectories = True
        fps_counter = 0
        fps_start_time = time.time()
        current_fps = 0
        
        try:
            while self.running:
                if not paused:
                    # è¯»å–æ‘„åƒå¤´å¸§
                    ret, frame = self.cap.read()
                    
                    if not ret or frame is None:
                        logger.warning("âš ï¸ æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
                        continue
                    
                    self.frame_count += 1
                    fps_counter += 1
                    
                    # æ‰§è¡Œå¢å¼ºç‰ˆå¤šæ¨¡æ€æ£€æµ‹
                    start_time = time.time()
                    try:
                        annotated_frame, detections = self.detect_multimodal_enhanced(frame)
                        processing_time = time.time() - start_time
                    except Exception as e:
                        logger.warning(f"æ£€æµ‹å¤„ç†å¤±è´¥: {e}")
                        annotated_frame = frame
                        processing_time = 0
                    
                    # è®¡ç®—FPS
                    current_time = time.time()
                    if current_time - fps_start_time >= 1.0:
                        current_fps = fps_counter / (current_time - fps_start_time)
                        fps_counter = 0
                        fps_start_time = current_time
                    
                    # ç»˜åˆ¶ä¸»è¦ä¿¡æ¯è¦†ç›–å±‚
                    self.draw_main_overlay(annotated_frame, current_fps)
                    
                    # æ˜¾ç¤ºå¸§
                    cv2.imshow(window_name, annotated_frame)
                
                # å¤„ç†é”®ç›˜è¾“å…¥
                key = cv2.waitKey(1) & 0xFF
                
                if key == ord('q') or key == 27:  # 'q' æˆ– ESC é€€å‡º
                    logger.info("ç”¨æˆ·è¯·æ±‚é€€å‡º")
                    break
                elif key == ord('s'):  # 's' ä¿å­˜æˆªå›¾
                    if 'annotated_frame' in locals():
                        self.save_screenshot(annotated_frame)
                elif key == ord('r'):  # 'r' é‡ç½®ç»Ÿè®¡
                    self.reset_stats()
                elif key == ord(' '):  # ç©ºæ ¼é”®æš‚åœ/ç»§ç»­
                    paused = not paused
                    status = "Paused" if paused else "Resumed"
                    logger.info(f"ğŸ“¹ Video {status}")
                elif key == ord('t'):  # 't' åˆ‡æ¢è½¨è¿¹æ˜¾ç¤º
                    show_trajectories = not show_trajectories
                    logger.info(f"è½¨è¿¹æ˜¾ç¤º: {'å¼€å¯' if show_trajectories else 'å…³é—­'}")
                
                # æ£€æŸ¥çª—å£æ˜¯å¦è¢«å…³é—­
                if cv2.getWindowProperty(window_name, cv2.WND_PROP_VISIBLE) < 1:
                    logger.info("çª—å£è¢«å…³é—­")
                    break
            
            # ç”Ÿæˆè¯¦ç»†æµ‹è¯•æŠ¥å‘Š
            self.generate_detailed_report(current_fps)
            
            logger.info("âœ… å¢å¼ºç‰ˆæµ‹è¯•å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False
        
        finally:
            self.cleanup()
    
    def generate_detailed_report(self, fps):
        """ç”Ÿæˆè¯¦ç»†æµ‹è¯•æŠ¥å‘Š"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ“Š å¢å¼ºç‰ˆå¤šæ¨¡æ€è¯†åˆ«æµ‹è¯•è¯¦ç»†æŠ¥å‘Š")
        logger.info("="*80)
        logger.info(f"æ€»å¤„ç†å¸§æ•°: {self.frame_count}")
        logger.info(f"å¹³å‡FPS: {fps:.1f}")
        logger.info(f"æ£€æµ‹é—´éš”: æ¯{self.detection_interval}å¸§æ£€æµ‹ä¸€æ¬¡")
        
        logger.info("\nğŸ¯ æ£€æµ‹ç»Ÿè®¡:")
        logger.info(f"  é¢éƒ¨æ£€æµ‹æ¬¡æ•°: {self.detection_stats['faces_detected']}")
        logger.info(f"  é¢éƒ¨è¯†åˆ«æ¬¡æ•°: {self.detection_stats['faces_recognized']}")
        logger.info(f"  æ‰‹éƒ¨æ£€æµ‹æ¬¡æ•°: {self.detection_stats['hands_detected']}")
        logger.info(f"  æ‰‹åŠ¿åˆ†ç±»æ¬¡æ•°: {self.detection_stats['gestures_classified']}")
        logger.info(f"  å§¿åŠ¿æ£€æµ‹æ¬¡æ•°: {self.detection_stats['poses_detected']}")
        logger.info(f"  å…³é”®ç‚¹è¿½è¸ªæ¬¡æ•°: {self.detection_stats['keypoints_tracked']}")
        
        logger.info("\nğŸ¨ è½¨è¿¹è¿½è¸ªç»Ÿè®¡:")
        logger.info(f"  æ´»è·ƒé¢éƒ¨è½¨è¿¹: {len(self.trajectory_tracker.face_trajectories)}")
        logger.info(f"  æ´»è·ƒæ‰‹éƒ¨è½¨è¿¹: {len(self.trajectory_tracker.hand_trajectories)}")
        logger.info(f"  æ´»è·ƒå§¿åŠ¿è½¨è¿¹: {len(self.trajectory_tracker.pose_trajectories)}")
        
        # è®¡ç®—æ£€æµ‹ç‡
        if self.frame_count > 0:
            detection_frames = self.frame_count // self.detection_interval
            if detection_frames > 0:
                face_rate = (self.detection_stats['faces_detected'] / detection_frames) * 100
                hand_rate = (self.detection_stats['hands_detected'] / detection_frames) * 100
                pose_rate = (self.detection_stats['poses_detected'] / detection_frames) * 100
                
                logger.info("\nğŸ“ˆ æ£€æµ‹æˆåŠŸç‡:")
                logger.info(f"  é¢éƒ¨æ£€æµ‹ç‡: {face_rate:.1f}%")
                logger.info(f"  æ‰‹éƒ¨æ£€æµ‹ç‡: {hand_rate:.1f}%")
                logger.info(f"  å§¿åŠ¿æ£€æµ‹ç‡: {pose_rate:.1f}%")
        
        logger.info("\nâœ¨ åŠŸèƒ½éªŒè¯:")
        logger.info("  âœ… å¤šäººè„¸åŒæ—¶æ£€æµ‹å’Œè¯†åˆ«")
        logger.info("  âœ… åŒæ‰‹21ä¸ªå…³é”®ç‚¹è¿½è¸ª")
        logger.info("  âœ… å¤šç§æ‰‹åŠ¿åˆ†ç±»è¯†åˆ«")
        logger.info("  âœ… 33ä¸ªäººä½“å…³é”®ç‚¹æ£€æµ‹")
        logger.info("  âœ… å¤šç§å§¿åŠ¿åˆ†ç±»è¯†åˆ«")
        logger.info("  âœ… å®æ—¶è½¨è¿¹è¿½è¸ªå’Œå¯è§†åŒ–")
        logger.info("  âœ… å…³èŠ‚è§’åº¦å’Œèº«ä½“æ¯”ä¾‹è®¡ç®—")
        logger.info("="*80)
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†èµ„æº...")
        
        self.running = False
        
        # é‡Šæ”¾æ‘„åƒå¤´
        if self.cap:
            self.cap.release()
        
        # å…³é—­æ‰€æœ‰OpenCVçª—å£
        cv2.destroyAllWindows()
        
        # æ¸…ç†è¯†åˆ«å™¨
        if self.face_recognizer and hasattr(self.face_recognizer, 'close'):
            self.face_recognizer.close()
        if self.gesture_recognizer and hasattr(self.gesture_recognizer, 'close'):
            self.gesture_recognizer.close()
        if self.pose_recognizer and hasattr(self.pose_recognizer, 'close'):
            self.pose_recognizer.close()
        
        logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ–¥ï¸ å¢å¼ºç‰ˆå¤šæ¨¡æ€è¯†åˆ«GUIæµ‹è¯•")
    print("=" * 60)
    print("ğŸ¯ å®Œæ•´åŠŸèƒ½å±•ç¤º:")
    print("  â€¢ å¤šäººè„¸åŒæ—¶æ£€æµ‹å’Œèº«ä»½è¯†åˆ«")
    print("  â€¢ å®æ—¶é¢éƒ¨å…³é”®ç‚¹è¿½è¸ª")
    print("  â€¢ åŒæ‰‹21ä¸ªå…³é”®ç‚¹æ£€æµ‹")
    print("  â€¢ å¤šç§æ‰‹åŠ¿åˆ†ç±»(æ‹³å¤´ã€å¼ å¼€ã€ç‚¹èµç­‰)")
    print("  â€¢ 33ä¸ªäººä½“å…³é”®ç‚¹æ£€æµ‹")
    print("  â€¢ å¤šç§å§¿åŠ¿åˆ†ç±»(ç«™ç«‹ã€åç€ã€æŒ¥æ‰‹ç­‰)")
    print("  â€¢ å…³èŠ‚è§’åº¦å’Œèº«ä½“æ¯”ä¾‹è®¡ç®—")
    print("  â€¢ å®æ—¶æ´»åŠ¨è½¨è¿¹è¿½è¸ªå’Œå¯è§†åŒ–")
    print("  â€¢ å¼‚å¸¸è¡Œä¸ºæ£€æµ‹æ”¯æŒ")
    print()
    print("ğŸ® æ§åˆ¶è¯´æ˜:")
    print("  â€¢ æŒ‰ 'q' æˆ– ESC é€€å‡º")
    print("  â€¢ æŒ‰ 's' ä¿å­˜æˆªå›¾")
    print("  â€¢ æŒ‰ 'r' é‡ç½®ç»Ÿè®¡å’Œè½¨è¿¹")
    print("  â€¢ æŒ‰ç©ºæ ¼é”®æš‚åœ/ç»§ç»­")
    print("  â€¢ æŒ‰ 't' åˆ‡æ¢è½¨è¿¹æ˜¾ç¤º")
    print("=" * 60)
    
    try:
        tester = EnhancedMultimodalTester()
        success = tester.run_test()
        
        if success:
            print("\nğŸ‰ å¢å¼ºç‰ˆæµ‹è¯•å®Œæˆï¼æ‰€æœ‰å¤šæ¨¡æ€åŠŸèƒ½å·²éªŒè¯ï¼")
            sys.exit(0)
        else:
            print("\nâŒ æµ‹è¯•å¤±è´¥ï¼")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}")
        logger.error(f"ä¸»ç¨‹åºå¼‚å¸¸: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()