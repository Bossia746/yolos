#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¶…é«˜æ€§èƒ½å¤šæ¨¡æ€è¯†åˆ«GUIç³»ç»Ÿ
åŸºäºæœ€ä½³å®è·µé‡æ„ï¼Œè§£å†³å¡é¡¿å’Œå¤šæ‰‹æ£€æµ‹é—®é¢˜

å‚è€ƒæœ€ä½³å®è·µ:
- OpenCVå¤šçº¿ç¨‹ä¼˜åŒ–: https://nrsyed.com/2018/07/05/multithreading-with-opencv-python-to-improve-video-processing-performance/
- MediaPipeæ€§èƒ½ä¼˜åŒ–: https://mediapipe.readthedocs.io/en/latest/solutions/hands.html
- GUIæ€§èƒ½ä¼˜åŒ–: https://pysource.com/2024/10/15/increase-opencv-speed-by-2x-with-python-and-multithreading-tutorial/
"""

import cv2
import numpy as np
import threading
import queue
import time
import logging
from typing import Optional, Tuple, List, Dict, Any
from dataclasses import dataclass
from collections import deque
import mediapipe as mp
from concurrent.futures import ThreadPoolExecutor
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from src.recognition.optimized_gesture_recognizer import OptimizedGestureRecognizer
from src.recognition.optimized_face_recognizer import OptimizedFaceRecognizer
from src.recognition.optimized_pose_recognizer import OptimizedPoseRecognizer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ultra_performance_gui.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class FrameData:
    """å¸§æ•°æ®ç»“æ„"""
    frame: np.ndarray
    timestamp: float
    frame_id: int

@dataclass
class DetectionResult:
    """æ£€æµ‹ç»“æœç»“æ„"""
    frame: np.ndarray
    hands: List[Dict[str, Any]]
    faces: List[Dict[str, Any]]
    poses: List[Dict[str, Any]]
    fps: float
    processing_time: float
    frame_id: int

class ThreadSafeCamera:
    """çº¿ç¨‹å®‰å…¨çš„æ‘„åƒå¤´ç±»"""
    
    def __init__(self, camera_id: int = 0, buffer_size: int = 2):
        self.camera_id = camera_id
        self.buffer_size = buffer_size
        self.cap = None
        self.frame_queue = queue.Queue(maxsize=buffer_size)
        self.running = False
        self.capture_thread = None
        self.frame_count = 0
        self.fps_counter = deque(maxlen=30)
        
    def start(self) -> bool:
        """å¯åŠ¨æ‘„åƒå¤´æ•è·"""
        try:
            self.cap = cv2.VideoCapture(self.camera_id)
            if not self.cap.isOpened():
                logger.error(f"æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {self.camera_id}")
                return False
                
            # ä¼˜åŒ–æ‘„åƒå¤´è®¾ç½®
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
            self.cap.set(cv2.CAP_PROP_FPS, 30)
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # å‡å°‘ç¼“å†²åŒºå»¶è¿Ÿ
            
            self.running = True
            self.capture_thread = threading.Thread(target=self._capture_frames, daemon=True)
            self.capture_thread.start()
            
            logger.info(f"æ‘„åƒå¤´ {self.camera_id} å¯åŠ¨æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"æ‘„åƒå¤´å¯åŠ¨å¤±è´¥: {e}")
            return False
    
    def _capture_frames(self):
        """å¸§æ•è·çº¿ç¨‹"""
        last_time = time.time()
        
        while self.running:
            try:
                ret, frame = self.cap.read()
                if not ret:
                    logger.warning("æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
                    continue
                
                current_time = time.time()
                self.fps_counter.append(current_time - last_time)
                last_time = current_time
                
                frame_data = FrameData(
                    frame=frame,
                    timestamp=current_time,
                    frame_id=self.frame_count
                )
                
                # éé˜»å¡æ”¾å…¥é˜Ÿåˆ—
                try:
                    self.frame_queue.put_nowait(frame_data)
                    self.frame_count += 1
                except queue.Full:
                    # é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€æ—§çš„å¸§
                    try:
                        self.frame_queue.get_nowait()
                        self.frame_queue.put_nowait(frame_data)
                    except queue.Empty:
                        pass
                        
            except Exception as e:
                logger.error(f"å¸§æ•è·é”™è¯¯: {e}")
                time.sleep(0.01)
    
    def get_frame(self) -> Optional[FrameData]:
        """è·å–æœ€æ–°å¸§"""
        try:
            return self.frame_queue.get_nowait()
        except queue.Empty:
            return None
    
    def get_fps(self) -> float:
        """è·å–å®é™…FPS"""
        if len(self.fps_counter) < 2:
            return 0.0
        avg_interval = sum(self.fps_counter) / len(self.fps_counter)
        return 1.0 / avg_interval if avg_interval > 0 else 0.0
    
    def stop(self):
        """åœæ­¢æ‘„åƒå¤´"""
        self.running = False
        if self.capture_thread:
            self.capture_thread.join(timeout=1.0)
        if self.cap:
            self.cap.release()
        logger.info("æ‘„åƒå¤´å·²åœæ­¢")

class MultiModalDetector:
    """å¤šæ¨¡æ€æ£€æµ‹å™¨ - å¹¶è¡Œå¤„ç†"""
    
    def __init__(self):
        self.gesture_recognizer = None
        self.face_recognizer = None
        self.pose_recognizer = None
        self.executor = ThreadPoolExecutor(max_workers=3)
        self.detection_stats = {
            'total_detections': 0,
            'avg_processing_time': 0.0,
            'last_fps': 0.0
        }
        
    def initialize(self) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰æ£€æµ‹å™¨"""
        try:
            logger.info("åˆå§‹åŒ–å¤šæ¨¡æ€æ£€æµ‹å™¨...")
            
            # åˆå§‹åŒ–æ‰‹åŠ¿è¯†åˆ«å™¨ - ä¼˜åŒ–é…ç½®
            self.gesture_recognizer = OptimizedGestureRecognizer(
                max_num_hands=4,  # æ”¯æŒå¤šæ‰‹æ£€æµ‹
                min_detection_confidence=0.6,  # é™ä½é˜ˆå€¼æé«˜æ£€æµ‹ç‡
                min_tracking_confidence=0.4,   # é™ä½è·Ÿè¸ªé˜ˆå€¼
                enable_threading=True,
                enable_dynamic_gestures=True,
                enable_interaction_detection=True
            )
            
            # åˆå§‹åŒ–äººè„¸è¯†åˆ«å™¨
            self.face_recognizer = OptimizedFaceRecognizer(
                min_detection_confidence=0.6,
                enable_threading=True
            )
            
            # åˆå§‹åŒ–å§¿åŠ¿è¯†åˆ«å™¨
            self.pose_recognizer = OptimizedPoseRecognizer(
                min_detection_confidence=0.6,
                enable_threading=True
            )
            
            logger.info("å¤šæ¨¡æ€æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def detect_parallel(self, frame: np.ndarray) -> DetectionResult:
        """å¹¶è¡Œæ£€æµ‹"""
        start_time = time.time()
        
        # æäº¤å¹¶è¡Œä»»åŠ¡
        hand_future = self.executor.submit(self._detect_hands, frame)
        face_future = self.executor.submit(self._detect_faces, frame)
        pose_future = self.executor.submit(self._detect_poses, frame)
        
        # æ”¶é›†ç»“æœ
        try:
            hands = hand_future.result(timeout=0.1)  # 100msè¶…æ—¶
        except:
            hands = []
            
        try:
            faces = face_future.result(timeout=0.1)
        except:
            faces = []
            
        try:
            poses = pose_future.result(timeout=0.1)
        except:
            poses = []
        
        processing_time = time.time() - start_time
        fps = 1.0 / processing_time if processing_time > 0 else 0.0
        
        # æ›´æ–°ç»Ÿè®¡
        self._update_stats(processing_time, fps)
        
        return DetectionResult(
            frame=frame,
            hands=hands,
            faces=faces,
            poses=poses,
            fps=fps,
            processing_time=processing_time,
            frame_id=0
        )
    
    def _detect_hands(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """æ‰‹åŠ¿æ£€æµ‹"""
        try:
            if self.gesture_recognizer:
                result = self.gesture_recognizer.detect_gestures(frame)
                return [{
                    'type': 'hand',
                    'data': hand,
                    'confidence': hand.quality_score
                } for hand in result.hands_data]
        except Exception as e:
            logger.debug(f"æ‰‹åŠ¿æ£€æµ‹é”™è¯¯: {e}")
        return []
    
    def _detect_faces(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """äººè„¸æ£€æµ‹"""
        try:
            if self.face_recognizer:
                result = self.face_recognizer.detect_faces(frame)
                return [{
                    'type': 'face',
                    'data': face,
                    'confidence': face.get('confidence', 0.0)
                } for face in result.get('faces', [])]
        except Exception as e:
            logger.debug(f"äººè„¸æ£€æµ‹é”™è¯¯: {e}")
        return []
    
    def _detect_poses(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """å§¿åŠ¿æ£€æµ‹"""
        try:
            if self.pose_recognizer:
                result = self.pose_recognizer.detect_poses(frame)
                return [{
                    'type': 'pose',
                    'data': pose,
                    'confidence': pose.get('confidence', 0.0)
                } for pose in result.get('poses', [])]
        except Exception as e:
            logger.debug(f"å§¿åŠ¿æ£€æµ‹é”™è¯¯: {e}")
        return []
    
    def _update_stats(self, processing_time: float, fps: float):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.detection_stats['total_detections'] += 1
        self.detection_stats['avg_processing_time'] = (
            (self.detection_stats['avg_processing_time'] * (self.detection_stats['total_detections'] - 1) + processing_time) /
            self.detection_stats['total_detections']
        )
        self.detection_stats['last_fps'] = fps
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.detection_stats.copy()
    
    def close(self):
        """å…³é—­æ£€æµ‹å™¨"""
        self.executor.shutdown(wait=True)
        if self.gesture_recognizer:
            self.gesture_recognizer.close()
        if self.face_recognizer:
            self.face_recognizer.close()
        if self.pose_recognizer:
            self.pose_recognizer.close()

class UltraPerformanceGUI:
    """è¶…é«˜æ€§èƒ½GUIä¸»ç±»"""
    
    def __init__(self):
        self.camera = None
        self.detector = None
        self.running = False
        self.display_thread = None
        self.result_queue = queue.Queue(maxsize=5)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.frame_times = deque(maxlen=30)
        self.total_frames = 0
        self.start_time = time.time()
        
        # GUIè®¾ç½®
        self.window_name = "è¶…é«˜æ€§èƒ½å¤šæ¨¡æ€è¯†åˆ«ç³»ç»Ÿ"
        self.window_size = (1280, 720)
        
    def initialize(self) -> bool:
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        logger.info("ğŸš€ å¯åŠ¨è¶…é«˜æ€§èƒ½å¤šæ¨¡æ€è¯†åˆ«GUIç³»ç»Ÿ")
        
        # åˆå§‹åŒ–æ‘„åƒå¤´
        self.camera = ThreadSafeCamera(camera_id=0, buffer_size=2)
        if not self.camera.start():
            logger.error("æ‘„åƒå¤´åˆå§‹åŒ–å¤±è´¥")
            return False
        
        # åˆå§‹åŒ–æ£€æµ‹å™¨
        self.detector = MultiModalDetector()
        if not self.detector.initialize():
            logger.error("æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥")
            return False
        
        # åˆ›å»ºGUIçª—å£
        cv2.namedWindow(self.window_name, cv2.WINDOW_NORMAL)
        cv2.resizeWindow(self.window_name, *self.window_size)
        cv2.setWindowProperty(self.window_name, cv2.WND_PROP_TOPMOST, 1)
        
        logger.info("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return True
    
    def run(self):
        """è¿è¡Œä¸»å¾ªç¯"""
        if not self.initialize():
            logger.error("ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return
        
        self.running = True
        
        # å¯åŠ¨æ˜¾ç¤ºçº¿ç¨‹
        self.display_thread = threading.Thread(target=self._display_loop, daemon=True)
        self.display_thread.start()
        
        logger.info("ğŸ¯ å¼€å§‹ä¸»å¤„ç†å¾ªç¯")
        
        try:
            self._main_loop()
        except KeyboardInterrupt:
            logger.info("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        except Exception as e:
            logger.error(f"ä¸»å¾ªç¯é”™è¯¯: {e}")
        finally:
            self._cleanup()
    
    def _main_loop(self):
        """ä¸»å¤„ç†å¾ªç¯"""
        frame_skip = 0  # å¸§è·³è·ƒè®¡æ•°
        
        while self.running:
            frame_start = time.time()
            
            # è·å–æœ€æ–°å¸§
            frame_data = self.camera.get_frame()
            if frame_data is None:
                time.sleep(0.001)  # 1msç­‰å¾…
                continue
            
            # å¸§è·³è·ƒä¼˜åŒ– - æ¯3å¸§å¤„ç†ä¸€æ¬¡æ£€æµ‹
            if frame_skip % 3 == 0:
                # æ‰§è¡Œæ£€æµ‹
                detection_result = self.detector.detect_parallel(frame_data.frame)
                detection_result.frame_id = frame_data.frame_id
                
                # éé˜»å¡æ”¾å…¥æ˜¾ç¤ºé˜Ÿåˆ—
                try:
                    self.result_queue.put_nowait(detection_result)
                except queue.Full:
                    # é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€æ—§ç»“æœ
                    try:
                        self.result_queue.get_nowait()
                        self.result_queue.put_nowait(detection_result)
                    except queue.Empty:
                        pass
            else:
                # è·³è¿‡æ£€æµ‹ï¼Œç›´æ¥æ˜¾ç¤ºåŸå§‹å¸§
                simple_result = DetectionResult(
                    frame=frame_data.frame,
                    hands=[], faces=[], poses=[],
                    fps=0.0, processing_time=0.0,
                    frame_id=frame_data.frame_id
                )
                try:
                    self.result_queue.put_nowait(simple_result)
                except queue.Full:
                    pass
            
            frame_skip += 1
            self.total_frames += 1
            
            # è®°å½•å¸§æ—¶é—´
            frame_time = time.time() - frame_start
            self.frame_times.append(frame_time)
            
            # æ¯100å¸§è¾“å‡ºä¸€æ¬¡ç»Ÿè®¡
            if self.total_frames % 100 == 0:
                self._log_performance_stats()
            
            # æ§åˆ¶å¸§ç‡ - ç›®æ ‡30FPS
            target_frame_time = 1.0 / 30.0
            if frame_time < target_frame_time:
                time.sleep(target_frame_time - frame_time)
    
    def _display_loop(self):
        """æ˜¾ç¤ºå¾ªç¯çº¿ç¨‹"""
        logger.info("ğŸ–¥ï¸ æ˜¾ç¤ºçº¿ç¨‹å¯åŠ¨")
        
        while self.running:
            try:
                # è·å–æ£€æµ‹ç»“æœ
                result = self.result_queue.get(timeout=0.1)
                
                # ç»˜åˆ¶æ³¨é‡Š
                annotated_frame = self._draw_annotations(result)
                
                # æ˜¾ç¤ºå¸§
                cv2.imshow(self.window_name, annotated_frame)
                
                # æ£€æŸ¥æŒ‰é”®
                key = cv2.waitKey(1) & 0xFF
                if key == 27:  # ESCé”®
                    self.running = False
                    break
                elif key == ord('q'):
                    self.running = False
                    break
                    
            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"æ˜¾ç¤ºå¾ªç¯é”™è¯¯: {e}")
                time.sleep(0.01)
    
    def _draw_annotations(self, result: DetectionResult) -> np.ndarray:
        """ç»˜åˆ¶æ³¨é‡Šä¿¡æ¯"""
        frame = result.frame.copy()
        
        # ç»˜åˆ¶æ‰‹åŠ¿
        for hand in result.hands:
            if hand['type'] == 'hand':
                self._draw_hand_info(frame, hand['data'])
        
        # ç»˜åˆ¶äººè„¸
        for face in result.faces:
            if face['type'] == 'face':
                self._draw_face_info(frame, face['data'])
        
        # ç»˜åˆ¶å§¿åŠ¿
        for pose in result.poses:
            if pose['type'] == 'pose':
                self._draw_pose_info(frame, pose['data'])
        
        # ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯
        self._draw_performance_info(frame, result)
        
        return frame
    
    def _draw_hand_info(self, frame: np.ndarray, hand_data: Any):
        """ç»˜åˆ¶æ‰‹éƒ¨ä¿¡æ¯"""
        try:
            if hasattr(hand_data, 'landmarks') and hand_data.landmarks:
                # ç»˜åˆ¶å…³é”®ç‚¹
                for i, (x, y) in enumerate(hand_data.landmarks):
                    cv2.circle(frame, (int(x), int(y)), 3, (0, 255, 0), -1)
                
                # ç»˜åˆ¶è¾¹ç•Œæ¡†
                if hasattr(hand_data, 'bbox'):
                    x1, y1, x2, y2 = hand_data.bbox
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                
                # ç»˜åˆ¶æ‰‹åŠ¿æ ‡ç­¾
                if hasattr(hand_data, 'static_gesture'):
                    gesture = hand_data.static_gesture.get('gesture', 'Unknown')
                    confidence = hand_data.static_gesture.get('confidence', 0.0)
                    label = f"{gesture} ({confidence:.2f})"
                    cv2.putText(frame, label, (int(hand_data.landmarks[0][0]), int(hand_data.landmarks[0][1]) - 10),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        except Exception as e:
            logger.debug(f"ç»˜åˆ¶æ‰‹éƒ¨ä¿¡æ¯é”™è¯¯: {e}")
    
    def _draw_face_info(self, frame: np.ndarray, face_data: Dict[str, Any]):
        """ç»˜åˆ¶äººè„¸ä¿¡æ¯"""
        try:
            if 'bbox' in face_data:
                x1, y1, x2, y2 = face_data['bbox']
                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)
                cv2.putText(frame, f"Face ({face_data.get('confidence', 0.0):.2f})",
                           (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
        except Exception as e:
            logger.debug(f"ç»˜åˆ¶äººè„¸ä¿¡æ¯é”™è¯¯: {e}")
    
    def _draw_pose_info(self, frame: np.ndarray, pose_data: Dict[str, Any]):
        """ç»˜åˆ¶å§¿åŠ¿ä¿¡æ¯"""
        try:
            if 'keypoints' in pose_data:
                for x, y in pose_data['keypoints']:
                    cv2.circle(frame, (int(x), int(y)), 2, (0, 0, 255), -1)
        except Exception as e:
            logger.debug(f"ç»˜åˆ¶å§¿åŠ¿ä¿¡æ¯é”™è¯¯: {e}")
    
    def _draw_performance_info(self, frame: np.ndarray, result: DetectionResult):
        """ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯"""
        try:
            # è®¡ç®—FPS
            camera_fps = self.camera.get_fps()
            avg_frame_time = sum(self.frame_times) / len(self.frame_times) if self.frame_times else 0
            display_fps = 1.0 / avg_frame_time if avg_frame_time > 0 else 0
            
            # æ€§èƒ½æ–‡æœ¬
            perf_text = [
                f"Camera FPS: {camera_fps:.1f}",
                f"Display FPS: {display_fps:.1f}",
                f"Processing: {result.processing_time*1000:.1f}ms",
                f"Hands: {len(result.hands)}",
                f"Faces: {len(result.faces)}",
                f"Poses: {len(result.poses)}",
                f"Total Frames: {self.total_frames}"
            ]
            
            # ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯
            y_offset = 30
            for i, text in enumerate(perf_text):
                cv2.putText(frame, text, (10, y_offset + i * 25),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                cv2.putText(frame, text, (10, y_offset + i * 25),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)
            
        except Exception as e:
            logger.debug(f"ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯é”™è¯¯: {e}")
    
    def _log_performance_stats(self):
        """è®°å½•æ€§èƒ½ç»Ÿè®¡"""
        try:
            runtime = time.time() - self.start_time
            avg_fps = self.total_frames / runtime if runtime > 0 else 0
            camera_fps = self.camera.get_fps()
            detector_stats = self.detector.get_stats()
            
            logger.info(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡ - æ€»å¸§æ•°: {self.total_frames}, "
                       f"å¹³å‡FPS: {avg_fps:.1f}, æ‘„åƒå¤´FPS: {camera_fps:.1f}, "
                       f"æ£€æµ‹FPS: {detector_stats['last_fps']:.1f}")
        except Exception as e:
            logger.debug(f"æ€§èƒ½ç»Ÿè®¡é”™è¯¯: {e}")
    
    def _cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†ç³»ç»Ÿèµ„æº...")
        
        self.running = False
        
        if self.display_thread:
            self.display_thread.join(timeout=1.0)
        
        if self.camera:
            self.camera.stop()
        
        if self.detector:
            self.detector.close()
        
        cv2.destroyAllWindows()
        
        # æœ€ç»ˆç»Ÿè®¡
        runtime = time.time() - self.start_time
        avg_fps = self.total_frames / runtime if runtime > 0 else 0
        logger.info(f"ğŸ ç³»ç»Ÿè¿è¡Œå®Œæˆ - æ€»è¿è¡Œæ—¶é—´: {runtime:.1f}s, "
                   f"æ€»å¸§æ•°: {self.total_frames}, å¹³å‡FPS: {avg_fps:.1f}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        gui = UltraPerformanceGUI()
        gui.run()
    except Exception as e:
        logger.error(f"ç¨‹åºè¿è¡Œé”™è¯¯: {e}")
        return 1
    return 0

if __name__ == "__main__":
    sys.exit(main())