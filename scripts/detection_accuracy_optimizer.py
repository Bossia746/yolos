#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ£€æµ‹ç²¾åº¦ä¼˜åŒ–æ¨¡å—
å®ç°å¤šç§æ£€æµ‹ç²¾åº¦æå‡ç­–ç•¥
"""

import cv2
import numpy as np
from pathlib import Path
import json
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass

try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False

@dataclass
class OptimizationConfig:
    """ä¼˜åŒ–é…ç½®"""
    # å›¾åƒé¢„å¤„ç†
    enable_preprocessing: bool = True
    resize_strategy: str = "adaptive"  # "fixed", "adaptive", "multi_scale"
    target_size: int = 640
    
    # æ£€æµ‹å‚æ•°
    conf_threshold: float = 0.25
    iou_threshold: float = 0.45
    max_det: int = 300
    
    # åå¤„ç†ä¼˜åŒ–
    enable_nms_optimization: bool = True
    enable_confidence_calibration: bool = True
    
    # å¤šå°ºåº¦æ£€æµ‹
    enable_multi_scale: bool = False
    scales: List[int] = None
    
    # æµ‹è¯•æ—¶å¢å¼º (TTA)
    enable_tta: bool = False
    tta_scales: List[float] = None
    tta_flips: List[bool] = None

class DetectionAccuracyOptimizer:
    """æ£€æµ‹ç²¾åº¦ä¼˜åŒ–å™¨"""
    
    def __init__(self, model_path: str = "yolov8n.pt", config: OptimizationConfig = None):
        self.model_path = model_path
        self.config = config or OptimizationConfig()
        self.model = None
        self.optimization_stats = {
            'total_optimizations': 0,
            'accuracy_improvements': [],
            'processing_time_overhead': []
        }
        
        # è®¾ç½®é»˜è®¤å€¼
        if self.config.scales is None:
            self.config.scales = [480, 640, 800]
        if self.config.tta_scales is None:
            self.config.tta_scales = [0.8, 1.0, 1.2]
        if self.config.tta_flips is None:
            self.config.tta_flips = [False, True]
        
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        if not ULTRALYTICS_AVAILABLE:
            print("âŒ Ultralyticsåº“ä¸å¯ç”¨")
            return
        
        try:
            self.model = YOLO(self.model_path)
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_path}")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    def optimize_detection(self, image: np.ndarray) -> Dict[str, Any]:
        """ä¼˜åŒ–æ£€æµ‹æµç¨‹"""
        if self.model is None:
            return self._create_error_result("æ¨¡å‹æœªåŠ è½½")
        
        start_time = time.time()
        
        # 1. å›¾åƒé¢„å¤„ç†ä¼˜åŒ–
        if self.config.enable_preprocessing:
            processed_image = self._preprocess_image(image)
        else:
            processed_image = image
        
        # 2. å¤šå°ºåº¦æ£€æµ‹
        if self.config.enable_multi_scale:
            detection_result = self._multi_scale_detection(processed_image)
        elif self.config.enable_tta:
            detection_result = self._tta_detection(processed_image)
        else:
            detection_result = self._single_scale_detection(processed_image)
        
        # 3. åå¤„ç†ä¼˜åŒ–
        if self.config.enable_nms_optimization:
            detection_result = self._optimize_nms(detection_result)
        
        if self.config.enable_confidence_calibration:
            detection_result = self._calibrate_confidence(detection_result)
        
        processing_time = time.time() - start_time
        detection_result['optimization_time'] = processing_time
        detection_result['optimization_config'] = self.config.__dict__
        
        # æ›´æ–°ç»Ÿè®¡
        self._update_stats(detection_result, processing_time)
        
        return detection_result
    
    def _preprocess_image(self, image: np.ndarray) -> np.ndarray:
        """å›¾åƒé¢„å¤„ç†ä¼˜åŒ–"""
        processed = image.copy()
        
        # 1. è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–
        if len(processed.shape) == 3:
            # è½¬æ¢åˆ°LABè‰²å½©ç©ºé—´
            lab = cv2.cvtColor(processed, cv2.COLOR_BGR2LAB)
            l, a, b = cv2.split(lab)
            
            # å¯¹Lé€šé“è¿›è¡ŒCLAHE
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            l = clahe.apply(l)
            
            # åˆå¹¶é€šé“
            processed = cv2.merge([l, a, b])
            processed = cv2.cvtColor(processed, cv2.COLOR_LAB2BGR)
        
        # 2. å»å™ª
        processed = cv2.bilateralFilter(processed, 9, 75, 75)
        
        # 3. é”åŒ–
        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
        processed = cv2.filter2D(processed, -1, kernel)
        
        # 4. å°ºå¯¸è°ƒæ•´ç­–ç•¥
        if self.config.resize_strategy == "adaptive":
            processed = self._adaptive_resize(processed)
        elif self.config.resize_strategy == "fixed":
            processed = cv2.resize(processed, (self.config.target_size, self.config.target_size))
        
        return processed
    
    def _adaptive_resize(self, image: np.ndarray) -> np.ndarray:
        """è‡ªé€‚åº”å°ºå¯¸è°ƒæ•´"""
        h, w = image.shape[:2]
        
        # è®¡ç®—æœ€ä½³å°ºå¯¸
        max_size = max(h, w)
        if max_size > self.config.target_size:
            scale = self.config.target_size / max_size
            new_w = int(w * scale)
            new_h = int(h * scale)
            
            # ç¡®ä¿å°ºå¯¸æ˜¯32çš„å€æ•°ï¼ˆYOLOè¦æ±‚ï¼‰
            new_w = (new_w // 32) * 32
            new_h = (new_h // 32) * 32
            
            return cv2.resize(image, (new_w, new_h))
        
        return image
    
    def _single_scale_detection(self, image: np.ndarray) -> Dict[str, Any]:
        """å•å°ºåº¦æ£€æµ‹"""
        results = self.model(
            image,
            conf=self.config.conf_threshold,
            iou=self.config.iou_threshold,
            max_det=self.config.max_det,
            verbose=False
        )
        
        return self._parse_yolo_results(results[0])
    
    def _multi_scale_detection(self, image: np.ndarray) -> Dict[str, Any]:
        """å¤šå°ºåº¦æ£€æµ‹"""
        all_detections = []
        
        for scale in self.config.scales:
            # è°ƒæ•´å›¾åƒå°ºå¯¸
            h, w = image.shape[:2]
            scale_factor = scale / max(h, w)
            new_w = int(w * scale_factor)
            new_h = int(h * scale_factor)
            
            # ç¡®ä¿å°ºå¯¸æ˜¯32çš„å€æ•°
            new_w = (new_w // 32) * 32
            new_h = (new_h // 32) * 32
            
            scaled_image = cv2.resize(image, (new_w, new_h))
            
            # æ£€æµ‹
            results = self.model(
                scaled_image,
                conf=self.config.conf_threshold * 0.8,  # é™ä½é˜ˆå€¼
                iou=self.config.iou_threshold,
                max_det=self.config.max_det,
                verbose=False
            )
            
            # å°†æ£€æµ‹ç»“æœç¼©æ”¾å›åŸå§‹å°ºå¯¸
            scale_back_x = w / new_w
            scale_back_y = h / new_h
            
            if results[0].boxes is not None:
                for box in results[0].boxes:
                    x1, y1, x2, y2 = box.xyxy[0].tolist()
                    
                    # ç¼©æ”¾åæ ‡
                    x1 *= scale_back_x
                    y1 *= scale_back_y
                    x2 *= scale_back_x
                    y2 *= scale_back_y
                    
                    detection = {
                        'bbox': [x1, y1, x2, y2],
                        'confidence': float(box.conf[0]),
                        'class_id': int(box.cls[0]),
                        'class': self.model.names[int(box.cls[0])],
                        'scale': scale
                    }
                    all_detections.append(detection)
        
        # åˆå¹¶å¤šå°ºåº¦æ£€æµ‹ç»“æœ
        merged_detections = self._merge_multi_scale_detections(all_detections)
        
        return {
            'objects': merged_detections,
            'objects_count': len(merged_detections),
            'method': 'multi_scale',
            'scales_used': self.config.scales,
            'status': 'success'
        }
    
    def _tta_detection(self, image: np.ndarray) -> Dict[str, Any]:
        """æµ‹è¯•æ—¶å¢å¼ºæ£€æµ‹"""
        all_detections = []
        
        for scale in self.config.tta_scales:
            for flip in self.config.tta_flips:
                # åº”ç”¨å˜æ¢
                transformed_image = image.copy()
                
                # ç¼©æ”¾
                if scale != 1.0:
                    h, w = image.shape[:2]
                    new_h, new_w = int(h * scale), int(w * scale)
                    transformed_image = cv2.resize(transformed_image, (new_w, new_h))
                
                # ç¿»è½¬
                if flip:
                    transformed_image = cv2.flip(transformed_image, 1)
                
                # æ£€æµ‹
                results = self.model(
                    transformed_image,
                    conf=self.config.conf_threshold * 0.9,
                    iou=self.config.iou_threshold,
                    max_det=self.config.max_det,
                    verbose=False
                )
                
                # é€†å˜æ¢æ£€æµ‹ç»“æœ
                if results[0].boxes is not None:
                    for box in results[0].boxes:
                        x1, y1, x2, y2 = box.xyxy[0].tolist()
                        
                        # é€†ç¼©æ”¾
                        if scale != 1.0:
                            x1 /= scale
                            y1 /= scale
                            x2 /= scale
                            y2 /= scale
                        
                        # é€†ç¿»è½¬
                        if flip:
                            img_w = image.shape[1]
                            x1, x2 = img_w - x2, img_w - x1
                        
                        detection = {
                            'bbox': [x1, y1, x2, y2],
                            'confidence': float(box.conf[0]),
                            'class_id': int(box.cls[0]),
                            'class': self.model.names[int(box.cls[0])],
                            'tta_params': {'scale': scale, 'flip': flip}
                        }
                        all_detections.append(detection)
        
        # åˆå¹¶TTAç»“æœ
        merged_detections = self._merge_tta_detections(all_detections)
        
        return {
            'objects': merged_detections,
            'objects_count': len(merged_detections),
            'method': 'tta',
            'tta_config': {
                'scales': self.config.tta_scales,
                'flips': self.config.tta_flips
            },
            'status': 'success'
        }
    
    def _merge_multi_scale_detections(self, detections: List[Dict]) -> List[Dict]:
        """åˆå¹¶å¤šå°ºåº¦æ£€æµ‹ç»“æœ"""
        if not detections:
            return []
        
        # æŒ‰ç±»åˆ«åˆ†ç»„
        class_groups = {}
        for det in detections:
            class_id = det['class_id']
            if class_id not in class_groups:
                class_groups[class_id] = []
            class_groups[class_id].append(det)
        
        merged = []
        for class_id, group in class_groups.items():
            # å¯¹æ¯ä¸ªç±»åˆ«åº”ç”¨NMS
            boxes = np.array([det['bbox'] for det in group])
            scores = np.array([det['confidence'] for det in group])
            
            # OpenCV NMS
            indices = cv2.dnn.NMSBoxes(
                boxes.tolist(), scores.tolist(), 
                self.config.conf_threshold, self.config.iou_threshold
            )
            
            if len(indices) > 0:
                for i in indices.flatten():
                    merged.append(group[i])
        
        return merged
    
    def _merge_tta_detections(self, detections: List[Dict]) -> List[Dict]:
        """åˆå¹¶TTAæ£€æµ‹ç»“æœ"""
        if not detections:
            return []
        
        # ä½¿ç”¨åŠ æƒå¹³å‡åˆå¹¶ç›¸ä¼¼æ£€æµ‹
        merged = []
        used_indices = set()
        
        for i, det1 in enumerate(detections):
            if i in used_indices:
                continue
            
            similar_detections = [det1]
            used_indices.add(i)
            
            for j, det2 in enumerate(detections[i+1:], i+1):
                if j in used_indices:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºç›¸ä¼¼æ£€æµ‹
                if (det1['class_id'] == det2['class_id'] and 
                    self._calculate_iou(det1['bbox'], det2['bbox']) > 0.5):
                    similar_detections.append(det2)
                    used_indices.add(j)
            
            # åˆå¹¶ç›¸ä¼¼æ£€æµ‹
            if len(similar_detections) > 1:
                merged_det = self._average_detections(similar_detections)
                merged.append(merged_det)
            else:
                merged.append(det1)
        
        return merged
    
    def _average_detections(self, detections: List[Dict]) -> Dict:
        """å¹³å‡å¤šä¸ªæ£€æµ‹ç»“æœ"""
        weights = [det['confidence'] for det in detections]
        total_weight = sum(weights)
        
        # åŠ æƒå¹³å‡è¾¹ç•Œæ¡†
        avg_bbox = [0, 0, 0, 0]
        for det, weight in zip(detections, weights):
            for i in range(4):
                avg_bbox[i] += det['bbox'][i] * weight / total_weight
        
        # å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = sum(weights) / len(weights)
        
        return {
            'bbox': avg_bbox,
            'confidence': avg_confidence,
            'class_id': detections[0]['class_id'],
            'class': detections[0]['class'],
            'merged_count': len(detections)
        }
    
    def _calculate_iou(self, box1: List[float], box2: List[float]) -> float:
        """è®¡ç®—IoU"""
        x1_1, y1_1, x2_1, y2_1 = box1
        x1_2, y1_2, x2_2, y2_2 = box2
        
        # è®¡ç®—äº¤é›†
        x1_i = max(x1_1, x1_2)
        y1_i = max(y1_1, y1_2)
        x2_i = min(x2_1, x2_2)
        y2_i = min(y2_1, y2_2)
        
        if x2_i <= x1_i or y2_i <= y1_i:
            return 0.0
        
        intersection = (x2_i - x1_i) * (y2_i - y1_i)
        
        # è®¡ç®—å¹¶é›†
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def _optimize_nms(self, detection_result: Dict[str, Any]) -> Dict[str, Any]:
        """ä¼˜åŒ–NMS"""
        if not detection_result.get('objects'):
            return detection_result
        
        # å®ç°æ›´ç²¾ç»†çš„NMSç­–ç•¥
        objects = detection_result['objects']
        
        # æŒ‰ç±»åˆ«åˆ†ç»„
        class_groups = {}
        for obj in objects:
            class_id = obj['class_id']
            if class_id not in class_groups:
                class_groups[class_id] = []
            class_groups[class_id].append(obj)
        
        optimized_objects = []
        for class_id, group in class_groups.items():
            if len(group) <= 1:
                optimized_objects.extend(group)
                continue
            
            # è½¯NMSå®ç°
            group_sorted = sorted(group, key=lambda x: x['confidence'], reverse=True)
            keep = []
            
            while group_sorted:
                current = group_sorted.pop(0)
                keep.append(current)
                
                # é™ä½é‡å æ£€æµ‹çš„ç½®ä¿¡åº¦
                remaining = []
                for det in group_sorted:
                    iou = self._calculate_iou(current['bbox'], det['bbox'])
                    if iou > 0.3:  # è½¯é˜ˆå€¼
                        # é™ä½ç½®ä¿¡åº¦è€Œä¸æ˜¯å®Œå…¨åˆ é™¤
                        det['confidence'] *= (1 - iou)
                    
                    if det['confidence'] > self.config.conf_threshold * 0.5:
                        remaining.append(det)
                
                group_sorted = remaining
            
            optimized_objects.extend(keep)
        
        detection_result['objects'] = optimized_objects
        detection_result['objects_count'] = len(optimized_objects)
        detection_result['nms_optimized'] = True
        
        return detection_result
    
    def _calibrate_confidence(self, detection_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç½®ä¿¡åº¦æ ¡å‡†"""
        if not detection_result.get('objects'):
            return detection_result
        
        for obj in detection_result['objects']:
            original_conf = obj['confidence']
            
            # åŸºäºè¾¹ç•Œæ¡†å¤§å°çš„æ ¡å‡†
            bbox = obj['bbox']
            area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
            
            # é¢ç§¯æ ¡å‡†å› å­
            if area < 1000:  # å°ç‰©ä½“
                area_factor = 0.9
            elif area > 50000:  # å¤§ç‰©ä½“
                area_factor = 1.1
            else:
                area_factor = 1.0
            
            # åº”ç”¨æ ¡å‡†
            calibrated_conf = original_conf * area_factor
            calibrated_conf = max(0.0, min(1.0, calibrated_conf))  # é™åˆ¶åœ¨[0,1]
            
            obj['confidence'] = calibrated_conf
            obj['original_confidence'] = original_conf
        
        detection_result['confidence_calibrated'] = True
        return detection_result
    
    def _parse_yolo_results(self, result) -> Dict[str, Any]:
        """è§£æYOLOç»“æœ"""
        detection_info = {
            'objects': [],
            'objects_count': 0,
            'status': 'success'
        }
        
        if result.boxes is not None and len(result.boxes) > 0:
            for box in result.boxes:
                confidence = float(box.conf[0])
                class_id = int(box.cls[0])
                class_name = self.model.names[class_id]
                
                x1, y1, x2, y2 = box.xyxy[0].tolist()
                
                obj_info = {
                    'class': class_name,
                    'confidence': confidence,
                    'bbox': [x1, y1, x2, y2],
                    'class_id': class_id
                }
                
                detection_info['objects'].append(obj_info)
            
            detection_info['objects_count'] = len(detection_info['objects'])
        
        return detection_info
    
    def _create_error_result(self, error_msg: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return {
            'error': error_msg,
            'objects': [],
            'objects_count': 0,
            'status': 'error'
        }
    
    def _update_stats(self, result: Dict[str, Any], processing_time: float):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.optimization_stats['total_optimizations'] += 1
        self.optimization_stats['processing_time_overhead'].append(processing_time)
        
        if result.get('status') == 'success':
            confidence_scores = [obj['confidence'] for obj in result.get('objects', [])]
            if confidence_scores:
                avg_confidence = np.mean(confidence_scores)
                self.optimization_stats['accuracy_improvements'].append(avg_confidence)
    
    def get_optimization_stats(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.optimization_stats.copy()
        
        if stats['accuracy_improvements']:
            stats['average_confidence'] = float(np.mean(stats['accuracy_improvements']))
            stats['confidence_std'] = float(np.std(stats['accuracy_improvements']))
        
        if stats['processing_time_overhead']:
            stats['average_overhead'] = float(np.mean(stats['processing_time_overhead']))
        
        return stats

def test_accuracy_optimization():
    """æµ‹è¯•ç²¾åº¦ä¼˜åŒ–"""
    print("ğŸ”„ å¼€å§‹æ£€æµ‹ç²¾åº¦ä¼˜åŒ–æµ‹è¯•...")
    
    # æµ‹è¯•å›¾åƒè·¯å¾„
    test_images_dir = Path("test_images")
    if not test_images_dir.exists():
        print("âŒ æµ‹è¯•å›¾åƒç›®å½•ä¸å­˜åœ¨")
        return
    
    # è·å–æµ‹è¯•å›¾åƒ
    image_files = []
    for ext in ['.jpg', '.jpeg', '.png']:
        image_files.extend(list(test_images_dir.glob(f"*{ext}")))
    
    if not image_files:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•å›¾åƒ")
        return
    
    test_images = image_files[:3]  # é™åˆ¶æµ‹è¯•å›¾åƒæ•°é‡
    
    # ä¸åŒä¼˜åŒ–é…ç½®
    configs = {
        'baseline': OptimizationConfig(
            enable_preprocessing=False,
            enable_multi_scale=False,
            enable_tta=False
        ),
        'preprocessing_only': OptimizationConfig(
            enable_preprocessing=True,
            enable_multi_scale=False,
            enable_tta=False
        ),
        'multi_scale': OptimizationConfig(
            enable_preprocessing=True,
            enable_multi_scale=True,
            enable_tta=False,
            scales=[480, 640, 800]
        ),
        'tta': OptimizationConfig(
            enable_preprocessing=True,
            enable_multi_scale=False,
            enable_tta=True,
            tta_scales=[0.9, 1.0, 1.1],
            tta_flips=[False, True]
        ),
        'full_optimization': OptimizationConfig(
            enable_preprocessing=True,
            enable_multi_scale=False,  # é¿å…è¿‡åº¦è®¡ç®—
            enable_tta=True,
            enable_nms_optimization=True,
            enable_confidence_calibration=True,
            tta_scales=[0.95, 1.0, 1.05],
            tta_flips=[False, True]
        )
    }
    
    results = {}
    
    for config_name, config in configs.items():
        print(f"\nğŸ” æµ‹è¯•é…ç½®: {config_name}")
        
        optimizer = DetectionAccuracyOptimizer(config=config)
        if optimizer.model is None:
            continue
        
        config_results = []
        
        for i, image_path in enumerate(test_images, 1):
            print(f"  [{i}/{len(test_images)}] {image_path.name}")
            
            image = cv2.imread(str(image_path))
            if image is None:
                continue
            
            result = optimizer.optimize_detection(image)
            config_results.append(result)
        
        # è®¡ç®—é…ç½®ç»Ÿè®¡
        total_objects = sum(r.get('objects_count', 0) for r in config_results)
        confidences = []
        processing_times = []
        
        for r in config_results:
            if r.get('objects'):
                confidences.extend([obj['confidence'] for obj in r['objects']])
            processing_times.append(r.get('optimization_time', 0))
        
        results[config_name] = {
            'total_objects': total_objects,
            'average_confidence': float(np.mean(confidences)) if confidences else 0,
            'confidence_std': float(np.std(confidences)) if confidences else 0,
            'average_processing_time': float(np.mean(processing_times)),
            'images_processed': len(config_results)
        }
    
    # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
    generate_optimization_report(results)
    
    return results

def generate_optimization_report(results: Dict[str, Dict[str, Any]]):
    """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
    
    html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ£€æµ‹ç²¾åº¦ä¼˜åŒ–æŠ¥å‘Š</title>
    <style>
        body {{
            font-family: 'Microsoft YaHei', Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background: linear-gradient(135deg, #74b9ff 0%, #0984e3 100%);
            min-height: 100vh;
        }}
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #2d3436 0%, #636e72 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
            font-weight: 300;
        }}
        .optimization-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 25px;
            padding: 40px;
        }}
        .config-card {{
            background: #f8f9fa;
            border-radius: 12px;
            padding: 25px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
            border-left: 5px solid #74b9ff;
            transition: transform 0.3s ease;
        }}
        .config-card:hover {{
            transform: translateY(-5px);
        }}
        .config-name {{
            font-size: 1.4em;
            font-weight: bold;
            color: #2d3436;
            margin-bottom: 20px;
            text-align: center;
            text-transform: uppercase;
        }}
        .metric {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 12px 0;
            border-bottom: 1px solid #e0e0e0;
        }}
        .metric:last-child {{
            border-bottom: none;
        }}
        .metric-label {{
            color: #636e72;
            font-weight: 500;
        }}
        .metric-value {{
            font-weight: bold;
            color: #2d3436;
            font-size: 1.1em;
        }}
        .best-config {{
            border-left-color: #00b894;
            background: linear-gradient(135deg, #d1f2eb 0%, #a3e4d7 100%);
        }}
        .performance-summary {{
            background: linear-gradient(135deg, #fdcb6e 0%, #e17055 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .summary-title {{
            font-size: 1.8em;
            margin-bottom: 20px;
        }}
        .improvement-stats {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }}
        .improvement-item {{
            background: rgba(255,255,255,0.2);
            padding: 15px;
            border-radius: 8px;
            text-align: center;
        }}
        .improvement-value {{
            font-size: 1.5em;
            font-weight: bold;
            margin-bottom: 5px;
        }}
        .footer {{
            background: #f8f9fa;
            padding: 25px;
            text-align: center;
            color: #636e72;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ¯ æ£€æµ‹ç²¾åº¦ä¼˜åŒ–æŠ¥å‘Š</h1>
            <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            <p>å¤šç§ä¼˜åŒ–ç­–ç•¥æ€§èƒ½å¯¹æ¯”åˆ†æ</p>
        </div>
        
        <div class="optimization-grid">
"""
    
    # æ‰¾å‡ºæœ€ä½³é…ç½®
    best_confidence = max(results.values(), key=lambda x: x['average_confidence'])['average_confidence']
    best_objects = max(results.values(), key=lambda x: x['total_objects'])['total_objects']
    
    for config_name, stats in results.items():
        is_best = (stats['average_confidence'] == best_confidence or 
                  stats['total_objects'] == best_objects)
        
        card_class = "config-card best-config" if is_best else "config-card"
        
        html_content += f"""
            <div class="{card_class}">
                <div class="config-name">{config_name.replace('_', ' ')}</div>
                <div class="metric">
                    <span class="metric-label">æ£€æµ‹ç‰©ä½“æ€»æ•°</span>
                    <span class="metric-value">{stats['total_objects']}</span>
                </div>
                <div class="metric">
                    <span class="metric-label">å¹³å‡ç½®ä¿¡åº¦</span>
                    <span class="metric-value">{stats['average_confidence']:.1%}</span>
                </div>
                <div class="metric">
                    <span class="metric-label">ç½®ä¿¡åº¦æ ‡å‡†å·®</span>
                    <span class="metric-value">{stats['confidence_std']:.3f}</span>
                </div>
                <div class="metric">
                    <span class="metric-label">å¹³å‡å¤„ç†æ—¶é—´</span>
                    <span class="metric-value">{stats['average_processing_time']:.3f}s</span>
                </div>
                <div class="metric">
                    <span class="metric-label">å¤„ç†å›¾åƒæ•°</span>
                    <span class="metric-value">{stats['images_processed']}</span>
                </div>
            </div>
"""
    
    # è®¡ç®—æ”¹è¿›ç»Ÿè®¡
    baseline_confidence = results.get('baseline', {}).get('average_confidence', 0)
    best_improvement = ((best_confidence - baseline_confidence) / baseline_confidence * 100) if baseline_confidence > 0 else 0
    
    html_content += f"""
        </div>
        
        <div class="performance-summary">
            <div class="summary-title">ğŸ“Š ä¼˜åŒ–æ•ˆæœæ€»ç»“</div>
            <div class="improvement-stats">
                <div class="improvement-item">
                    <div class="improvement-value">{best_improvement:.1f}%</div>
                    <div>æœ€å¤§ç½®ä¿¡åº¦æå‡</div>
                </div>
                <div class="improvement-item">
                    <div class="improvement-value">{best_confidence:.1%}</div>
                    <div>æœ€ä½³å¹³å‡ç½®ä¿¡åº¦</div>
                </div>
                <div class="improvement-item">
                    <div class="improvement-value">{best_objects}</div>
                    <div>æœ€å¤šæ£€æµ‹ç‰©ä½“</div>
                </div>
                <div class="improvement-item">
                    <div class="improvement-value">{len(results)}</div>
                    <div>æµ‹è¯•é…ç½®æ•°é‡</div>
                </div>
            </div>
        </div>
        
        <div class="footer">
            <p><strong>ğŸ”§ ä¼˜åŒ–å»ºè®®:</strong> æ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç­–ç•¥</p>
            <p><strong>âš¡ æ€§èƒ½æƒè¡¡:</strong> ç²¾åº¦æå‡é€šå¸¸ä¼´éšå¤„ç†æ—¶é—´å¢åŠ </p>
            <p><strong>ğŸ“ˆ æœ€ä½³å®è·µ:</strong> åœ¨ç²¾åº¦å’Œé€Ÿåº¦ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹</p>
        </div>
    </div>
</body>
</html>
"""
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = "detection_accuracy_optimization_report.html"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print(f"âœ… æ£€æµ‹ç²¾åº¦ä¼˜åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

if __name__ == "__main__":
    print("ğŸš€ æ£€æµ‹ç²¾åº¦ä¼˜åŒ–æµ‹è¯•å¼€å§‹...")
    
    results = test_accuracy_optimization()
    
    if results:
        print("\nğŸ“Š ä¼˜åŒ–æµ‹è¯•å®Œæˆ!")
        for config, stats in results.items():
            print(f"{config}: {stats['average_confidence']:.1%} ç½®ä¿¡åº¦, {stats['total_objects']} ç‰©ä½“")
    else:
        print("âŒ ä¼˜åŒ–æµ‹è¯•å¤±è´¥")