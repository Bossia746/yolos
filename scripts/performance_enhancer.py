#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½æå‡æ¨¡å—
å®ç°å¤šç§æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
"""

import cv2
import numpy as np
import time
import threading
import queue
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import json
import psutil
import gc

try:
    from ultralytics import YOLO
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

class PerformanceEnhancer:
    """æ€§èƒ½å¢å¼ºå™¨"""
    
    def __init__(self, model_path: str = "yolov8n.pt"):
        self.model_path = model_path
        self.model = None
        self.device = 'cpu'
        
        # æ€§èƒ½ä¼˜åŒ–é…ç½®
        self.optimization_config = {
            'enable_gpu': True,
            'enable_half_precision': True,
            'enable_batch_processing': True,
            'enable_model_optimization': True,
            'enable_memory_optimization': True,
            'batch_size': 4,
            'num_threads': 4
        }
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'total_inferences': 0,
            'total_time': 0.0,
            'average_fps': 0.0,
            'memory_usage': [],
            'gpu_usage': [],
            'batch_processing_gains': []
        }
        
        # ç¼“å­˜ç³»ç»Ÿ
        self.result_cache = {}
        self.cache_size_limit = 100
        
        self._initialize_performance_optimization()
    
    def _initialize_performance_optimization(self):
        """åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–"""
        print("ğŸš€ åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–...")
        
        if not TORCH_AVAILABLE:
            print("âŒ PyTorchä¸å¯ç”¨ï¼Œæ€§èƒ½ä¼˜åŒ–å—é™")
            return
        
        # 1. è®¾å¤‡é€‰æ‹©
        self._setup_device()
        
        # 2. åŠ è½½å’Œä¼˜åŒ–æ¨¡å‹
        self._load_and_optimize_model()
        
        # 3. å†…å­˜ä¼˜åŒ–
        self._setup_memory_optimization()
        
        # 4. çº¿ç¨‹æ± è®¾ç½®
        self._setup_threading()
    
    def _setup_device(self):
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if self.optimization_config['enable_gpu'] and torch.cuda.is_available():
            self.device = 'cuda'
            # GPUä¼˜åŒ–è®¾ç½®
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            print(f"âœ… GPUåŠ é€Ÿå¯ç”¨: {torch.cuda.get_device_name()}")
        else:
            self.device = 'cpu'
            # CPUä¼˜åŒ–è®¾ç½®
            torch.set_num_threads(self.optimization_config['num_threads'])
            print(f"âœ… CPUä¼˜åŒ–å¯ç”¨: {self.optimization_config['num_threads']} çº¿ç¨‹")
    
    def _load_and_optimize_model(self):
        """åŠ è½½å¹¶ä¼˜åŒ–æ¨¡å‹"""
        try:
            self.model = YOLO(self.model_path)
            
            # æ¨¡å‹ä¼˜åŒ–
            if self.optimization_config['enable_model_optimization']:
                # é¢„çƒ­æ¨¡å‹
                dummy_input = torch.randn(1, 3, 640, 640).to(self.device)
                if self.device == 'cuda':
                    dummy_input = dummy_input.half() if self.optimization_config['enable_half_precision'] else dummy_input
                
                # æ‰§è¡Œé¢„çƒ­æ¨ç†
                with torch.no_grad():
                    _ = self.model(dummy_input, verbose=False)
                
                print("ğŸ”¥ æ¨¡å‹é¢„çƒ­å®Œæˆ")
            
            # åŠç²¾åº¦ä¼˜åŒ–
            if self.optimization_config['enable_half_precision'] and self.device == 'cuda':
                self.model.model.half()
                print("âš¡ åŠç²¾åº¦ä¼˜åŒ–å¯ç”¨")
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_path}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    def _setup_memory_optimization(self):
        """è®¾ç½®å†…å­˜ä¼˜åŒ–"""
        if self.optimization_config['enable_memory_optimization']:
            # åƒåœ¾å›æ”¶ä¼˜åŒ–
            gc.set_threshold(700, 10, 10)
            
            # CUDAå†…å­˜ä¼˜åŒ–
            if self.device == 'cuda':
                torch.cuda.empty_cache()
                # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
                torch.cuda.set_per_process_memory_fraction(0.8)
            
            print("ğŸ§¹ å†…å­˜ä¼˜åŒ–å¯ç”¨")
    
    def _setup_threading(self):
        """è®¾ç½®çº¿ç¨‹æ± """
        self.thread_pool = []
        self.task_queue = queue.Queue()
        self.result_queue = queue.Queue()
        
        # åˆ›å»ºå·¥ä½œçº¿ç¨‹
        for i in range(self.optimization_config['num_threads']):
            thread = threading.Thread(target=self._worker_thread, daemon=True)
            thread.start()
            self.thread_pool.append(thread)
        
        print(f"ğŸ”§ çº¿ç¨‹æ± å¯ç”¨: {len(self.thread_pool)} ä¸ªå·¥ä½œçº¿ç¨‹")
    
    def _worker_thread(self):
        """å·¥ä½œçº¿ç¨‹å‡½æ•°"""
        while True:
            try:
                task = self.task_queue.get(timeout=1)
                if task is None:
                    break
                
                task_id, image, params = task
                result = self._single_inference(image, **params)
                self.result_queue.put((task_id, result))
                self.task_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ å·¥ä½œçº¿ç¨‹é”™è¯¯: {e}")
    
    def enhanced_detect(self, image: np.ndarray, **kwargs) -> Dict[str, Any]:
        """å¢å¼ºæ£€æµ‹ - å•å¼ å›¾åƒ"""
        if self.model is None:
            return self._create_error_result("æ¨¡å‹æœªåŠ è½½")
        
        start_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜
        image_hash = self._calculate_image_hash(image)
        if image_hash in self.result_cache:
            cached_result = self.result_cache[image_hash].copy()
            cached_result['from_cache'] = True
            cached_result['processing_time'] = time.time() - start_time
            return cached_result
        
        # æ‰§è¡Œæ£€æµ‹
        result = self._single_inference(image, **kwargs)
        
        # ç¼“å­˜ç»“æœ
        self._cache_result(image_hash, result)
        
        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        processing_time = time.time() - start_time
        self._update_performance_stats(processing_time)
        
        result['processing_time'] = processing_time
        result['device'] = self.device
        result['optimizations_applied'] = self._get_applied_optimizations()
        
        return result
    
    def enhanced_detect_batch(self, images: List[np.ndarray], **kwargs) -> List[Dict[str, Any]]:
        """å¢å¼ºæ‰¹é‡æ£€æµ‹"""
        if self.model is None:
            return [self._create_error_result("æ¨¡å‹æœªåŠ è½½") for _ in images]
        
        if not self.optimization_config['enable_batch_processing']:
            # é€ä¸ªå¤„ç†
            return [self.enhanced_detect(img, **kwargs) for img in images]
        
        start_time = time.time()
        
        # æ‰¹é‡å¤„ç†
        batch_size = self.optimization_config['batch_size']
        results = []
        
        for i in range(0, len(images), batch_size):
            batch = images[i:i + batch_size]
            batch_results = self._batch_inference(batch, **kwargs)
            results.extend(batch_results)
        
        # è®¡ç®—æ‰¹é‡å¤„ç†æ€§èƒ½æå‡
        total_time = time.time() - start_time
        estimated_sequential_time = len(images) * 0.1  # ä¼°ç®—é¡ºåºå¤„ç†æ—¶é—´
        performance_gain = (estimated_sequential_time - total_time) / estimated_sequential_time
        
        self.performance_stats['batch_processing_gains'].append(performance_gain)
        
        return results
    
    def enhanced_detect_async(self, images: List[np.ndarray], **kwargs) -> List[Dict[str, Any]]:
        """å¼‚æ­¥å¢å¼ºæ£€æµ‹"""
        if not images:
            return []
        
        # æäº¤ä»»åŠ¡åˆ°é˜Ÿåˆ—
        task_ids = []
        for i, image in enumerate(images):
            task_id = f"async_{int(time.time() * 1000)}_{i}"
            self.task_queue.put((task_id, image, kwargs))
            task_ids.append(task_id)
        
        # æ”¶é›†ç»“æœ
        results = {}
        collected = 0
        
        while collected < len(task_ids):
            try:
                task_id, result = self.result_queue.get(timeout=10)
                results[task_id] = result
                collected += 1
            except queue.Empty:
                print("âš ï¸ å¼‚æ­¥æ£€æµ‹è¶…æ—¶")
                break
        
        # æŒ‰é¡ºåºè¿”å›ç»“æœ
        ordered_results = []
        for task_id in task_ids:
            if task_id in results:
                ordered_results.append(results[task_id])
            else:
                ordered_results.append(self._create_error_result("å¼‚æ­¥å¤„ç†è¶…æ—¶"))
        
        return ordered_results
    
    def _single_inference(self, image: np.ndarray, **kwargs) -> Dict[str, Any]:
        """å•æ¬¡æ¨ç†"""
        try:
            # é¢„å¤„ç†
            processed_image = self._preprocess_for_performance(image)
            
            # æ¨ç†
            with torch.no_grad():
                results = self.model(
                    processed_image,
                    device=self.device,
                    verbose=False,
                    **kwargs
                )
            
            # è§£æç»“æœ
            return self._parse_results(results[0])
            
        except Exception as e:
            return self._create_error_result(f"æ¨ç†å¤±è´¥: {str(e)}")
    
    def _batch_inference(self, images: List[np.ndarray], **kwargs) -> List[Dict[str, Any]]:
        """æ‰¹é‡æ¨ç†"""
        try:
            # é¢„å¤„ç†æ‰¹é‡å›¾åƒ
            processed_images = [self._preprocess_for_performance(img) for img in images]
            
            # æ‰¹é‡æ¨ç†
            with torch.no_grad():
                results = self.model(
                    processed_images,
                    device=self.device,
                    verbose=False,
                    **kwargs
                )
            
            # è§£ææ‰¹é‡ç»“æœ
            return [self._parse_results(result) for result in results]
            
        except Exception as e:
            error_result = self._create_error_result(f"æ‰¹é‡æ¨ç†å¤±è´¥: {str(e)}")
            return [error_result for _ in images]
    
    def _preprocess_for_performance(self, image: np.ndarray) -> np.ndarray:
        """æ€§èƒ½ä¼˜åŒ–çš„é¢„å¤„ç†"""
        # å¿«é€Ÿresizeç­–ç•¥
        h, w = image.shape[:2]
        
        # å¦‚æœå›¾åƒå·²ç»æ˜¯åˆé€‚å¤§å°ï¼Œç›´æ¥è¿”å›
        if h == 640 and w == 640:
            return image
        
        # ä½¿ç”¨æ›´å¿«çš„æ’å€¼æ–¹æ³•
        if max(h, w) > 640:
            scale = 640 / max(h, w)
            new_h, new_w = int(h * scale), int(w * scale)
            
            # ä½¿ç”¨INTER_LINEARè€Œä¸æ˜¯INTER_CUBICä»¥æé«˜é€Ÿåº¦
            resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
            
            # å¡«å……åˆ°640x640
            if new_h != 640 or new_w != 640:
                padded = np.zeros((640, 640, 3), dtype=np.uint8)
                padded[:new_h, :new_w] = resized
                return padded
            
            return resized
        
        return image
    
    def _parse_results(self, result) -> Dict[str, Any]:
        """è§£æç»“æœ"""
        detection_info = {
            'objects': [],
            'objects_count': 0,
            'status': 'success'
        }
        
        if result.boxes is not None and len(result.boxes) > 0:
            for box in result.boxes:
                confidence = float(box.conf[0])
                class_id = int(box.cls[0])
                class_name = self.model.names[class_id]
                
                x1, y1, x2, y2 = box.xyxy[0].tolist()
                
                obj_info = {
                    'class': class_name,
                    'confidence': confidence,
                    'bbox': [int(x1), int(y1), int(x2), int(y2)],
                    'class_id': class_id
                }
                
                detection_info['objects'].append(obj_info)
            
            detection_info['objects_count'] = len(detection_info['objects'])
        
        return detection_info
    
    def _calculate_image_hash(self, image: np.ndarray) -> str:
        """è®¡ç®—å›¾åƒå“ˆå¸Œç”¨äºç¼“å­˜"""
        # ç®€å•çš„å›¾åƒå“ˆå¸Œ
        small = cv2.resize(image, (8, 8), interpolation=cv2.INTER_AREA)
        gray = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)
        return str(hash(gray.tobytes()))
    
    def _cache_result(self, image_hash: str, result: Dict[str, Any]):
        """ç¼“å­˜ç»“æœ"""
        if len(self.result_cache) >= self.cache_size_limit:
            # åˆ é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
            oldest_key = next(iter(self.result_cache))
            del self.result_cache[oldest_key]
        
        self.result_cache[image_hash] = result.copy()
    
    def _update_performance_stats(self, processing_time: float):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        self.performance_stats['total_inferences'] += 1
        self.performance_stats['total_time'] += processing_time
        
        # è®¡ç®—å¹³å‡FPS
        if self.performance_stats['total_time'] > 0:
            self.performance_stats['average_fps'] = (
                self.performance_stats['total_inferences'] / 
                self.performance_stats['total_time']
            )
        
        # è®°å½•å†…å­˜ä½¿ç”¨
        memory_percent = psutil.virtual_memory().percent
        self.performance_stats['memory_usage'].append(memory_percent)
        
        # è®°å½•GPUä½¿ç”¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.device == 'cuda' and torch.cuda.is_available():
            gpu_memory = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated()
            self.performance_stats['gpu_usage'].append(gpu_memory * 100)
    
    def _get_applied_optimizations(self) -> List[str]:
        """è·å–å·²åº”ç”¨çš„ä¼˜åŒ–"""
        optimizations = []
        
        if self.device == 'cuda':
            optimizations.append('GPUåŠ é€Ÿ')
        
        if self.optimization_config['enable_half_precision'] and self.device == 'cuda':
            optimizations.append('åŠç²¾åº¦è®¡ç®—')
        
        if self.optimization_config['enable_batch_processing']:
            optimizations.append('æ‰¹é‡å¤„ç†')
        
        if self.optimization_config['enable_memory_optimization']:
            optimizations.append('å†…å­˜ä¼˜åŒ–')
        
        if len(self.result_cache) > 0:
            optimizations.append('ç»“æœç¼“å­˜')
        
        return optimizations
    
    def _create_error_result(self, error_msg: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return {
            'error': error_msg,
            'objects': [],
            'objects_count': 0,
            'status': 'error'
        }
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        stats = self.performance_stats.copy()
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if stats['memory_usage']:
            stats['average_memory_usage'] = np.mean(stats['memory_usage'])
            stats['peak_memory_usage'] = np.max(stats['memory_usage'])
        
        if stats['gpu_usage']:
            stats['average_gpu_usage'] = np.mean(stats['gpu_usage'])
            stats['peak_gpu_usage'] = np.max(stats['gpu_usage'])
        
        if stats['batch_processing_gains']:
            stats['average_batch_gain'] = np.mean(stats['batch_processing_gains'])
        
        # ç³»ç»Ÿä¿¡æ¯
        stats['system_info'] = {
            'cpu_count': psutil.cpu_count(),
            'memory_total': psutil.virtual_memory().total / (1024**3),  # GB
            'cuda_available': torch.cuda.is_available() if TORCH_AVAILABLE else False,
            'device_used': self.device
        }
        
        return stats
    
    def benchmark_performance(self, test_images: List[np.ndarray], iterations: int = 3) -> Dict[str, Any]:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print(f"ğŸƒ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯• ({iterations} è½®)...")
        
        benchmark_results = {
            'single_image_fps': [],
            'batch_processing_fps': [],
            'async_processing_fps': [],
            'memory_efficiency': [],
            'optimization_overhead': []
        }
        
        for iteration in range(iterations):
            print(f"  ç¬¬ {iteration + 1}/{iterations} è½®æµ‹è¯•")
            
            # 1. å•å›¾åƒå¤„ç†æµ‹è¯•
            start_time = time.time()
            for img in test_images:
                _ = self.enhanced_detect(img)
            single_time = time.time() - start_time
            single_fps = len(test_images) / single_time
            benchmark_results['single_image_fps'].append(single_fps)
            
            # 2. æ‰¹é‡å¤„ç†æµ‹è¯•
            start_time = time.time()
            _ = self.enhanced_detect_batch(test_images)
            batch_time = time.time() - start_time
            batch_fps = len(test_images) / batch_time
            benchmark_results['batch_processing_fps'].append(batch_fps)
            
            # 3. å¼‚æ­¥å¤„ç†æµ‹è¯•
            start_time = time.time()
            _ = self.enhanced_detect_async(test_images)
            async_time = time.time() - start_time
            async_fps = len(test_images) / async_time
            benchmark_results['async_processing_fps'].append(async_fps)
            
            # 4. å†…å­˜æ•ˆç‡
            memory_before = psutil.virtual_memory().percent
            _ = self.enhanced_detect_batch(test_images * 2)  # å¤„ç†æ›´å¤šå›¾åƒ
            memory_after = psutil.virtual_memory().percent
            memory_increase = memory_after - memory_before
            benchmark_results['memory_efficiency'].append(memory_increase)
            
            # æ¸…ç†å†…å­˜
            if self.device == 'cuda':
                torch.cuda.empty_cache()
            gc.collect()
        
        # è®¡ç®—å¹³å‡å€¼
        final_results = {}
        for key, values in benchmark_results.items():
            final_results[f'average_{key}'] = np.mean(values)
            final_results[f'std_{key}'] = np.std(values)
            final_results[f'best_{key}'] = np.max(values) if 'fps' in key else np.min(values)
        
        # æ€§èƒ½æå‡è®¡ç®—
        if final_results['average_batch_processing_fps'] > 0:
            batch_improvement = (
                final_results['average_batch_processing_fps'] / 
                final_results['average_single_image_fps'] - 1
            ) * 100
            final_results['batch_processing_improvement'] = batch_improvement
        
        return final_results

def test_performance_enhancement():
    """æµ‹è¯•æ€§èƒ½å¢å¼º"""
    print("ğŸš€ æ€§èƒ½å¢å¼ºæµ‹è¯•å¼€å§‹...")
    
    # åŠ è½½æµ‹è¯•å›¾åƒ
    test_images_dir = Path("test_images")
    if not test_images_dir.exists():
        print("âŒ æµ‹è¯•å›¾åƒç›®å½•ä¸å­˜åœ¨")
        return
    
    image_files = list(test_images_dir.glob("*.jpg"))[:5]
    if not image_files:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•å›¾åƒ")
        return
    
    test_images = []
    for img_path in image_files:
        img = cv2.imread(str(img_path))
        if img is not None:
            test_images.append(img)
    
    if not test_images:
        print("âŒ æ— æ³•åŠ è½½æµ‹è¯•å›¾åƒ")
        return
    
    # åˆ›å»ºæ€§èƒ½å¢å¼ºå™¨
    enhancer = PerformanceEnhancer()
    
    if enhancer.model is None:
        print("âŒ æ€§èƒ½å¢å¼ºå™¨åˆå§‹åŒ–å¤±è´¥")
        return
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark_results = enhancer.benchmark_performance(test_images)
    
    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    performance_report = enhancer.get_performance_report()
    
    # ç”ŸæˆHTMLæŠ¥å‘Š
    generate_performance_report(benchmark_results, performance_report)
    
    return benchmark_results, performance_report

def generate_performance_report(benchmark_results: Dict[str, Any], 
                              performance_report: Dict[str, Any]):
    """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
    
    html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š</title>
    <style>
        body {{
            font-family: 'Microsoft YaHei', Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }}
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #2c3e50 0%, #34495e 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
            font-weight: 300;
        }}
        .metrics-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 25px;
            padding: 40px;
        }}
        .metric-card {{
            background: #f8f9fa;
            border-radius: 12px;
            padding: 25px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
            border-left: 5px solid #3498db;
        }}
        .metric-title {{
            font-size: 1.3em;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 15px;
        }}
        .metric-value {{
            font-size: 2em;
            font-weight: bold;
            color: #e74c3c;
            margin-bottom: 10px;
        }}
        .metric-description {{
            color: #666;
            font-size: 0.9em;
        }}
        .system-info {{
            background: #e8f5e8;
            padding: 30px;
            margin: 20px 40px;
            border-radius: 10px;
        }}
        .system-info h3 {{
            color: #27ae60;
            margin-bottom: 20px;
        }}
        .info-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
        }}
        .info-item {{
            background: white;
            padding: 15px;
            border-radius: 8px;
            text-align: center;
        }}
        .improvement-highlight {{
            border-left-color: #27ae60;
            background: linear-gradient(135deg, #d4edda 0%, #c3e6cb 100%);
        }}
        .footer {{
            background: #f8f9fa;
            padding: 25px;
            text-align: center;
            color: #666;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>âš¡ æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š</h1>
            <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            <p>YOLOæ£€æµ‹æ€§èƒ½ä¼˜åŒ–æ•ˆæœåˆ†æ</p>
        </div>
        
        <div class="metrics-grid">
            <div class="metric-card improvement-highlight">
                <div class="metric-title">ğŸš€ å•å›¾åƒå¤„ç†</div>
                <div class="metric-value">{benchmark_results.get('average_single_image_fps', 0):.1f} FPS</div>
                <div class="metric-description">å¹³å‡å•å›¾åƒå¤„ç†é€Ÿåº¦</div>
            </div>
            
            <div class="metric-card improvement-highlight">
                <div class="metric-title">ğŸ“¦ æ‰¹é‡å¤„ç†</div>
                <div class="metric-value">{benchmark_results.get('average_batch_processing_fps', 0):.1f} FPS</div>
                <div class="metric-description">æ‰¹é‡å¤„ç†å¹³å‡é€Ÿåº¦</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">ğŸ”„ å¼‚æ­¥å¤„ç†</div>
                <div class="metric-value">{benchmark_results.get('average_async_processing_fps', 0):.1f} FPS</div>
                <div class="metric-description">å¼‚æ­¥å¤„ç†å¹³å‡é€Ÿåº¦</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">ğŸ“ˆ æ€§èƒ½æå‡</div>
                <div class="metric-value">{benchmark_results.get('batch_processing_improvement', 0):.1f}%</div>
                <div class="metric-description">æ‰¹é‡å¤„ç†ç›¸å¯¹æå‡</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">ğŸ§  å†…å­˜ä½¿ç”¨</div>
                <div class="metric-value">{performance_report.get('average_memory_usage', 0):.1f}%</div>
                <div class="metric-description">å¹³å‡å†…å­˜å ç”¨ç‡</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-title">âš¡ æ€»æ¨ç†æ¬¡æ•°</div>
                <div class="metric-value">{performance_report.get('total_inferences', 0)}</div>
                <div class="metric-description">ç´¯è®¡æ¨ç†æ¬¡æ•°</div>
            </div>
        </div>
        
        <div class="system-info">
            <h3>ğŸ–¥ï¸ ç³»ç»Ÿä¿¡æ¯</h3>
            <div class="info-grid">
                <div class="info-item">
                    <strong>CPUæ ¸å¿ƒæ•°</strong><br>
                    {performance_report.get('system_info', {}).get('cpu_count', 'N/A')}
                </div>
                <div class="info-item">
                    <strong>æ€»å†…å­˜</strong><br>
                    {performance_report.get('system_info', {}).get('memory_total', 0):.1f} GB
                </div>
                <div class="info-item">
                    <strong>CUDAæ”¯æŒ</strong><br>
                    {'âœ… æ˜¯' if performance_report.get('system_info', {}).get('cuda_available') else 'âŒ å¦'}
                </div>
                <div class="info-item">
                    <strong>ä½¿ç”¨è®¾å¤‡</strong><br>
                    {performance_report.get('system_info', {}).get('device_used', 'CPU').upper()}
                </div>
            </div>
        </div>
        
        <div class="footer">
            <p><strong>ğŸ¯ ä¼˜åŒ–æ•ˆæœ:</strong> é€šè¿‡å¤šç§ä¼˜åŒ–ç­–ç•¥æ˜¾è‘—æå‡æ£€æµ‹æ€§èƒ½</p>
            <p><strong>ğŸ’¡ å»ºè®®:</strong> æ ¹æ®ç¡¬ä»¶é…ç½®é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç­–ç•¥</p>
        </div>
    </div>
</body>
</html>
"""
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = "performance_optimization_report.html"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print(f"âœ… æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

if __name__ == "__main__":
    print("ğŸš€ æ€§èƒ½å¢å¼ºæµ‹è¯•å¼€å§‹...")
    
    results = test_performance_enhancement()
    
    if results:
        benchmark_results, performance_report = results
        print("\nğŸ“Š æ€§èƒ½æµ‹è¯•å®Œæˆ!")
        print(f"å•å›¾åƒ: {benchmark_results.get('average_single_image_fps', 0):.1f} FPS")
        print(f"æ‰¹é‡å¤„ç†: {benchmark_results.get('average_batch_processing_fps', 0):.1f} FPS")
        print(f"æ€§èƒ½æå‡: {benchmark_results.get('batch_processing_improvement', 0):.1f}%")
    else:
        print("âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥")