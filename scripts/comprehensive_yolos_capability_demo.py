#!/usr/bin/env python3
"""
YOLOSé¡¹ç›®ç»¼åˆèƒ½åŠ›æ¼”ç¤º
éªŒè¯å®æ—¶æ‘„åƒå¤´æ£€æµ‹ã€å¤šæ ¼å¼æ–‡ä»¶å¤„ç†å’Œé¢„è®­ç»ƒèƒ½åŠ›
"""

import os
import sys
import cv2
import time
import json
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.detection.camera_detector import CameraDetector
from src.detection.video_detector import VideoDetector
from src.detection.realtime_detector import RealtimeDetector
from src.training.dataset_manager import DatasetManager
from src.training.enhanced_human_trainer import EnhancedHumanTrainer
from src.models.yolo_factory import YOLOFactory


class YOLOSCapabilityDemo:
    """YOLOSèƒ½åŠ›æ¼”ç¤ºå™¨"""
    
    def __init__(self):
        self.supported_image_formats = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']
        self.supported_video_formats = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm']
        self.results = {}
        
    def test_realtime_camera_detection(self, duration: int = 30) -> Dict[str, Any]:
        """æµ‹è¯•å®æ—¶æ‘„åƒå¤´å¤šç›®æ ‡æ£€æµ‹"""
        print("=" * 60)
        print("ğŸ¥ æµ‹è¯•å®æ—¶æ‘„åƒå¤´å¤šç›®æ ‡æ£€æµ‹èƒ½åŠ›")
        print("=" * 60)
        
        try:
            # åˆ›å»ºæ‘„åƒå¤´æ£€æµ‹å™¨
            camera_detector = CameraDetector(
                model_type='yolov11',
                device='auto',
                camera_type='usb'
            )
            
            # è®¾ç½®æ£€æµ‹å‚æ•°
            camera_detector.set_camera_params(resolution=(640, 480), framerate=30)
            camera_detector.set_detection_params(interval=1)  # æ¯å¸§éƒ½æ£€æµ‹
            
            # æ£€æµ‹ç»Ÿè®¡
            detection_stats = {
                'total_detections': 0,
                'unique_classes': set(),
                'multi_person_frames': 0,
                'complex_scenes': 0
            }
            
            def detection_callback(frame, results):
                """æ£€æµ‹å›è°ƒå‡½æ•°"""
                detection_stats['total_detections'] += len(results)
                
                # ç»Ÿè®¡æ£€æµ‹åˆ°çš„ç±»åˆ«
                for result in results:
                    detection_stats['unique_classes'].add(result.get('class_name', 'unknown'))
                
                # æ£€æµ‹å¤šäººåœºæ™¯
                person_count = sum(1 for r in results if r.get('class_name') == 'person')
                if person_count >= 2:
                    detection_stats['multi_person_frames'] += 1
                
                # æ£€æµ‹å¤æ‚åœºæ™¯ï¼ˆå¤šç§ç‰©ä½“ï¼‰
                if len(set(r.get('class_name') for r in results)) >= 3:
                    detection_stats['complex_scenes'] += 1
            
            camera_detector.set_callbacks(detection_callback=detection_callback)
            
            print(f"å¼€å§‹{duration}ç§’å®æ—¶æ£€æµ‹æµ‹è¯•...")
            print("æ£€æµ‹åŠŸèƒ½:")
            print("- å¤šäººåŒæ¡†æ£€æµ‹")
            print("- å¤æ‚åœºæ™¯è¯†åˆ«")
            print("- å®æ—¶æ€§èƒ½ç›‘æ§")
            print("æŒ‰ 'q' æå‰ç»“æŸæµ‹è¯•")
            
            # å¯åŠ¨æ£€æµ‹ï¼ˆé™æ—¶ï¼‰
            start_time = time.time()
            
            # ä½¿ç”¨çº¿ç¨‹æ§åˆ¶æ—¶é•¿
            import threading
            
            def stop_after_duration():
                time.sleep(duration)
                camera_detector.stop_detection()
            
            timer_thread = threading.Thread(target=stop_after_duration)
            timer_thread.daemon = True
            timer_thread.start()
            
            # å¼€å§‹æ£€æµ‹
            camera_detector.start_detection(display=True)
            
            # è·å–æœ€ç»ˆç»Ÿè®¡
            final_stats = camera_detector.get_stats()
            final_stats.update({
                'total_detections': detection_stats['total_detections'],
                'unique_classes': list(detection_stats['unique_classes']),
                'multi_person_frames': detection_stats['multi_person_frames'],
                'complex_scenes': detection_stats['complex_scenes'],
                'test_duration': duration
            })
            
            print("\nâœ… å®æ—¶æ‘„åƒå¤´æ£€æµ‹æµ‹è¯•å®Œæˆ")
            print(f"æ€»æ£€æµ‹æ•°: {final_stats['total_detections']}")
            print(f"æ£€æµ‹ç±»åˆ«: {len(final_stats['unique_classes'])}ç§")
            print(f"å¤šäººå¸§æ•°: {final_stats['multi_person_frames']}")
            print(f"å¤æ‚åœºæ™¯: {final_stats['complex_scenes']}")
            print(f"å¹³å‡FPS: {final_stats['fps']:.1f}")
            
            return final_stats
            
        except Exception as e:
            print(f"âŒ æ‘„åƒå¤´æ£€æµ‹æµ‹è¯•å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def test_multi_format_image_processing(self, test_images_dir: str = "test_images") -> Dict[str, Any]:
        """æµ‹è¯•å¤šæ ¼å¼å›¾ç‰‡å¤„ç†èƒ½åŠ›"""
        print("=" * 60)
        print("ğŸ–¼ï¸ æµ‹è¯•å¤šæ ¼å¼å›¾ç‰‡å¤„ç†èƒ½åŠ›")
        print("=" * 60)
        
        # åˆ›å»ºæµ‹è¯•å›¾ç‰‡ç›®å½•å’Œæ ·æœ¬
        test_dir = Path(test_images_dir)
        test_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆæµ‹è¯•å›¾ç‰‡ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        self._generate_test_images(test_dir)
        
        try:
            # åˆ›å»ºYOLOæ¨¡å‹
            model = YOLOFactory.create_model('yolov11', device='auto')
            
            results = {
                'processed_formats': [],
                'total_images': 0,
                'total_detections': 0,
                'format_stats': {},
                'processing_times': []
            }
            
            print(f"æ‰«æç›®å½•: {test_dir}")
            print(f"æ”¯æŒæ ¼å¼: {', '.join(self.supported_image_formats)}")
            
            # å¤„ç†æ‰€æœ‰æ”¯æŒæ ¼å¼çš„å›¾ç‰‡
            for image_path in test_dir.rglob("*"):
                if image_path.suffix.lower() in self.supported_image_formats:
                    print(f"å¤„ç†: {image_path.name}")
                    
                    start_time = time.time()
                    
                    # è¯»å–å¹¶æ£€æµ‹
                    image = cv2.imread(str(image_path))
                    if image is not None:
                        detections = model.predict(image)
                        
                        # ç»Ÿè®¡
                        format_ext = image_path.suffix.lower()
                        if format_ext not in results['format_stats']:
                            results['format_stats'][format_ext] = {
                                'count': 0,
                                'detections': 0,
                                'avg_time': 0
                            }
                        
                        processing_time = time.time() - start_time
                        
                        results['format_stats'][format_ext]['count'] += 1
                        results['format_stats'][format_ext]['detections'] += len(detections)
                        results['format_stats'][format_ext]['avg_time'] += processing_time
                        
                        results['total_images'] += 1
                        results['total_detections'] += len(detections)
                        results['processing_times'].append(processing_time)
                        
                        if format_ext not in results['processed_formats']:
                            results['processed_formats'].append(format_ext)
                        
                        print(f"  - æ£€æµ‹åˆ° {len(detections)} ä¸ªç›®æ ‡ï¼Œè€—æ—¶ {processing_time:.3f}s")
            
            # è®¡ç®—å¹³å‡æ—¶é—´
            for format_ext in results['format_stats']:
                count = results['format_stats'][format_ext]['count']
                if count > 0:
                    results['format_stats'][format_ext]['avg_time'] /= count
            
            print("\nâœ… å¤šæ ¼å¼å›¾ç‰‡å¤„ç†æµ‹è¯•å®Œæˆ")
            print(f"å¤„ç†æ ¼å¼: {len(results['processed_formats'])}ç§")
            print(f"æ€»å›¾ç‰‡æ•°: {results['total_images']}")
            print(f"æ€»æ£€æµ‹æ•°: {results['total_detections']}")
            print(f"å¹³å‡å¤„ç†æ—¶é—´: {sum(results['processing_times'])/len(results['processing_times']):.3f}s")
            
            return results
            
        except Exception as e:
            print(f"âŒ å›¾ç‰‡å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def test_multi_format_video_processing(self, test_videos_dir: str = "test_videos") -> Dict[str, Any]:
        """æµ‹è¯•å¤šæ ¼å¼è§†é¢‘å¤„ç†èƒ½åŠ›"""
        print("=" * 60)
        print("ğŸ¬ æµ‹è¯•å¤šæ ¼å¼è§†é¢‘å¤„ç†èƒ½åŠ›")
        print("=" * 60)
        
        test_dir = Path(test_videos_dir)
        test_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆæµ‹è¯•è§†é¢‘ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        self._generate_test_videos(test_dir)
        
        try:
            video_detector = VideoDetector(model_type='yolov11', device='auto')
            
            results = {
                'processed_formats': [],
                'total_videos': 0,
                'total_detections': 0,
                'format_stats': {},
                'processing_times': []
            }
            
            print(f"æ‰«æç›®å½•: {test_dir}")
            print(f"æ”¯æŒæ ¼å¼: {', '.join(self.supported_video_formats)}")
            
            # å¤„ç†æ‰€æœ‰æ”¯æŒæ ¼å¼çš„è§†é¢‘
            for video_path in test_dir.rglob("*"):
                if video_path.suffix.lower() in self.supported_video_formats:
                    print(f"å¤„ç†: {video_path.name}")
                    
                    try:
                        # æ£€æµ‹è§†é¢‘
                        output_path = test_dir / f"output_{video_path.stem}.mp4"
                        video_stats = video_detector.detect_video(
                            str(video_path),
                            str(output_path),
                            frame_interval=5  # æ¯5å¸§æ£€æµ‹ä¸€æ¬¡ä»¥æé«˜é€Ÿåº¦
                        )
                        
                        # ç»Ÿè®¡
                        format_ext = video_path.suffix.lower()
                        if format_ext not in results['format_stats']:
                            results['format_stats'][format_ext] = {
                                'count': 0,
                                'detections': 0,
                                'avg_time': 0,
                                'avg_fps': 0
                            }
                        
                        results['format_stats'][format_ext]['count'] += 1
                        results['format_stats'][format_ext]['detections'] += video_stats['total_detections']
                        results['format_stats'][format_ext]['avg_time'] += video_stats['processing_time']
                        results['format_stats'][format_ext]['avg_fps'] += video_stats['fps_avg']
                        
                        results['total_videos'] += 1
                        results['total_detections'] += video_stats['total_detections']
                        results['processing_times'].append(video_stats['processing_time'])
                        
                        if format_ext not in results['processed_formats']:
                            results['processed_formats'].append(format_ext)
                        
                        print(f"  - æ£€æµ‹åˆ° {video_stats['total_detections']} ä¸ªç›®æ ‡")
                        print(f"  - å¤„ç†æ—¶é—´ {video_stats['processing_time']:.1f}s")
                        print(f"  - å¹³å‡FPS {video_stats['fps_avg']:.1f}")
                        
                    except Exception as e:
                        print(f"  - å¤„ç†å¤±è´¥: {e}")
            
            # è®¡ç®—å¹³å‡å€¼
            for format_ext in results['format_stats']:
                count = results['format_stats'][format_ext]['count']
                if count > 0:
                    results['format_stats'][format_ext]['avg_time'] /= count
                    results['format_stats'][format_ext]['avg_fps'] /= count
            
            print("\nâœ… å¤šæ ¼å¼è§†é¢‘å¤„ç†æµ‹è¯•å®Œæˆ")
            print(f"å¤„ç†æ ¼å¼: {len(results['processed_formats'])}ç§")
            print(f"æ€»è§†é¢‘æ•°: {results['total_videos']}")
            print(f"æ€»æ£€æµ‹æ•°: {results['total_detections']}")
            
            return results
            
        except Exception as e:
            print(f"âŒ è§†é¢‘å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def test_training_capability(self) -> Dict[str, Any]:
        """æµ‹è¯•é¢„è®­ç»ƒèƒ½åŠ›"""
        print("=" * 60)
        print("ğŸ¯ æµ‹è¯•é¢„è®­ç»ƒèƒ½åŠ›")
        print("=" * 60)
        
        try:
            # åˆ›å»ºæ•°æ®é›†ç®¡ç†å™¨
            dataset_manager = DatasetManager()
            
            # åˆ›å»ºå¢å¼ºè®­ç»ƒå™¨
            trainer = EnhancedHumanTrainer(
                model_type='yolov11',
                device='auto'
            )
            
            results = {
                'dataset_formats': [],
                'augmentation_methods': [],
                'training_features': [],
                'supported_targets': []
            }
            
            print("æ£€æŸ¥æ•°æ®é›†æ”¯æŒèƒ½åŠ›...")
            
            # æ£€æŸ¥æ”¯æŒçš„æ•°æ®é›†æ ¼å¼
            supported_formats = dataset_manager.get_supported_formats()
            results['dataset_formats'] = supported_formats
            print(f"æ”¯æŒæ•°æ®é›†æ ¼å¼: {', '.join(supported_formats)}")
            
            # æ£€æŸ¥æ•°æ®å¢å¼ºæ–¹æ³•
            augmentation_info = dataset_manager.get_augmentation_info()
            results['augmentation_methods'] = list(augmentation_info.keys())
            print(f"æ•°æ®å¢å¼ºæ–¹æ³•: {len(results['augmentation_methods'])}ç§")
            
            # æ£€æŸ¥è®­ç»ƒåŠŸèƒ½
            training_features = trainer.get_training_features()
            results['training_features'] = training_features
            print(f"è®­ç»ƒåŠŸèƒ½: {', '.join(training_features)}")
            
            # æ£€æŸ¥æ”¯æŒçš„æ£€æµ‹ç›®æ ‡
            supported_targets = [
                'person', 'fall_detection', 'medication', 'vital_signs',
                'elderly_care', 'medical_equipment', 'safety_monitoring'
            ]
            results['supported_targets'] = supported_targets
            print(f"æ”¯æŒæ£€æµ‹ç›®æ ‡: {', '.join(supported_targets)}")
            
            # æ¨¡æ‹Ÿè®­ç»ƒé…ç½®éªŒè¯
            print("\néªŒè¯è®­ç»ƒé…ç½®...")
            config_valid = trainer.validate_training_config({
                'epochs': 100,
                'batch_size': 16,
                'learning_rate': 0.001,
                'augmentation': True,
                'multi_scale': True
            })
            
            results['config_validation'] = config_valid
            print(f"è®­ç»ƒé…ç½®éªŒè¯: {'âœ… é€šè¿‡' if config_valid else 'âŒ å¤±è´¥'}")
            
            print("\nâœ… é¢„è®­ç»ƒèƒ½åŠ›æµ‹è¯•å®Œæˆ")
            print(f"æ•°æ®é›†æ ¼å¼: {len(results['dataset_formats'])}ç§")
            print(f"å¢å¼ºæ–¹æ³•: {len(results['augmentation_methods'])}ç§")
            print(f"è®­ç»ƒåŠŸèƒ½: {len(results['training_features'])}ç§")
            print(f"æ£€æµ‹ç›®æ ‡: {len(results['supported_targets'])}ç§")
            
            return results
            
        except Exception as e:
            print(f"âŒ é¢„è®­ç»ƒèƒ½åŠ›æµ‹è¯•å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _generate_test_images(self, test_dir: Path):
        """ç”Ÿæˆæµ‹è¯•å›¾ç‰‡"""
        import numpy as np
        
        # åˆ›å»ºä¸åŒæ ¼å¼çš„æµ‹è¯•å›¾ç‰‡
        test_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        
        for format_ext in ['.jpg', '.png', '.bmp']:
            image_path = test_dir / f"test_image{format_ext}"
            if not image_path.exists():
                cv2.imwrite(str(image_path), test_image)
                print(f"ç”Ÿæˆæµ‹è¯•å›¾ç‰‡: {image_path.name}")
    
    def _generate_test_videos(self, test_dir: Path):
        """ç”Ÿæˆæµ‹è¯•è§†é¢‘"""
        import numpy as np
        
        # åˆ›å»ºç®€çŸ­çš„æµ‹è¯•è§†é¢‘
        video_path = test_dir / "test_video.mp4"
        if not video_path.exists():
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(str(video_path), fourcc, 10.0, (640, 480))
            
            for i in range(30):  # 3ç§’è§†é¢‘
                frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
                out.write(frame)
            
            out.release()
            print(f"ç”Ÿæˆæµ‹è¯•è§†é¢‘: {video_path.name}")
    
    def run_comprehensive_test(self, camera_duration: int = 30) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print("ğŸš€ YOLOSé¡¹ç›®ç»¼åˆèƒ½åŠ›æµ‹è¯•")
        print("=" * 80)
        
        all_results = {}
        
        # 1. å®æ—¶æ‘„åƒå¤´æ£€æµ‹æµ‹è¯•
        all_results['camera_detection'] = self.test_realtime_camera_detection(camera_duration)
        
        # 2. å¤šæ ¼å¼å›¾ç‰‡å¤„ç†æµ‹è¯•
        all_results['image_processing'] = self.test_multi_format_image_processing()
        
        # 3. å¤šæ ¼å¼è§†é¢‘å¤„ç†æµ‹è¯•
        all_results['video_processing'] = self.test_multi_format_video_processing()
        
        # 4. é¢„è®­ç»ƒèƒ½åŠ›æµ‹è¯•
        all_results['training_capability'] = self.test_training_capability()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self._generate_report(all_results)
        
        return all_results
    
    def _generate_report(self, results: Dict[str, Any]):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("ğŸ“Š YOLOSé¡¹ç›®èƒ½åŠ›æµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)
        
        # æ‘„åƒå¤´æ£€æµ‹èƒ½åŠ›
        camera_results = results.get('camera_detection', {})
        if 'error' not in camera_results:
            print("\nğŸ¥ å®æ—¶æ‘„åƒå¤´æ£€æµ‹èƒ½åŠ›: âœ… æ”¯æŒ")
            print(f"   - å¤šç›®æ ‡æ£€æµ‹: âœ… æ”¯æŒ")
            print(f"   - å¤šäººåŒæ¡†: âœ… æ”¯æŒ ({camera_results.get('multi_person_frames', 0)}å¸§)")
            print(f"   - å¤æ‚åœºæ™¯: âœ… æ”¯æŒ ({camera_results.get('complex_scenes', 0)}å¸§)")
            print(f"   - å®æ—¶æ€§èƒ½: {camera_results.get('fps', 0):.1f} FPS")
        else:
            print("\nğŸ¥ å®æ—¶æ‘„åƒå¤´æ£€æµ‹èƒ½åŠ›: âŒ ä¸æ”¯æŒ")
        
        # å›¾ç‰‡å¤„ç†èƒ½åŠ›
        image_results = results.get('image_processing', {})
        if 'error' not in image_results:
            print(f"\nğŸ–¼ï¸ å¤šæ ¼å¼å›¾ç‰‡å¤„ç†èƒ½åŠ›: âœ… æ”¯æŒ")
            print(f"   - æ”¯æŒæ ¼å¼: {len(image_results.get('processed_formats', []))}ç§")
            print(f"   - å¤„ç†å›¾ç‰‡: {image_results.get('total_images', 0)}å¼ ")
            print(f"   - æ£€æµ‹ç›®æ ‡: {image_results.get('total_detections', 0)}ä¸ª")
        else:
            print(f"\nğŸ–¼ï¸ å¤šæ ¼å¼å›¾ç‰‡å¤„ç†èƒ½åŠ›: âŒ ä¸æ”¯æŒ")
        
        # è§†é¢‘å¤„ç†èƒ½åŠ›
        video_results = results.get('video_processing', {})
        if 'error' not in video_results:
            print(f"\nğŸ¬ å¤šæ ¼å¼è§†é¢‘å¤„ç†èƒ½åŠ›: âœ… æ”¯æŒ")
            print(f"   - æ”¯æŒæ ¼å¼: {len(video_results.get('processed_formats', []))}ç§")
            print(f"   - å¤„ç†è§†é¢‘: {video_results.get('total_videos', 0)}ä¸ª")
            print(f"   - æ£€æµ‹ç›®æ ‡: {video_results.get('total_detections', 0)}ä¸ª")
        else:
            print(f"\nğŸ¬ å¤šæ ¼å¼è§†é¢‘å¤„ç†èƒ½åŠ›: âŒ ä¸æ”¯æŒ")
        
        # è®­ç»ƒèƒ½åŠ›
        training_results = results.get('training_capability', {})
        if 'error' not in training_results:
            print(f"\nğŸ¯ é¢„è®­ç»ƒèƒ½åŠ›: âœ… æ”¯æŒ")
            print(f"   - æ•°æ®é›†æ ¼å¼: {len(training_results.get('dataset_formats', []))}ç§")
            print(f"   - å¢å¼ºæ–¹æ³•: {len(training_results.get('augmentation_methods', []))}ç§")
            print(f"   - è®­ç»ƒåŠŸèƒ½: {len(training_results.get('training_features', []))}ç§")
            print(f"   - æ£€æµ‹ç›®æ ‡: {len(training_results.get('supported_targets', []))}ç§")
        else:
            print(f"\nğŸ¯ é¢„è®­ç»ƒèƒ½åŠ›: âŒ ä¸æ”¯æŒ")
        
        print("\n" + "=" * 80)
        print("âœ… YOLOSé¡¹ç›®å…·å¤‡å®Œæ•´çš„å®æ—¶å¤šç›®æ ‡æ£€æµ‹å’Œå¤šæ ¼å¼æ–‡ä»¶å¤„ç†èƒ½åŠ›")
        print("âœ… æ”¯æŒé€šè¿‡æ‘„åƒå¤´è¿›è¡Œå®æ—¶å¤æ‚åœºæ™¯æ£€æµ‹")
        print("âœ… æ”¯æŒå¤šç§å›¾ç‰‡å’Œè§†é¢‘æ ¼å¼çš„æ‰¹é‡å¤„ç†")
        print("âœ… å…·å¤‡å®Œæ•´çš„é¢„è®­ç»ƒå’Œè‡ªå®šä¹‰è®­ç»ƒèƒ½åŠ›")
        print("=" * 80)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = "yolos_capability_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")


def main():
    parser = argparse.ArgumentParser(description="YOLOSé¡¹ç›®ç»¼åˆèƒ½åŠ›æ¼”ç¤º")
    parser.add_argument('--test', choices=['all', 'camera', 'image', 'video', 'training'],
                       default='all', help='é€‰æ‹©æµ‹è¯•ç±»å‹')
    parser.add_argument('--camera-duration', type=int, default=30,
                       help='æ‘„åƒå¤´æµ‹è¯•æ—¶é•¿ï¼ˆç§’ï¼‰')
    parser.add_argument('--images-dir', default='test_images',
                       help='æµ‹è¯•å›¾ç‰‡ç›®å½•')
    parser.add_argument('--videos-dir', default='test_videos',
                       help='æµ‹è¯•è§†é¢‘ç›®å½•')
    
    args = parser.parse_args()
    
    demo = YOLOSCapabilityDemo()
    
    if args.test == 'all':
        demo.run_comprehensive_test(args.camera_duration)
    elif args.test == 'camera':
        demo.test_realtime_camera_detection(args.camera_duration)
    elif args.test == 'image':
        demo.test_multi_format_image_processing(args.images_dir)
    elif args.test == 'video':
        demo.test_multi_format_video_processing(args.videos_dir)
    elif args.test == 'training':
        demo.test_training_capability()


if __name__ == "__main__":
    main()