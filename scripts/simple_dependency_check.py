#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„ä¾èµ–åˆ†æå·¥å…·
æ£€æŸ¥é¡¹ç›®çš„å…³é”®æ¶æ„é—®é¢˜
"""

import os
import ast
import sys
from pathlib import Path
from collections import defaultdict, deque
from typing import Dict, List, Set, Tuple

class SimpleDependencyAnalyzer:
    """ç®€åŒ–ä¾èµ–åˆ†æå™¨"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.imports: Dict[str, Set[str]] = defaultdict(set)
        self.files: List[str] = []
        
    def scan_project(self):
        """æ‰«æé¡¹ç›®æ–‡ä»¶"""
        print("ğŸ” æ‰«æé¡¹ç›®æ–‡ä»¶...")
        
        for py_file in self.project_root.rglob("*.py"):
            if self._should_skip_file(py_file):
                continue
                
            rel_path = str(py_file.relative_to(self.project_root))
            self.files.append(rel_path)
            
            try:
                self._analyze_file(py_file, rel_path)
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {rel_path}: {e}")
        
        print(f"ğŸ“ æ‰«æå®Œæˆï¼Œå…± {len(self.files)} ä¸ªæ–‡ä»¶")
    
    def _should_skip_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦è·³è¿‡æ–‡ä»¶"""
        skip_dirs = {'.git', '__pycache__', '.pytest_cache', 'node_modules'}
        return any(part in skip_dirs for part in file_path.parts)
    
    def _analyze_file(self, file_path: Path, rel_path: str):
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        self.imports[rel_path].add(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        self.imports[rel_path].add(node.module)
        except Exception:
            # å°è¯•å…¶ä»–ç¼–ç 
            try:
                with open(file_path, 'r', encoding='gbk') as f:
                    content = f.read()
                tree = ast.parse(content)
                # å¤„ç†å¯¼å…¥...
            except Exception:
                raise
    
    def find_circular_dependencies(self) -> List[List[str]]:
        """æŸ¥æ‰¾å¾ªç¯ä¾èµ–"""
        print("ğŸ”„ æ£€æŸ¥å¾ªç¯ä¾èµ–...")
        
        # æ„å»ºä¾èµ–å›¾
        graph = defaultdict(set)
        for file, imports in self.imports.items():
            for imp in imports:
                # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
                target_file = self._resolve_import_to_file(imp)
                if target_file and target_file in self.files:
                    graph[file].add(target_file)
        
        # ä½¿ç”¨DFSæ£€æµ‹å¾ªç¯
        cycles = []
        visited = set()
        rec_stack = set()
        
        def dfs(node, path):
            if node in rec_stack:
                # æ‰¾åˆ°å¾ªç¯
                cycle_start = path.index(node)
                cycle = path[cycle_start:] + [node]
                cycles.append(cycle)
                return
            
            if node in visited:
                return
            
            visited.add(node)
            rec_stack.add(node)
            
            for neighbor in graph[node]:
                dfs(neighbor, path + [neighbor])
            
            rec_stack.remove(node)
        
        for file in self.files:
            if file not in visited:
                dfs(file, [file])
        
        return cycles
    
    def _resolve_import_to_file(self, import_name: str) -> str:
        """å°†å¯¼å…¥åç§°è§£æä¸ºæ–‡ä»¶è·¯å¾„"""
        # ç®€åŒ–çš„è§£æé€»è¾‘
        if import_name.startswith('.'):
            return None  # ç›¸å¯¹å¯¼å…¥ï¼Œæš‚æ—¶è·³è¿‡
        
        # è½¬æ¢ä¸ºæ–‡ä»¶è·¯å¾„
        parts = import_name.split('.')
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é¡¹ç›®å†…çš„æ¨¡å—
        for i in range(len(parts), 0, -1):
            potential_path = '/'.join(parts[:i]) + '.py'
            if potential_path in self.files:
                return potential_path
            
            # æ£€æŸ¥åŒ…ç»“æ„
            potential_dir = '/'.join(parts[:i])
            init_file = f"{potential_dir}/__init__.py"
            if init_file in self.files:
                return init_file
        
        return None
    
    def find_duplicate_classes(self) -> Dict[str, List[str]]:
        """æŸ¥æ‰¾é‡å¤çš„ç±»å®šä¹‰"""
        print("ğŸ” æ£€æŸ¥é‡å¤ç±»...")
        
        class_definitions = defaultdict(list)
        
        for file_path in self.files:
            try:
                full_path = self.project_root / file_path
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                tree = ast.parse(content)
                
                for node in ast.walk(tree):
                    if isinstance(node, ast.ClassDef):
                        class_definitions[node.name].append(file_path)
            except Exception:
                continue
        
        # åªè¿”å›æœ‰é‡å¤çš„ç±»
        duplicates = {name: files for name, files in class_definitions.items() 
                     if len(files) > 1}
        
        return duplicates
    
    def analyze_complexity(self) -> Dict[str, int]:
        """åˆ†ææ–‡ä»¶å¤æ‚åº¦"""
        print("ğŸ“Š åˆ†æä»£ç å¤æ‚åº¦...")
        
        complexity = {}
        
        for file_path in self.files:
            try:
                full_path = self.project_root / file_path
                with open(full_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                # ç®€å•çš„å¤æ‚åº¦è®¡ç®—ï¼šè¡Œæ•° + ç±»æ•° + å‡½æ•°æ•°
                line_count = len([line for line in lines if line.strip() and not line.strip().startswith('#')])
                
                try:
                    tree = ast.parse(''.join(lines))
                    class_count = len([node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)])
                    func_count = len([node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)])
                    
                    complexity[file_path] = line_count + class_count * 5 + func_count * 2
                except:
                    complexity[file_path] = line_count
                    
            except Exception:
                complexity[file_path] = 0
        
        return complexity
    
    def generate_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("\n" + "="*50)
        print("ğŸ“‹ YOLOSé¡¹ç›®æ¶æ„åˆ†ææŠ¥å‘Š")
        print("="*50)
        
        # å¾ªç¯ä¾èµ–
        cycles = self.find_circular_dependencies()
        print(f"\nğŸ”„ å¾ªç¯ä¾èµ–æ£€æŸ¥:")
        if cycles:
            print(f"âŒ å‘ç° {len(cycles)} ä¸ªå¾ªç¯ä¾èµ–:")
            for i, cycle in enumerate(cycles[:5], 1):
                print(f"   {i}. {' -> '.join(cycle)}")
            if len(cycles) > 5:
                print(f"   ... è¿˜æœ‰ {len(cycles) - 5} ä¸ª")
        else:
            print("âœ… æœªå‘ç°å¾ªç¯ä¾èµ–")
        
        # é‡å¤ç±»
        duplicates = self.find_duplicate_classes()
        print(f"\nğŸ” é‡å¤ç±»æ£€æŸ¥:")
        if duplicates:
            print(f"âŒ å‘ç° {len(duplicates)} ä¸ªé‡å¤ç±»:")
            for class_name, files in list(duplicates.items())[:5]:
                print(f"   - {class_name}: {', '.join(files)}")
            if len(duplicates) > 5:
                print(f"   ... è¿˜æœ‰ {len(duplicates) - 5} ä¸ª")
        else:
            print("âœ… æœªå‘ç°é‡å¤ç±»å®šä¹‰")
        
        # å¤æ‚åº¦åˆ†æ
        complexity = self.analyze_complexity()
        high_complexity = {f: c for f, c in complexity.items() if c > 200}
        
        print(f"\nğŸ“Š å¤æ‚åº¦åˆ†æ:")
        print(f"   æ€»æ–‡ä»¶æ•°: {len(self.files)}")
        print(f"   é«˜å¤æ‚åº¦æ–‡ä»¶ (>200): {len(high_complexity)}")
        
        if high_complexity:
            print("   æœ€å¤æ‚çš„æ–‡ä»¶:")
            sorted_complex = sorted(high_complexity.items(), key=lambda x: x[1], reverse=True)
            for file, comp in sorted_complex[:5]:
                print(f"     - {file}: {comp}")
        
        # å»ºè®®
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        if cycles:
            print("   - é‡æ„å¾ªç¯ä¾èµ–ï¼Œä½¿ç”¨ä¾èµ–æ³¨å…¥æˆ–æ¥å£æŠ½è±¡")
        if duplicates:
            print("   - åˆå¹¶é‡å¤ç±»ï¼Œæå–å…¬å…±åŸºç±»")
        if high_complexity:
            print("   - æ‹†åˆ†é«˜å¤æ‚åº¦æ–‡ä»¶ï¼Œéµå¾ªå•ä¸€èŒè´£åŸåˆ™")
        
        if not cycles and not duplicates and not high_complexity:
            print("   âœ… é¡¹ç›®æ¶æ„è‰¯å¥½ï¼Œæ— éœ€ç‰¹åˆ«ä¼˜åŒ–")

def main():
    """ä¸»å‡½æ•°"""
    project_root = os.getcwd()
    analyzer = SimpleDependencyAnalyzer(project_root)
    
    try:
        analyzer.scan_project()
        analyzer.generate_report()
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main())