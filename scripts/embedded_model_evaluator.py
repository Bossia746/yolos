#!/usr/bin/env python3
"""
åµŒå…¥å¼è®¾å¤‡æ¨¡å‹æ€§èƒ½è¯„ä¼°å™¨
è¯„ä¼°YOLOæ¨¡å‹åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„æ€§èƒ½è¡¨ç°
"""

import os
import sys
import time
import psutil
import logging
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

try:
    import torch
    import torchvision
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False

try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False

try:
    import tensorflow as tf
    TF_AVAILABLE = True
except ImportError:
    TF_AVAILABLE = False

@dataclass
class PlatformSpec:
    """å¹³å°è§„æ ¼"""
    name: str
    memory_mb: int
    storage_mb: int
    cpu_cores: int
    cpu_freq_mhz: int
    has_gpu: bool = False
    has_npu: bool = False
    power_limit_mw: int = 5000

@dataclass
class ModelMetrics:
    """æ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""
    model_size_mb: float
    memory_usage_mb: float
    inference_time_ms: float
    fps: float
    accuracy: float = 0.0
    power_consumption_mw: float = 0.0
    is_feasible: bool = True
    bottlenecks: List[str] = None

class EmbeddedModelEvaluator:
    """åµŒå…¥å¼æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.setup_logging()
        
        # å®šä¹‰ç›®æ ‡å¹³å°
        self.platforms = {
            'esp32': PlatformSpec(
                name='ESP32',
                memory_mb=32,  # 512KB SRAM + 8MB PSRAM
                storage_mb=16,
                cpu_cores=2,
                cpu_freq_mhz=240,
                power_limit_mw=500
            ),
            'esp32_s3': PlatformSpec(
                name='ESP32-S3',
                memory_mb=64,  # 512KB SRAM + 32MB PSRAM
                storage_mb=32,
                cpu_cores=2,
                cpu_freq_mhz=240,
                power_limit_mw=600
            ),
            'raspberry_pi_zero': PlatformSpec(
                name='Raspberry Pi Zero 2W',
                memory_mb=512,
                storage_mb=8192,  # 8GB SDå¡
                cpu_cores=4,
                cpu_freq_mhz=1000,
                power_limit_mw=2000
            ),
            'raspberry_pi_4': PlatformSpec(
                name='Raspberry Pi 4B',
                memory_mb=4096,
                storage_mb=32768,  # 32GB SDå¡
                cpu_cores=4,
                cpu_freq_mhz=1500,
                has_gpu=True,
                power_limit_mw=3000
            ),
            'jetson_nano': PlatformSpec(
                name='NVIDIA Jetson Nano',
                memory_mb=4096,
                storage_mb=16384,
                cpu_cores=4,
                cpu_freq_mhz=1430,
                has_gpu=True,
                power_limit_mw=5000
            ),
            'k230': PlatformSpec(
                name='Canaan K230',
                memory_mb=512,
                storage_mb=8192,
                cpu_cores=2,
                cpu_freq_mhz=1600,
                has_npu=True,
                power_limit_mw=2000
            )
        }
        
        # æ¨¡å‹é…ç½®
        self.model_configs = {
            'yolov11n': {'size': 'n', 'expected_size_mb': 5.9},
            'yolov11s': {'size': 's', 'expected_size_mb': 21.5},
            'yolov11m': {'size': 'm', 'expected_size_mb': 49.7},
            'yolov11l': {'size': 'l', 'expected_size_mb': 86.9},
            'yolov11x': {'size': 'x', 'expected_size_mb': 137.3}
        }
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
    def evaluate_all_combinations(self) -> Dict:
        """è¯„ä¼°æ‰€æœ‰å¹³å°å’Œæ¨¡å‹ç»„åˆ"""
        results = {}
        
        for platform_name, platform_spec in self.platforms.items():
            results[platform_name] = {}
            
            for model_name, model_config in self.model_configs.items():
                self.logger.info(f"è¯„ä¼° {model_name} åœ¨ {platform_name} ä¸Šçš„æ€§èƒ½")
                
                metrics = self.evaluate_model_on_platform(
                    model_name, model_config, platform_spec
                )
                
                results[platform_name][model_name] = metrics
                
        return results
        
    def evaluate_model_on_platform(self, model_name: str, 
                                  model_config: Dict, 
                                  platform: PlatformSpec) -> ModelMetrics:
        """è¯„ä¼°ç‰¹å®šæ¨¡å‹åœ¨ç‰¹å®šå¹³å°ä¸Šçš„æ€§èƒ½"""
        
        # ä¼°ç®—æ¨¡å‹å¤§å°
        model_size_mb = model_config['expected_size_mb']
        
        # ä¼°ç®—å†…å­˜ä½¿ç”¨ï¼ˆæ¨¡å‹å¤§å° + æ¨ç†ç¼“å†²åŒºï¼‰
        inference_buffer_mb = self._estimate_inference_buffer(model_config['size'])
        total_memory_mb = model_size_mb + inference_buffer_mb
        
        # ä¼°ç®—æ¨ç†æ—¶é—´
        inference_time_ms = self._estimate_inference_time(
            model_config['size'], platform
        )
        
        # è®¡ç®—FPS
        fps = 1000.0 / inference_time_ms if inference_time_ms > 0 else 0
        
        # å¯è¡Œæ€§åˆ†æ
        bottlenecks = []
        is_feasible = True
        
        # å†…å­˜æ£€æŸ¥
        if total_memory_mb > platform.memory_mb:
            bottlenecks.append(f"å†…å­˜ä¸è¶³: éœ€è¦{total_memory_mb}MB, å¯ç”¨{platform.memory_mb}MB")
            is_feasible = False
            
        # å­˜å‚¨æ£€æŸ¥
        if model_size_mb > platform.storage_mb * 0.5:  # æ¨¡å‹ä¸åº”å ç”¨è¶…è¿‡50%å­˜å‚¨
            bottlenecks.append(f"å­˜å‚¨ä¸è¶³: æ¨¡å‹{model_size_mb}MB, å¯ç”¨{platform.storage_mb}MB")
            is_feasible = False
            
        # æ€§èƒ½æ£€æŸ¥
        if fps < 1.0:  # è‡³å°‘1FPS
            bottlenecks.append(f"æ€§èƒ½ä¸è¶³: FPS={fps:.2f}")
            is_feasible = False
            
        # åŠŸè€—æ£€æŸ¥
        power_consumption = self._estimate_power_consumption(
            model_config['size'], platform
        )
        if power_consumption > platform.power_limit_mw:
            bottlenecks.append(f"åŠŸè€—è¿‡é«˜: {power_consumption}mW > {platform.power_limit_mw}mW")
            is_feasible = False
            
        return ModelMetrics(
            model_size_mb=model_size_mb,
            memory_usage_mb=total_memory_mb,
            inference_time_ms=inference_time_ms,
            fps=fps,
            power_consumption_mw=power_consumption,
            is_feasible=is_feasible,
            bottlenecks=bottlenecks or []
        )
        
    def _estimate_inference_buffer(self, model_size: str) -> float:
        """ä¼°ç®—æ¨ç†ç¼“å†²åŒºå¤§å°"""
        buffer_sizes = {
            'n': 20,   # 20MB
            's': 40,   # 40MB
            'm': 80,   # 80MB
            'l': 120,  # 120MB
            'x': 200   # 200MB
        }
        return buffer_sizes.get(model_size, 50)
        
    def _estimate_inference_time(self, model_size: str, platform: PlatformSpec) -> float:
        """ä¼°ç®—æ¨ç†æ—¶é—´"""
        # åŸºç¡€æ¨ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
        base_times = {
            'n': 50,
            's': 100,
            'm': 200,
            'l': 400,
            'x': 800
        }
        
        base_time = base_times.get(model_size, 200)
        
        # æ ¹æ®å¹³å°è°ƒæ•´
        if platform.name == 'ESP32':
            # ESP32æ€§èƒ½å¾ˆä½ï¼Œä½†é€šå¸¸ä¸ä¼šè¿è¡Œå®Œæ•´YOLO
            return base_time * 50  # å‡è®¾æ€§èƒ½å·®50å€
        elif 'Raspberry Pi Zero' in platform.name:
            return base_time * 10  # æ€§èƒ½å·®10å€
        elif 'Raspberry Pi 4' in platform.name:
            if platform.has_gpu:
                return base_time * 2  # GPUåŠ é€Ÿ
            else:
                return base_time * 5  # CPUæ¨ç†
        elif 'Jetson' in platform.name:
            return base_time * 0.5  # GPUåŠ é€Ÿï¼Œæ€§èƒ½å¥½
        elif 'K230' in platform.name:
            if platform.has_npu:
                return base_time * 0.3  # NPUåŠ é€Ÿ
            else:
                return base_time * 3
        else:
            return base_time
            
    def _estimate_power_consumption(self, model_size: str, platform: PlatformSpec) -> float:
        """ä¼°ç®—åŠŸè€—"""
        # åŸºç¡€åŠŸè€—ï¼ˆæ¯«ç“¦ï¼‰
        base_power = {
            'n': 200,
            's': 400,
            'm': 800,
            'l': 1200,
            'x': 2000
        }
        
        power = base_power.get(model_size, 500)
        
        # å¹³å°è°ƒæ•´
        if platform.name == 'ESP32':
            return min(power * 0.1, platform.power_limit_mw)  # ä½åŠŸè€—
        elif 'Raspberry Pi' in platform.name:
            return power * 0.8
        elif 'Jetson' in platform.name:
            return power * 1.5  # GPUåŠŸè€—é«˜
        else:
            return power
            
    def generate_optimization_recommendations(self, results: Dict) -> Dict:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = {}
        
        for platform_name, platform_results in results.items():
            platform_spec = self.platforms[platform_name]
            recommendations[platform_name] = {
                'feasible_models': [],
                'optimization_strategies': [],
                'alternative_approaches': []
            }
            
            # æ‰¾å‡ºå¯è¡Œçš„æ¨¡å‹
            for model_name, metrics in platform_results.items():
                if metrics.is_feasible:
                    recommendations[platform_name]['feasible_models'].append({
                        'model': model_name,
                        'fps': metrics.fps,
                        'memory_mb': metrics.memory_usage_mb
                    })
                    
            # ç”Ÿæˆä¼˜åŒ–ç­–ç•¥
            if platform_spec.memory_mb < 100:  # æä½å†…å­˜è®¾å¤‡
                recommendations[platform_name]['optimization_strategies'].extend([
                    "ä½¿ç”¨INT8é‡åŒ–å‡å°‘å†…å­˜å ç”¨",
                    "å®ç°æ¨¡å‹åˆ†å‰²ï¼Œåˆ†é˜¶æ®µæ¨ç†",
                    "ä½¿ç”¨ä¸“ç”¨çš„è½»é‡çº§æ£€æµ‹ç®—æ³•",
                    "è€ƒè™‘è¾¹ç¼˜-äº‘ååŒæ¶æ„"
                ])
                recommendations[platform_name]['alternative_approaches'].extend([
                    "ä½¿ç”¨TensorFlow Lite Micro",
                    "ä½¿ç”¨OpenMVæˆ–ç±»ä¼¼çš„è®¡ç®—æœºè§†è§‰åº“",
                    "å®ç°ç®€å•çš„ç‰¹å¾æ£€æµ‹ç®—æ³•"
                ])
            elif platform_spec.memory_mb < 1000:  # ä¸­ç­‰å†…å­˜è®¾å¤‡
                recommendations[platform_name]['optimization_strategies'].extend([
                    "ä½¿ç”¨ONNX Runtimeä¼˜åŒ–æ¨ç†",
                    "å¯ç”¨FP16ç²¾åº¦",
                    "ä½¿ç”¨æ¨¡å‹å‰ªææŠ€æœ¯",
                    "ä¼˜åŒ–è¾“å…¥åˆ†è¾¨ç‡"
                ])
            else:  # é«˜å†…å­˜è®¾å¤‡
                recommendations[platform_name]['optimization_strategies'].extend([
                    "ä½¿ç”¨TensorRTåŠ é€Ÿï¼ˆå¦‚æœæœ‰GPUï¼‰",
                    "å¯ç”¨æ‰¹å¤„ç†æ¨ç†",
                    "ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹è·å¾—æ›´å¥½ç²¾åº¦"
                ])
                
        return recommendations
        
    def save_results(self, results: Dict, recommendations: Dict, output_file: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_results = {}
        for platform, models in results.items():
            serializable_results[platform] = {}
            for model, metrics in models.items():
                serializable_results[platform][model] = {
                    'model_size_mb': metrics.model_size_mb,
                    'memory_usage_mb': metrics.memory_usage_mb,
                    'inference_time_ms': metrics.inference_time_ms,
                    'fps': metrics.fps,
                    'power_consumption_mw': metrics.power_consumption_mw,
                    'is_feasible': metrics.is_feasible,
                    'bottlenecks': metrics.bottlenecks
                }
                
        output_data = {
            'evaluation_results': serializable_results,
            'optimization_recommendations': recommendations,
            'evaluation_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'platform_specifications': {
                name: {
                    'memory_mb': spec.memory_mb,
                    'storage_mb': spec.storage_mb,
                    'cpu_cores': spec.cpu_cores,
                    'cpu_freq_mhz': spec.cpu_freq_mhz,
                    'has_gpu': spec.has_gpu,
                    'has_npu': spec.has_npu,
                    'power_limit_mw': spec.power_limit_mw
                } for name, spec in self.platforms.items()
            }
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
            
        self.logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    def print_summary(self, results: Dict, recommendations: Dict):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print("\n" + "="*80)
        print("åµŒå…¥å¼è®¾å¤‡æ¨¡å‹æ€§èƒ½è¯„ä¼°æ‘˜è¦")
        print("="*80)
        
        for platform_name, platform_results in results.items():
            platform_spec = self.platforms[platform_name]
            print(f"\nğŸ“± {platform_spec.name}")
            print(f"   å†…å­˜: {platform_spec.memory_mb}MB | å­˜å‚¨: {platform_spec.storage_mb}MB")
            print(f"   CPU: {platform_spec.cpu_cores}æ ¸@{platform_spec.cpu_freq_mhz}MHz")
            
            feasible_count = sum(1 for metrics in platform_results.values() if metrics.is_feasible)
            total_count = len(platform_results)
            
            print(f"   âœ… å¯è¡Œæ¨¡å‹: {feasible_count}/{total_count}")
            
            # æ˜¾ç¤ºæœ€ä½³æ¨¡å‹
            best_model = None
            best_fps = 0
            for model_name, metrics in platform_results.items():
                if metrics.is_feasible and metrics.fps > best_fps:
                    best_model = model_name
                    best_fps = metrics.fps
                    
            if best_model:
                metrics = platform_results[best_model]
                print(f"   ğŸ† æ¨èæ¨¡å‹: {best_model}")
                print(f"      FPS: {metrics.fps:.1f} | å†…å­˜: {metrics.memory_usage_mb:.1f}MB")
            else:
                print(f"   âŒ æ— å¯è¡Œæ¨¡å‹")
                
        print("\n" + "="*80)
        
if __name__ == "__main__":
    evaluator = EmbeddedModelEvaluator()
    
    print("å¼€å§‹åµŒå…¥å¼è®¾å¤‡æ¨¡å‹æ€§èƒ½è¯„ä¼°...")
    
    # æ‰§è¡Œè¯„ä¼°
    results = evaluator.evaluate_all_combinations()
    
    # ç”Ÿæˆå»ºè®®
    recommendations = evaluator.generate_optimization_recommendations(results)
    
    # ä¿å­˜ç»“æœ
    output_file = "embedded_model_evaluation_results.json"
    evaluator.save_results(results, recommendations, output_file)
    
    # æ‰“å°æ‘˜è¦
    evaluator.print_summary(results, recommendations)
    
    print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")